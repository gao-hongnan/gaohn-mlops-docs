{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Welcome to Hongnan G.'s website. Here you can follow my learning journey. I write consistently about mathematics, computer science, and programming, with heavy focus on fundamental concepts. I also dabble in computer vision and participate in competitions from time to time.","title":"Home"},{"location":"#welcome","text":"Welcome to Hongnan G.'s website. Here you can follow my learning journey. I write consistently about mathematics, computer science, and programming, with heavy focus on fundamental concepts. I also dabble in computer vision and participate in competitions from time to time.","title":"Welcome"},{"location":"about/","text":"My name is Hongnan Gao, hailing from Singapore. I graduated from the National University of Singapore with a degree in Mathematics in 2019.","title":"About"},{"location":"mlops_docs/cicd/github_actions/","text":"Test Packages Compatibility We can use GitHub Actions to test for compatibility on different Operating Systems (OS) such as Windows/Linux. packages_compatibility.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 name : Commit Checks on : [ push , pull_request ] jobs : check_code : runs-on : ${{ matrix.os }} strategy : fail-fast : true matrix : os : [ ubuntu-latest , windows-latest ] python-version : [ 3.7 ] steps : - name : Checkout code uses : actions/checkout@v2 - name : Setup Python uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} cache : \"pip\" - name : Install dependencies run : | python -m pip install --upgrade pip setuptools wheel pip install -r requirements.txt [ Line 10 ] : This line tells us that we want to test the installation on both Ubuntu and Windows Latest version. [ Line 11 ] : This line tells us that we want to test the installation on python version 3.7. [ Lines 21 - 23 ] : This line tells us that we will run the installation using pip on requirements.txt file.","title":"Github actions"},{"location":"mlops_docs/cicd/github_actions/#test-packages-compatibility","text":"We can use GitHub Actions to test for compatibility on different Operating Systems (OS) such as Windows/Linux. packages_compatibility.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 name : Commit Checks on : [ push , pull_request ] jobs : check_code : runs-on : ${{ matrix.os }} strategy : fail-fast : true matrix : os : [ ubuntu-latest , windows-latest ] python-version : [ 3.7 ] steps : - name : Checkout code uses : actions/checkout@v2 - name : Setup Python uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} cache : \"pip\" - name : Install dependencies run : | python -m pip install --upgrade pip setuptools wheel pip install -r requirements.txt [ Line 10 ] : This line tells us that we want to test the installation on both Ubuntu and Windows Latest version. [ Line 11 ] : This line tells us that we want to test the installation on python version 3.7. [ Lines 21 - 23 ] : This line tells us that we will run the installation using pip on requirements.txt file.","title":"Test Packages Compatibility"},{"location":"mlops_docs/cloud/gcp/","text":"GCP Bucket This is a tutorial for myself on how to setup GCP Bucket and use python to upload/download files. These are references here 2 . Create GCP Bucket We first create GCP Bucket here by following the steps here 1 . Python Google Cloud Storage Install the Cloud Client Libraries for Python for an individual API like Cloud Storage !pip install --upgrade google-cloud-storage Install Cloud SDK Install Cloud SDK which can be used to access Cloud Storage services from the command line and then do gcloud auth application-default login . Note that this command generates credentials for client libraries. The steps are detailed here 3 . After installation, we need to install gcloud . !pip install gcloud You can also install Cloud SDK in command line: # Windows ( New-Object Net.WebClient ) .DownloadFile ( \"https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe\" , \" $env :Temp\\GoogleCloudSDKInstaller.exe\" ) & $env :Temp \\G oogleCloudSDKInstaller.exe Follow the prompts to install the Cloud SDK. Note it will also ask you to set the default project. You can choose the project you want to use. Setup Service Account We open cmd prompt, cd to the directory where you are working on, then type gcloud auth login . Since I do not have a service account, we follow this link 4 and follow the steps using either the Cloud Consoler or Command Line. (I prefer the Cloud Console). The documentation is clear and you just need to follow the steps. Create a service account key by the following: 1. In the Cloud Console, click the email address for the service account that you created. 2. Click Keys. 3. Click Add key, then click Create new key. 4. Click Create. A JSON key file is downloaded to your computer. 5. Click Close. Setup Authenticated Environment Now you have a json file from previous step. Put the json file in a folder. Then everytime you start a terminal or new window, you can use $env :GOOGLE_APPLICATION_CREDENTIALS = \" $PATH$TO$JSON \" It seems a hassle to type the command everytime. May look into this link 5 and this 6 to see how to setup the environment. Upload and Download Files from gcloud import storage def return_bucket ( project_id : str ) -> List : \"\"\"Return a list of buckets for a given project. Args: project_id (str): The project id. Returns: List: A list of buckets. \"\"\" storage_client = storage . Client ( project = project_id ) buckets = list ( storage_client . list_buckets ()) return buckets def upload_to_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> str : \"\"\"Uploads a file to the bucket and returns the public url. Args: source_file_name (str): The file in local that you want to upload. destination_blob_name (str): The name of the file in the bucket. To include full path. bucket_name (str): The name of the bucket. \"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . upload_from_filename ( source_file_name ) print ( f \"file { source_file_name } uploaded to bucket { bucket_name } successfully!\" ) return blob . public_url def download_from_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> None : \"\"\"Download file from GCP bucket. Just do the opposite of upload_to_bucket.\"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . download_to_filename ( source_file_name ) if __name__ == \"__main__\" : PROJECT_ID = \"Your Project ID\" BUCKET_NAME = \"Bucket Name\" SOURCE_FILE_NAME = \"Source File Name stored Locally\" DESTINATION_BLOB_NAME = \"Destination File Name in GCP Bucket\" upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) download_from_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) If you want to mass upload or download, you just need to create a loop as such: for file in os . listdir ( path ): upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) Creating GCP Buckets \u21a9 How to write files from Local to GCP using Python \u21a9 Install Cloud SDK \u21a9 Service Account \u21a9 How to upload a file to Google Cloud Storage on Python 3? \u21a9 Setting GOOGLE_APPLICATION_CREDENTIALS for BigQuery Python CLI \u21a9","title":"Gcp"},{"location":"mlops_docs/cloud/gcp/#gcp-bucket","text":"This is a tutorial for myself on how to setup GCP Bucket and use python to upload/download files. These are references here 2 .","title":"GCP Bucket"},{"location":"mlops_docs/cloud/gcp/#create-gcp-bucket","text":"We first create GCP Bucket here by following the steps here 1 .","title":"Create GCP Bucket"},{"location":"mlops_docs/cloud/gcp/#python-google-cloud-storage","text":"Install the Cloud Client Libraries for Python for an individual API like Cloud Storage !pip install --upgrade google-cloud-storage","title":"Python Google Cloud Storage"},{"location":"mlops_docs/cloud/gcp/#install-cloud-sdk","text":"Install Cloud SDK which can be used to access Cloud Storage services from the command line and then do gcloud auth application-default login . Note that this command generates credentials for client libraries. The steps are detailed here 3 . After installation, we need to install gcloud . !pip install gcloud You can also install Cloud SDK in command line: # Windows ( New-Object Net.WebClient ) .DownloadFile ( \"https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe\" , \" $env :Temp\\GoogleCloudSDKInstaller.exe\" ) & $env :Temp \\G oogleCloudSDKInstaller.exe Follow the prompts to install the Cloud SDK. Note it will also ask you to set the default project. You can choose the project you want to use.","title":"Install Cloud SDK"},{"location":"mlops_docs/cloud/gcp/#setup-service-account","text":"We open cmd prompt, cd to the directory where you are working on, then type gcloud auth login . Since I do not have a service account, we follow this link 4 and follow the steps using either the Cloud Consoler or Command Line. (I prefer the Cloud Console). The documentation is clear and you just need to follow the steps. Create a service account key by the following: 1. In the Cloud Console, click the email address for the service account that you created. 2. Click Keys. 3. Click Add key, then click Create new key. 4. Click Create. A JSON key file is downloaded to your computer. 5. Click Close.","title":"Setup Service Account"},{"location":"mlops_docs/cloud/gcp/#setup-authenticated-environment","text":"Now you have a json file from previous step. Put the json file in a folder. Then everytime you start a terminal or new window, you can use $env :GOOGLE_APPLICATION_CREDENTIALS = \" $PATH$TO$JSON \" It seems a hassle to type the command everytime. May look into this link 5 and this 6 to see how to setup the environment.","title":"Setup Authenticated Environment"},{"location":"mlops_docs/cloud/gcp/#upload-and-download-files","text":"from gcloud import storage def return_bucket ( project_id : str ) -> List : \"\"\"Return a list of buckets for a given project. Args: project_id (str): The project id. Returns: List: A list of buckets. \"\"\" storage_client = storage . Client ( project = project_id ) buckets = list ( storage_client . list_buckets ()) return buckets def upload_to_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> str : \"\"\"Uploads a file to the bucket and returns the public url. Args: source_file_name (str): The file in local that you want to upload. destination_blob_name (str): The name of the file in the bucket. To include full path. bucket_name (str): The name of the bucket. \"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . upload_from_filename ( source_file_name ) print ( f \"file { source_file_name } uploaded to bucket { bucket_name } successfully!\" ) return blob . public_url def download_from_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> None : \"\"\"Download file from GCP bucket. Just do the opposite of upload_to_bucket.\"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . download_to_filename ( source_file_name ) if __name__ == \"__main__\" : PROJECT_ID = \"Your Project ID\" BUCKET_NAME = \"Bucket Name\" SOURCE_FILE_NAME = \"Source File Name stored Locally\" DESTINATION_BLOB_NAME = \"Destination File Name in GCP Bucket\" upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) download_from_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) If you want to mass upload or download, you just need to create a loop as such: for file in os . listdir ( path ): upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) Creating GCP Buckets \u21a9 How to write files from Local to GCP using Python \u21a9 Install Cloud SDK \u21a9 Service Account \u21a9 How to upload a file to Google Cloud Storage on Python 3? \u21a9 Setting GOOGLE_APPLICATION_CREDENTIALS for BigQuery Python CLI \u21a9","title":"Upload and Download Files"},{"location":"mlops_docs/config/config_management/","text":"Config Management This post is heavily inspired by Pydra - Pydantic and Hydra for configuration management of model training experiments . Hydra Pros Let's look at an example of a Yaml based configuration with no involvement of python object configs/config.yaml defaults : - _self_ - model : base - stores : base - optimizer : base general : num_classes : 10 device : \"cpu\" project_name : \"peekingduck\" debug : true seed : 1992 hydra : run : dir : \"outputs/${general.project_name}/${stores.unique_id}\" # in sync with stores configs/model/base.yaml 1 2 3 4 5 model_name : \"resnet18\" pretrained : True in_chans : 3 num_classes : ${general.num_classes} global_pool : \"avg\" configs/stores/base.yaml 1 2 3 4 project_name : ${general.project_name} unique_id : ${now:%Y%m%d_%H%M%S} # in sync with hydra output dir logs_dir : !!python/object/apply:pathlib.PosixPath [ \"\" ] model_artifacts_dir : !!python/object/apply:pathlib.PosixPath [ \"\" ] configs/optimizer/base.yaml 1 2 3 4 5 6 7 8 optimizer_params : optimizer : \"AdamW\" optimizer_params : lr : 0.0003 # bs: 32 -> lr = 3e-4 betas : [ 0.9 , 0.999 ] amsgrad : False weight_decay : 0.000001 eps : 0.00000001 main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import logging from dataclasses import dataclass from typing import List , Optional import hydra from hydra.core.config_store import ConfigStore from hydra.core.hydra_config import HydraConfig from omegaconf import MISSING , DictConfig , OmegaConf LOG = logging . getLogger ( __name__ ) @hydra . main ( version_base = None , config_path = \"configs\" , config_name = \"config\" ) def run ( config : DictConfig ) -> None : \"\"\"Run the main function.\"\"\" LOG . info ( \"Type of config is: %s \" , type ( config )) LOG . info ( \"Merged Yaml: \\n %s \" , OmegaConf . to_yaml ( config )) LOG . info ( HydraConfig . get () . job . name ) if __name__ == \"__main__\" : run () The @hydra.main decorator is used to initialize the hydra application. We specify config_path to tell hydra where to look for the base configuration files. We also specify config_name to tell hydra which file is the main controller of the hierarchy. In this case, it is config.yaml . One highlight is we can override this configuration file with command line arguments. So if you tend to have many config folders, then this can come in handy. But one major benefit I see is you can change config_name easily, that is because you may have multiple main config files for various experiments. $ python main.py --config_name config_mnist.yaml $ python main.py --config_name config_cifar10.yaml Once you load the configuration, you can access the configuration values using the dot notation. This is because config is loaded as OmegaConf 's DictConfig object. They inherit from dict and MutableMapping so you can access the values using the dot notation access pattern. This is better than using config[\"general\"][\"num_classes\"] because it is more readable and less error prone, and also allow you to manipulate the object. Allow command line overrides. For example, you can override the model_name from model by $ python main.py model.model_name = resnet50 and the model_name will change during runtime. This is very useful when you want to run multiple experiments with different configurations. You won't need to create multiple config files for each experiment just to change a single/few value(s). A natural follow-up question is about persistence. If I can easily override the configuration, then how do I make sure the configuration is saved somewhere? Versioning the configs is just as important as versioning your code in Machine Learning. The highlight here is Hydra also saves the final configuration to an output folder. By default, it is stored in outputs/YYYY-MM-DD/HH-MM-SS/.hydra/config.yaml . Now you can revert back to this exact run by specifying the directory. $ python main.py --config_path outputs/2022-01-12/12-00-00/.hydra/config.yaml to recover your run. Here YYYY-MM-DD/HH-MM-SS is like your unique run_id for each run. You can also change it as follows in config.yaml : configs/config.yaml 1 2 3 hydra : run : dir : \"outputs/${general.project_name}/${stores.unique_id}\" # in sync with stores One more good to have feature is overriding hydra's own default settings, such as the job and run settings. You can either do it via config.yaml or manually create a folder called hydra and put the specifics inside. See here for an example. Multi-run. Hydra allows you to run multiple experiments in parallel. This is very useful when you want to run multiple experiments with different configurations. You can specify the number of runs and the configuration to use. See here for more examples. $ python main.py model.model_name = resnet18,resnet50 --multirun This will run two experiments in parallel, one with resnet18 and the other with resnet50 . We see the true power of it if you want to do hyperparameter search, where you want to sweep over multiple values for a single parameter. Now in the same directory, an multirun folder will be created and inside it, you will see two runs, one with resnet18 and the other with resnet50 . Each process is indexed by an integer. For example, resnet18 is indexed by 0 and resnet50 is indexed by 1 . It is also worth noting that since the hydra's creator is from facebook, it is very easy to integrate with PyTorch's distributed trainings . Structured Config , this is what Hydra call when your config is complex enough to warrant object representations. For example, our config.yaml is currently just a Yaml representation. But under the hood, you can think of it as composed of model , optimizer , store and general objects. For each of these objects, you can define its own schema. This is very useful when you want to validate the configuration. In hydra, you can define the schema using dataclasses . configs/base.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from dataclasses import dataclass from typing import Dict , Any from pathlib import Path @dataclass class Model : model_name : str pretrained : bool in_chans : int num_classes : int global_pool : str @dataclass class Optimizer : optimizer : str optimizer_params : Dict [ str , Any ] @dataclass class Stores : project_name : str unique_id : int logs_dir : Path model_artifacts_dir : Path @dataclass class Config : model : Model optimizer : Optimizer stores : Stores main_structured.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import logging import hydra from configs.base import Config , Model , Optimizer , Stores from hydra.core.config_store import ConfigStore from hydra.core.hydra_config import HydraConfig from omegaconf import OmegaConf LOG = logging . getLogger ( __name__ ) cs = ConfigStore . instance () cs . store ( name = \"base_config\" , node = Config ) cs . store ( name = \"model\" , node = Model ) cs . store ( name = \"optimizer\" , node = Optimizer ) cs . store ( name = \"stores\" , node = Stores ) @hydra . main ( version_base = None , config_path = \"configs\" , config_name = \"config\" ) def run ( config : Config ) -> None : \"\"\"Run the main function.\"\"\" LOG . info ( \"Type of config is: %s \" , type ( config )) LOG . info ( \"Merged Yaml: \\n %s \" , OmegaConf . to_yaml ( config )) LOG . info ( HydraConfig . get () . job . name ) config_obj = OmegaConf . to_object ( config ) LOG . info ( \"Type of config is: %s \" , type ( config_obj )) if __name__ == \"__main__\" : run () More details here . You can find how groups can be used to indicate inheritance. The dependency injection here is merely a change of the type of the config argument, from DictConfig to Config . The rest of the code remains the same. The command line arguments are still the same. Having dataclass representation also offers more flexibility from manipulating the object to type hint. But validation still remains a problem as using __post_init__ method is not well supported when working with hydra. However, you can decouple the usage of dataclass from hydras' dependency injection. For example, you can load the config from hydra and instantiate through the dataclass without the use of ConfigStore . This idea is made better when used together with Pydantic since it offers validation and serialization. Interpolation: This is an extremely highlighted feature of hydra. For example, we have num_classes defined under the general schema. However, this parameter is also used in the model schema, defining the number of output neurons. It can also be used in the dataset schema (not shown here), where we need to know the number of classes to create certain class mapping. Repeatedly defining the same parameter over different config schema is prone to mistake. This is like hardcoding the same value NUM_CLASSES in different python scripts. Here we use the idea of polymorphism to define the num_classes in the general schema and use interpolation to inject this all around with ${general.num_classes} . Cons Serializing/Deserializing canonical or complex python objects are not well supported. In earlier versions, objects like pathlib.Path are not supported. Structured config is only limited to dataclass . This means that you cannot create your own custom abstraction. For example, you cannot create a Model class without invoking dataclass decorator, and still be able to interact with hydra. No validation support. This is a problem since what you define in the schema is not validated. For example, you can define num_classes as a int but user passes in a str of \"10\" instead of 10 , but hydra will not complain. This means that you have to do validation yourself (i.e. do checks all over the application/business logic code). Interpolation is really good, but the inability to simple manipulation over it caused a lot of complaints . Like if you defined a learning rate as lr: 0.001 and you want to multiply it by 10 in another config file, you cannot do 10 * ${lr} . Instantiating You can also instantiate objects with hydra. Composition Order By default, Hydra 1.1 appends _self_ to the end of the Defaults List. This behavior is new in Hydra 1.1 and different from previous Hydra versions. As such Hydra 1.1 issues a warning if _self_ is not specified in the primary config, asking you to add _self_ and thus indicate the desired composition order. To address the warning while maintaining the new behavior, append _self_ to the end of the Defaults List. Note that in some cases it may instead be desirable to add _self_ directly after the schema and before other Defaults List elements. See Composition Order for more information. Pydantic Pros Able to serialize and deserialize objects to and from DICT, JSON, YAML, and other formats. For example, the following code will serialize a Dict object to Pydantics' Model object. It can also convert back to Dict object. class Model ( BaseModel ): model_name : str = \"resnet18\" pretrained : bool = True in_chans : conint ( ge = 1 ) = 3 # in_channels must be greater than or equal to 1 num_classes : conint ( ge = 1 ) = 2 global_pool : str = \"avg\" @validator ( \"global_pool\" ) def validate_global_pool ( cls , global_pool : str ) -> str : if global_pool not in [ \"avg\" , \"max\" ]: raise ValueError ( \"global_pool must be avg or max\" ) return global_pool model_config_dict = { \"model_name\" : \"resnet18\" , \"pretrained\" : True , \"in_chans\" : 3 , \"num_classes\" : 1000 , \"global_pool\" : \"avg\" , } model = Model ( ** model_config_dict ) print ( model ) assert model . dict () == model_config_dict Validation of data types and values. For a large and complex configuration, you either validate the sanity of config at the config level, or check at the code level (i.e. sprinkled throughout your codebase). Constrained types model = Model ( model_name = \"resnet18\" , pretrained = True , in_chans = 0 , num_classes = 2 , global_pool = \"avg\" , ) This will raise an error because in_chans is less than 1. Pydantic offers a wide range of constrained types out of the box for you to use. If that is not enough, then the custom validators can be used to validate the data with custom needs. Custom Validators model = Model ( model_name = \"resnet18\" , pretrained = True , in_chans = 3 , num_classes = 2 , global_pool = \"average\" , ) This will raise an error because global_pool is not avg or max . We implemented this custom checks in the validate_global_pool method where we decorated it with @validator . References Hydra Pydantic Pydra - Pydantic and Hydra for configuration management of model training experiments","title":"Config Management"},{"location":"mlops_docs/config/config_management/#config-management","text":"This post is heavily inspired by Pydra - Pydantic and Hydra for configuration management of model training experiments .","title":"Config Management"},{"location":"mlops_docs/config/config_management/#hydra","text":"","title":"Hydra"},{"location":"mlops_docs/config/config_management/#pros","text":"Let's look at an example of a Yaml based configuration with no involvement of python object configs/config.yaml defaults : - _self_ - model : base - stores : base - optimizer : base general : num_classes : 10 device : \"cpu\" project_name : \"peekingduck\" debug : true seed : 1992 hydra : run : dir : \"outputs/${general.project_name}/${stores.unique_id}\" # in sync with stores configs/model/base.yaml 1 2 3 4 5 model_name : \"resnet18\" pretrained : True in_chans : 3 num_classes : ${general.num_classes} global_pool : \"avg\" configs/stores/base.yaml 1 2 3 4 project_name : ${general.project_name} unique_id : ${now:%Y%m%d_%H%M%S} # in sync with hydra output dir logs_dir : !!python/object/apply:pathlib.PosixPath [ \"\" ] model_artifacts_dir : !!python/object/apply:pathlib.PosixPath [ \"\" ] configs/optimizer/base.yaml 1 2 3 4 5 6 7 8 optimizer_params : optimizer : \"AdamW\" optimizer_params : lr : 0.0003 # bs: 32 -> lr = 3e-4 betas : [ 0.9 , 0.999 ] amsgrad : False weight_decay : 0.000001 eps : 0.00000001 main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import logging from dataclasses import dataclass from typing import List , Optional import hydra from hydra.core.config_store import ConfigStore from hydra.core.hydra_config import HydraConfig from omegaconf import MISSING , DictConfig , OmegaConf LOG = logging . getLogger ( __name__ ) @hydra . main ( version_base = None , config_path = \"configs\" , config_name = \"config\" ) def run ( config : DictConfig ) -> None : \"\"\"Run the main function.\"\"\" LOG . info ( \"Type of config is: %s \" , type ( config )) LOG . info ( \"Merged Yaml: \\n %s \" , OmegaConf . to_yaml ( config )) LOG . info ( HydraConfig . get () . job . name ) if __name__ == \"__main__\" : run () The @hydra.main decorator is used to initialize the hydra application. We specify config_path to tell hydra where to look for the base configuration files. We also specify config_name to tell hydra which file is the main controller of the hierarchy. In this case, it is config.yaml . One highlight is we can override this configuration file with command line arguments. So if you tend to have many config folders, then this can come in handy. But one major benefit I see is you can change config_name easily, that is because you may have multiple main config files for various experiments. $ python main.py --config_name config_mnist.yaml $ python main.py --config_name config_cifar10.yaml Once you load the configuration, you can access the configuration values using the dot notation. This is because config is loaded as OmegaConf 's DictConfig object. They inherit from dict and MutableMapping so you can access the values using the dot notation access pattern. This is better than using config[\"general\"][\"num_classes\"] because it is more readable and less error prone, and also allow you to manipulate the object. Allow command line overrides. For example, you can override the model_name from model by $ python main.py model.model_name = resnet50 and the model_name will change during runtime. This is very useful when you want to run multiple experiments with different configurations. You won't need to create multiple config files for each experiment just to change a single/few value(s). A natural follow-up question is about persistence. If I can easily override the configuration, then how do I make sure the configuration is saved somewhere? Versioning the configs is just as important as versioning your code in Machine Learning. The highlight here is Hydra also saves the final configuration to an output folder. By default, it is stored in outputs/YYYY-MM-DD/HH-MM-SS/.hydra/config.yaml . Now you can revert back to this exact run by specifying the directory. $ python main.py --config_path outputs/2022-01-12/12-00-00/.hydra/config.yaml to recover your run. Here YYYY-MM-DD/HH-MM-SS is like your unique run_id for each run. You can also change it as follows in config.yaml : configs/config.yaml 1 2 3 hydra : run : dir : \"outputs/${general.project_name}/${stores.unique_id}\" # in sync with stores One more good to have feature is overriding hydra's own default settings, such as the job and run settings. You can either do it via config.yaml or manually create a folder called hydra and put the specifics inside. See here for an example. Multi-run. Hydra allows you to run multiple experiments in parallel. This is very useful when you want to run multiple experiments with different configurations. You can specify the number of runs and the configuration to use. See here for more examples. $ python main.py model.model_name = resnet18,resnet50 --multirun This will run two experiments in parallel, one with resnet18 and the other with resnet50 . We see the true power of it if you want to do hyperparameter search, where you want to sweep over multiple values for a single parameter. Now in the same directory, an multirun folder will be created and inside it, you will see two runs, one with resnet18 and the other with resnet50 . Each process is indexed by an integer. For example, resnet18 is indexed by 0 and resnet50 is indexed by 1 . It is also worth noting that since the hydra's creator is from facebook, it is very easy to integrate with PyTorch's distributed trainings . Structured Config , this is what Hydra call when your config is complex enough to warrant object representations. For example, our config.yaml is currently just a Yaml representation. But under the hood, you can think of it as composed of model , optimizer , store and general objects. For each of these objects, you can define its own schema. This is very useful when you want to validate the configuration. In hydra, you can define the schema using dataclasses . configs/base.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from dataclasses import dataclass from typing import Dict , Any from pathlib import Path @dataclass class Model : model_name : str pretrained : bool in_chans : int num_classes : int global_pool : str @dataclass class Optimizer : optimizer : str optimizer_params : Dict [ str , Any ] @dataclass class Stores : project_name : str unique_id : int logs_dir : Path model_artifacts_dir : Path @dataclass class Config : model : Model optimizer : Optimizer stores : Stores main_structured.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import logging import hydra from configs.base import Config , Model , Optimizer , Stores from hydra.core.config_store import ConfigStore from hydra.core.hydra_config import HydraConfig from omegaconf import OmegaConf LOG = logging . getLogger ( __name__ ) cs = ConfigStore . instance () cs . store ( name = \"base_config\" , node = Config ) cs . store ( name = \"model\" , node = Model ) cs . store ( name = \"optimizer\" , node = Optimizer ) cs . store ( name = \"stores\" , node = Stores ) @hydra . main ( version_base = None , config_path = \"configs\" , config_name = \"config\" ) def run ( config : Config ) -> None : \"\"\"Run the main function.\"\"\" LOG . info ( \"Type of config is: %s \" , type ( config )) LOG . info ( \"Merged Yaml: \\n %s \" , OmegaConf . to_yaml ( config )) LOG . info ( HydraConfig . get () . job . name ) config_obj = OmegaConf . to_object ( config ) LOG . info ( \"Type of config is: %s \" , type ( config_obj )) if __name__ == \"__main__\" : run () More details here . You can find how groups can be used to indicate inheritance. The dependency injection here is merely a change of the type of the config argument, from DictConfig to Config . The rest of the code remains the same. The command line arguments are still the same. Having dataclass representation also offers more flexibility from manipulating the object to type hint. But validation still remains a problem as using __post_init__ method is not well supported when working with hydra. However, you can decouple the usage of dataclass from hydras' dependency injection. For example, you can load the config from hydra and instantiate through the dataclass without the use of ConfigStore . This idea is made better when used together with Pydantic since it offers validation and serialization. Interpolation: This is an extremely highlighted feature of hydra. For example, we have num_classes defined under the general schema. However, this parameter is also used in the model schema, defining the number of output neurons. It can also be used in the dataset schema (not shown here), where we need to know the number of classes to create certain class mapping. Repeatedly defining the same parameter over different config schema is prone to mistake. This is like hardcoding the same value NUM_CLASSES in different python scripts. Here we use the idea of polymorphism to define the num_classes in the general schema and use interpolation to inject this all around with ${general.num_classes} .","title":"Pros"},{"location":"mlops_docs/config/config_management/#cons","text":"Serializing/Deserializing canonical or complex python objects are not well supported. In earlier versions, objects like pathlib.Path are not supported. Structured config is only limited to dataclass . This means that you cannot create your own custom abstraction. For example, you cannot create a Model class without invoking dataclass decorator, and still be able to interact with hydra. No validation support. This is a problem since what you define in the schema is not validated. For example, you can define num_classes as a int but user passes in a str of \"10\" instead of 10 , but hydra will not complain. This means that you have to do validation yourself (i.e. do checks all over the application/business logic code). Interpolation is really good, but the inability to simple manipulation over it caused a lot of complaints . Like if you defined a learning rate as lr: 0.001 and you want to multiply it by 10 in another config file, you cannot do 10 * ${lr} .","title":"Cons"},{"location":"mlops_docs/config/config_management/#instantiating","text":"You can also instantiate objects with hydra.","title":"Instantiating"},{"location":"mlops_docs/config/config_management/#composition-order","text":"By default, Hydra 1.1 appends _self_ to the end of the Defaults List. This behavior is new in Hydra 1.1 and different from previous Hydra versions. As such Hydra 1.1 issues a warning if _self_ is not specified in the primary config, asking you to add _self_ and thus indicate the desired composition order. To address the warning while maintaining the new behavior, append _self_ to the end of the Defaults List. Note that in some cases it may instead be desirable to add _self_ directly after the schema and before other Defaults List elements. See Composition Order for more information.","title":"Composition Order"},{"location":"mlops_docs/config/config_management/#pydantic","text":"","title":"Pydantic"},{"location":"mlops_docs/config/config_management/#pros_1","text":"Able to serialize and deserialize objects to and from DICT, JSON, YAML, and other formats. For example, the following code will serialize a Dict object to Pydantics' Model object. It can also convert back to Dict object. class Model ( BaseModel ): model_name : str = \"resnet18\" pretrained : bool = True in_chans : conint ( ge = 1 ) = 3 # in_channels must be greater than or equal to 1 num_classes : conint ( ge = 1 ) = 2 global_pool : str = \"avg\" @validator ( \"global_pool\" ) def validate_global_pool ( cls , global_pool : str ) -> str : if global_pool not in [ \"avg\" , \"max\" ]: raise ValueError ( \"global_pool must be avg or max\" ) return global_pool model_config_dict = { \"model_name\" : \"resnet18\" , \"pretrained\" : True , \"in_chans\" : 3 , \"num_classes\" : 1000 , \"global_pool\" : \"avg\" , } model = Model ( ** model_config_dict ) print ( model ) assert model . dict () == model_config_dict Validation of data types and values. For a large and complex configuration, you either validate the sanity of config at the config level, or check at the code level (i.e. sprinkled throughout your codebase). Constrained types model = Model ( model_name = \"resnet18\" , pretrained = True , in_chans = 0 , num_classes = 2 , global_pool = \"avg\" , ) This will raise an error because in_chans is less than 1. Pydantic offers a wide range of constrained types out of the box for you to use. If that is not enough, then the custom validators can be used to validate the data with custom needs. Custom Validators model = Model ( model_name = \"resnet18\" , pretrained = True , in_chans = 3 , num_classes = 2 , global_pool = \"average\" , ) This will raise an error because global_pool is not avg or max . We implemented this custom checks in the validate_global_pool method where we decorated it with @validator .","title":"Pros"},{"location":"mlops_docs/config/config_management/#references","text":"Hydra Pydantic Pydra - Pydantic and Hydra for configuration management of model training experiments","title":"References"},{"location":"mlops_docs/developing/03.logging/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Logging Guide by Hongnan Gao Logging Setting Up This section will include previous lessons' setup code. Packaging and Setup This setup process can be found in lesson \\(1\\) . ! mkdir - p reighns % cd reighns /content/reighns from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () ! pip3 install - q virtualenv # install virtualenv package ! virtualenv venv_reighns # create the virtual environment # !source venv_reighns/bin/activate # this activates the vm in macOS, command is different for different OS. ! source venv_reighns / bin / activate ; python3 - m pip install -- upgrade pip setuptools wheel # upgrade pip so we download the latest package wheels |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.8 MB 18.4 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 461 kB 66.0 MB/s created virtual environment CPython3.7.13.final.0-64 in 1081ms creator CPython3Posix(dest=/content/reighns/venv_reighns, clear=False, no_vcs_ignore=False, global=False) seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv) added seed packages: pip==22.0.4, setuptools==62.1.0, wheel==0.37.1 activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator Requirement already satisfied: pip in ./venv_reighns/lib/python3.7/site-packages (22.0.4) Requirement already satisfied: setuptools in ./venv_reighns/lib/python3.7/site-packages (62.1.0) Requirement already satisfied: wheel in ./venv_reighns/lib/python3.7/site-packages (0.37.1) %% writefile { BASE_DIR } / requirements . txt numpy == 1.21.6 pandas == 1.3.5 matplotlib == 3.5.1 scikit - learn == 1.0.2 onnx == 1.11.0 onnxmltools == 1.11.0 skl2onnx == 1.11.1 holidays == 0.13 pytest == 7.1.1 pylint == 2.13.5 black == 22.3.0 flake8 == 4.0.1 Writing /content/reighns/requirements.txt %% writefile { BASE_DIR } / setup . py # setup.py # Setup installation for the application from pathlib import Path from setuptools import find_namespace_packages , setup BASE_DIR = Path ( __file__ ) . parent # Load packages from requirements.txt with open ( Path ( BASE_DIR , \"requirements.txt\" )) as file : required_packages = [ ln . strip () for ln in file . readlines ()] test_packages = [ \"coverage[toml]==6.0.2\" , \"great-expectations==0.13.14\" , \"pytest==6.0.2\" , \"pytest-cov==2.10.1\" , ] dev_packages = [ \"black==20.8b1\" , \"flake8==3.8.3\" , \"isort==5.5.3\" , \"jupyterlab==2.2.8\" , \"pre-commit==2.11.1\" , ] docs_packages = [ \"mkdocs==1.1.2\" , \"mkdocs-material==7.2.3\" , \"mkdocstrings==0.15.2\" , ] setup ( name = \"hongnan\" , version = \"0.1\" , license = \"MIT\" , description = \"MLOps guide.\" , author = \"Hongnan Gao\" , author_email = \"hongnang.sph@gmail.com\" , url = \"\" , keywords = [ \"machine-learning\" , \"artificial-intelligence\" ], classifiers = [ \"Development Status :: 3 - Alpha\" , \"Intended Audience :: Developers\" , \"Topic :: Software Development :: Build Tools\" , \"License :: OSI Approved :: MIT License\" , \"Programming Language :: Python :: 3\" , ], python_requires = \">=3.7\" , packages = find_namespace_packages (), install_requires = [ required_packages ], extras_require = { \"test\" : test_packages , \"dev\" : test_packages + dev_packages + docs_packages , \"docs\" : docs_packages , }, entry_points = { \"console_scripts\" : [ \"reighns = reighns.main:app\" , ], }, ) Writing /content/reighns/setup.py ! source venv_reighns / bin / activate ; python3 - m pip install - q - e . Preparing metadata (setup.py) ... done \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 15.7/15.7 MB 74.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.3/11.3 MB 83.2 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.2/11.2 MB 59.7 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24.8/24.8 MB 15.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.8/12.8 MB 83.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 302.4/302.4 KB 29.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 276.4/276.4 KB 22.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 172.1/172.1 KB 15.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 297.0/297.0 KB 28.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 437.6/437.6 KB 36.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 62.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.1/64.1 KB 8.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 843.7/843.7 KB 54.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 96.6/96.6 KB 12.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 69.7/69.7 KB 9.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.1/42.1 KB 5.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 247.7/247.7 KB 26.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 47.9/47.9 KB 6.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 930.9/930.9 KB 55.2 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40.8/40.8 KB 4.7 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 63.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.3/4.3 MB 41.9 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.5/98.5 KB 6.9 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 41.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 503.5/503.5 KB 28.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 250.7/250.7 KB 19.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 86.9/86.9 KB 7.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 103.4/103.4 KB 11.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 KB 6.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.7/98.7 KB 13.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 307.0/307.0 KB 29.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.1/38.1 MB 13.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.8/78.8 KB 8.9 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 57.8/57.8 KB 6.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 75.2/75.2 KB 9.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.4/5.4 MB 88.5 MB/s eta 0:00:00 Preparing metadata (setup.py) ... done Building wheel for pymeeus (setup.py) ... done Organization ! mkdir - p config CONFIG_DIR = Path . joinpath ( BASE_DIR , \"config\" ) %% writefile { CONFIG_DIR } / config . py import logging import sys from dataclasses import dataclass from pathlib import Path from typing import List , Optional # Creating Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) SRC_DIR = Path ( BASE_DIR , \"src\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) TEST_DIR = Path ( BASE_DIR , \"tests\" ) ## Local stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) PROCESSED_DATA = Path ( DATA_DIR , \"processed\" ) ## Create dirs for d in [ CONFIG_DIR , LOGS_DIR , DATA_DIR , STORES_DIR , TEST_DIR , MODEL_REGISTRY , RAW_DATA , PROCESSED_DATA , ]: d . mkdir ( parents = True , exist_ok = True ) Writing /content/reighns/config/config.py %% writefile { BASE_DIR } / main . py from config import config Writing /content/reighns/main.py ! python3 main . py # call main to create the folders since it calls config! Intuition Logging is the process of tracking and recording key events that occur in our applications. We want to log events so we can use them to inspect processes, fix issues, etc. They're a whole lot more powerful than print statements because they allow us to send specific pieces of information to specific locations, not to mention custom formatting, shared interface with other Python packages, etc. This makes logging a key proponent in being able to surface insightful information from the internal processes of our application. Components There are a few overarching concepts to be aware of first before we can create and use our loggers. Logger : the main object that emits the log messages from our application. Handler : used for sending log records to a specific location and specifications for that location (name, size, etc.). Formatter : used for style and layout of the log records. There is so much more to logging such as filters, exception logging, etc. but these basics will allows us to do everything we need for our application. Logging Levels The numeric values of logging levels are given in the following table. These are primarily of interest if you want to define your own levels, and need them to have specific values relative to the predefined levels. If you define a level with the same numeric value, it overwrites the predefined value; the predefined name is lost. Level Value DEBUG 10 INFO 20 WARN 30 ERROR 40 FATAL 50 Before we create our specialized, configured logger, let's look at what logged messages even look like by using a very basic configuration. import logging import sys from typing import Optional These are the basic levels of logging, where DEBUG is the lowest priority and CRITICAL is the highest. We defined our logger using basicConfig to emit log messages to our stdout console (we also could've written to any other stream or even a file) and to be sensitive to log messages starting from level DEBUG . This means that all of our logged messages will be displayed since DEBUG is the lowest level. Had we made the level ERROR , then only ERROR and CRITICAL log message would be displayed. (madewithml.com) In our first example, we set the level to be logging.DEBUG and all \\(5\\) messages are logged, as can be seen. # Create super basic logger logging . basicConfig ( stream = sys . stdout , level = logging . DEBUG ) # Logging levels (from lowest to highest priority) logging . debug ( \"Used for debugging your code.\" ) logging . info ( \"Informative messages from your code.\" ) logging . warning ( \"Everything works but there is something to be aware of.\" ) logging . error ( \"There's been a mistake with the process.\" ) logging . critical ( \"There is something terribly wrong and process may terminate.\" ) ERROR:root:There's been a mistake with the process. CRITICAL:root:There is something terribly wrong and process may terminate. In the next example, we set the level to be logging.ERROR , this means all messages lower than error is ignored, as can be seen here! Note, if you are working in google colab, one should factory reset the notebook so that the logger can be refreshed. # Create super basic logger logging . basicConfig ( stream = sys . stdout , level = logging . ERROR ) # Logging levels (from lowest to highest priority) logging . debug ( \"Used for debugging your code.\" ) logging . info ( \"Informative messages from your code.\" ) logging . warning ( \"Everything works but there is something to be aware of.\" ) logging . error ( \"There's been a mistake with the process.\" ) logging . critical ( \"There is something terribly wrong and process may terminate.\" ) ERROR:root:There's been a mistake with the process. CRITICAL:root:There is something terribly wrong and process may terminate. Custom Logger Function We will define a custom logger function for our purpose. If you encounter logger printing the same line multiple times, we should factory reset runtime. import logging import sys from pathlib import Path from typing import Optional We created a logging directory in the section Organization , however, for clarity, we create the LOGS_DIR again below (won't be overwritten). We will send all our logs to this directory. # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () LOGS_DIR = Path ( BASE_DIR , \"logs\" ) LOGS_DIR . mkdir ( parents = True , exist_ok = True ) # Logger def init_logger ( logs_dir : Path , default_level = logging . DEBUG , stream_level = logging . INFO , module_name : Optional [ str ] = None , ) -> logging . Logger : \"\"\"Initialize logger. Args: logs_dir (Path): Path to log directory. default_level (int, optional): Default logging level. Defaults to logging.DEBUG. stream_level (int, optional): Stream logging level. Defaults to logging.INFO. module_name (Optional[str]): Module name to be used in logger. Defaults to None. Returns: logging.Logger: The logger object. Example: >>> import logging >>> import sys >>> from pathlib import Path >>> from typing import Optional >>> # Creating Directories >>> BASE_DIR = Path(\"__file__\").parent.parent.absolute() >>> LOGS_DIR = Path(BASE_DIR, \"logs\") >>> LOGS_DIR.mkdir(parents=True, exist_ok=True) >>> train_logger = init_logger(LOGS_DIR, module_name=\"train\") >>> # Logging levels (from lowest to highest priority) >>> try: >>> train_logger.info(\"I am trying to divide by zero!\") >>> 1 / 0 >>> except ZeroDivisionError as e: >>> train_logger.error(e) # ERROR:root:division by zero >>> train_logger.critical(e, exc_info=True) # Logs error with stack trace \"\"\" if module_name is None : logger = logging . getLogger ( __name__ ) info_log_filepath = Path ( logs_dir , \"info.log\" ) error_log_filepath = Path ( logs_dir , \"error.log\" ) else : # get module name, useful for multi-module logging logger = logging . getLogger ( module_name ) info_log_filepath = Path ( logs_dir , f \" { module_name } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { module_name } _error.log\" ) logger . setLevel ( default_level ) stream_handler = logging . StreamHandler ( stream = sys . stdout ) stream_handler . setLevel ( stream_level ) stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) info_file_handler = logging . FileHandler ( filename = info_log_filepath ) info_file_handler . setLevel ( logging . INFO ) info_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) # add error file handler error_file_handler = logging . FileHandler ( filename = error_log_filepath ) error_file_handler . setLevel ( logging . ERROR ) error_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) logger . addHandler ( stream_handler ) logger . addHandler ( info_file_handler ) logger . addHandler ( error_file_handler ) logger . propagate = False return logger train_logger = init_logger ( logs_dir = LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"train\" ) # Logging levels (from lowest to highest priority) train_logger . debug ( \"Used for debugging your code.\" ) train_logger . info ( \"Informative messages from your code.\" ) train_logger . warning ( \"Everything works but there is something to be aware of.\" ) train_logger . error ( \"There's been a mistake with the process.\" ) train_logger . critical ( \"There is something terribly wrong and process may terminate.\" ) 2022-04-27 09:55:29 - train - INFO: Informative messages from your code. 2022-04-27 09:55:29 - train - WARNING: Everything works but there is something to be aware of. 2022-04-27 09:55:29 - train - ERROR: There's been a mistake with the process. 2022-04-27 09:55:29 - train - CRITICAL: There is something terribly wrong and process may terminate. Lo and behold, the train_logger is behaving properly: - console level: all messages above INFO are printed. - info file: all messages above INFO are logged in the file. - error file: all messages above DEBUG are logged in the file, in particular, messages of lower priority like .info and .debug are not logged. The reason of having \\(2\\) log files is that one file (info) logs almost everything, while the other (error) only logs the error messages etc. This avoids clutter and eases developer to pin-point errors when reviewing the code. For completeness sake, we define another logger called inference_logger and see that it behaves the same, except for the fact that it is logging messages for another module. inference_logger = init_logger ( logs_dir = LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"inference\" ) # Logging levels (from lowest to highest priority) inference_logger . debug ( \"Used for debugging your code.\" ) inference_logger . info ( \"Informative messages from your code.\" ) inference_logger . warning ( \"Everything works but there is something to be aware of.\" ) inference_logger . error ( \"There's been a mistake with the process.\" ) inference_logger . critical ( \"There is something terribly wrong and process may terminate.\" ) 2022-04-27 09:55:33 - inference - INFO: Informative messages from your code. 2022-04-27 09:55:33 - inference - WARNING: Everything works but there is something to be aware of. 2022-04-27 09:55:33 - inference - ERROR: There's been a mistake with the process. 2022-04-27 09:55:33 - inference - CRITICAL: There is something terribly wrong and process may terminate. Example Usage The below small example shows how one can log messages. In particular, in the except clause, we called logging.error(e) to log the error messages and logging.critical(e, exc_info=True) to log both the message and the stack trace. %% writefile { CONFIG_DIR } / config . py import logging import sys from dataclasses import dataclass from pathlib import Path from typing import List , Optional import datetime # Creating Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) SRC_DIR = Path ( BASE_DIR , \"src\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) TEST_DIR = Path ( BASE_DIR , \"tests\" ) ## Local stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) PROCESSED_DATA = Path ( DATA_DIR , \"processed\" ) ## Create dirs for d in [ CONFIG_DIR , LOGS_DIR , DATA_DIR , STORES_DIR , TEST_DIR , MODEL_REGISTRY , RAW_DATA , PROCESSED_DATA , ]: d . mkdir ( parents = True , exist_ok = True ) # Logger def init_logger ( logs_dir : Path , default_level = logging . DEBUG , stream_level = logging . INFO , module_name : Optional [ str ] = None , ) -> logging . Logger : \"\"\"Initialize logger. Args: logs_dir (Path): Path to log directory. default_level (int, optional): Default logging level. Defaults to logging.DEBUG. stream_level (int, optional): Stream logging level. Defaults to logging.INFO. module_name (Optional[str]): Module name to be used in logger. Defaults to None. Returns: logging.Logger: The logger object. Example: >>> import logging >>> import sys >>> from pathlib import Path >>> from typing import Optional >>> # Creating Directories >>> BASE_DIR = Path(\"__file__\").parent.parent.absolute() >>> LOGS_DIR = Path(BASE_DIR, \"logs\") >>> LOGS_DIR.mkdir(parents=True, exist_ok=True) >>> train_logger = init_logger(LOGS_DIR, module_name=\"train\") >>> # Logging levels (from lowest to highest priority) >>> try: >>> train_logger.info(\"I am trying to divide by zero!\") >>> 1 / 0 >>> except ZeroDivisionError as e: >>> train_logger.error(e) # ERROR:root:division by zero >>> train_logger.critical(e, exc_info=True) # Logs error with stack trace \"\"\" datetime_ = datetime . datetime . now () . strftime ( \"%Y-%m- %d _%H-%M-%S\" ) if module_name is None : logger = logging . getLogger ( __name__ ) info_log_filepath = Path ( logs_dir , f \" { datetime_ } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { datetime_ } _error.log\" ) else : # get module name, useful for multi-module logging logger = logging . getLogger ( module_name ) info_log_filepath = Path ( logs_dir , f \" { datetime_ } _ { module_name } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { datetime_ } _ { module_name } _error.log\" ) logger . setLevel ( default_level ) stream_handler = logging . StreamHandler ( stream = sys . stdout ) stream_handler . setLevel ( stream_level ) stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) info_file_handler = logging . FileHandler ( filename = info_log_filepath ) info_file_handler . setLevel ( logging . INFO ) info_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) # add error file handler error_file_handler = logging . FileHandler ( filename = error_log_filepath ) error_file_handler . setLevel ( logging . ERROR ) error_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) logger . addHandler ( stream_handler ) logger . addHandler ( info_file_handler ) logger . addHandler ( error_file_handler ) logger . propagate = False return logger Overwriting /content/reighns/config/config.py %% writefile { BASE_DIR } / main . py import logging from config import config def divide_by_zero ( logger : logging . Logger ): try : logger . info ( \"I am trying to divide by zero!\" ) 1 / 0 except ZeroDivisionError as e : logger . error ( e ) # ERROR:root:division by zero logger . critical ( e , exc_info = True ) # Logs error with stack trace if __name__ == \"__main__\" : train_logger = config . init_logger ( logs_dir = config . LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"train\" ) divide_by_zero ( train_logger ) Overwriting /content/reighns/main.py ! python main . py 2022-04-27 10:15:10 - train - INFO: I am trying to divide by zero! 2022-04-27 10:15:10 - train - ERROR: division by zero 2022-04-27 10:15:10 - train - CRITICAL: division by zero Traceback (most recent call last): File \"main.py\", line 7, in divide_by_zero 1 / 0 ZeroDivisionError: division by zero Workflow Workflow in IDE Workflow in Google Colab %% writefile { CONFIG_DIR } / config . py import logging import sys from dataclasses import dataclass from pathlib import Path from typing import List , Optional # Creating Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) SRC_DIR = Path ( BASE_DIR , \"src\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) TEST_DIR = Path ( BASE_DIR , \"tests\" ) ## Local stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) PROCESSED_DATA = Path ( DATA_DIR , \"processed\" ) ## Create dirs for d in [ CONFIG_DIR , LOGS_DIR , DATA_DIR , STORES_DIR , TEST_DIR , MODEL_REGISTRY , RAW_DATA , PROCESSED_DATA , ]: d . mkdir ( parents = True , exist_ok = True ) # Logger def init_logger ( logs_dir : Path , default_level = logging . DEBUG , stream_level = logging . INFO , module_name : Optional [ str ] = None , ) -> logging . Logger : \"\"\"Initialize logger. Args: logs_dir (Path): Path to log directory. default_level (int, optional): Default logging level. Defaults to logging.DEBUG. stream_level (int, optional): Stream logging level. Defaults to logging.INFO. module_name (Optional[str]): Module name to be used in logger. Defaults to None. Returns: logging.Logger: The logger object. Example: >>> import logging >>> import sys >>> from pathlib import Path >>> from typing import Optional >>> # Creating Directories >>> BASE_DIR = Path(\"__file__\").parent.parent.absolute() >>> LOGS_DIR = Path(BASE_DIR, \"logs\") >>> LOGS_DIR.mkdir(parents=True, exist_ok=True) >>> train_logger = init_logger(LOGS_DIR, module_name=\"train\") >>> # Logging levels (from lowest to highest priority) >>> try: >>> train_logger.info(\"I am trying to divide by zero!\") >>> 1 / 0 >>> except ZeroDivisionError as e: >>> train_logger.error(e) # ERROR:root:division by zero >>> train_logger.critical(e, exc_info=True) # Logs error with stack trace \"\"\" if module_name is None : logger = logging . getLogger ( __name__ ) info_log_filepath = Path ( logs_dir , \"info.log\" ) error_log_filepath = Path ( logs_dir , \"error.log\" ) else : # get module name, useful for multi-module logging logger = logging . getLogger ( module_name ) info_log_filepath = Path ( logs_dir , f \" { module_name } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { module_name } _error.log\" ) logger . setLevel ( default_level ) stream_handler = logging . StreamHandler ( stream = sys . stdout ) stream_handler . setLevel ( stream_level ) stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) info_file_handler = logging . FileHandler ( filename = info_log_filepath ) info_file_handler . setLevel ( logging . INFO ) info_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) # add error file handler error_file_handler = logging . FileHandler ( filename = error_log_filepath ) error_file_handler . setLevel ( logging . ERROR ) error_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) logger . addHandler ( stream_handler ) logger . addHandler ( info_file_handler ) logger . addHandler ( error_file_handler ) logger . propagate = False return logger Overwriting /content/reighns/config/config.py %% writefile { BASE_DIR } / main . py import logging from config import config def divide_by_zero ( logger : logging . Logger ): try : logger . info ( \"I am trying to divide by zero!\" ) 1 / 0 except ZeroDivisionError as e : logger . error ( e ) # ERROR:root:division by zero logger . critical ( e , exc_info = True ) # Logs error with stack trace if __name__ == \"__main__\" : train_logger = config . init_logger ( logs_dir = config . LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"train\" ) divide_by_zero ( train_logger ) Overwriting /content/reighns/main.py ! python main . py 2022-04-27 10:15:10 - train - INFO: I am trying to divide by zero! 2022-04-27 10:15:10 - train - ERROR: division by zero 2022-04-27 10:15:10 - train - CRITICAL: division by zero Traceback (most recent call last): File \"main.py\", line 7, in divide_by_zero 1 / 0 ZeroDivisionError: division by zero TODO Log Each individual ML experiment should come with its own log file for clarity. That means, if we have a total of \\(3\\) experiments of a ML project, named exp_1, exp_2, exp_3 , then each of their log files should be separated accordingly as well. If we find ourself adding too many handlers to the function, then we may define a logging config like in https://madewithml.com/courses/mlops/logging/ . If can have one log file with multiple module references instead of multiple log files individually. Add timestamp prefix for logger (experiment). For more practices, one can refer to the references below. References https://docs.python.org/3/howto/logging-cookbook.html https://docs.python.org/3/library/logging.html# https://madewithml.com/courses/mlops/logging/ Using logging in multiple modules: https://docs.python.org/3/howto/logging-cookbook.html","title":"03.logging"},{"location":"mlops_docs/developing/03.logging/#logging-guide","text":"by Hongnan Gao","title":"Logging Guide"},{"location":"mlops_docs/developing/03.logging/#logging","text":"","title":"Logging"},{"location":"mlops_docs/developing/03.logging/#setting-up","text":"This section will include previous lessons' setup code.","title":"Setting Up"},{"location":"mlops_docs/developing/03.logging/#packaging-and-setup","text":"This setup process can be found in lesson \\(1\\) . ! mkdir - p reighns % cd reighns /content/reighns from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () ! pip3 install - q virtualenv # install virtualenv package ! virtualenv venv_reighns # create the virtual environment # !source venv_reighns/bin/activate # this activates the vm in macOS, command is different for different OS. ! source venv_reighns / bin / activate ; python3 - m pip install -- upgrade pip setuptools wheel # upgrade pip so we download the latest package wheels |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.8 MB 18.4 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 461 kB 66.0 MB/s created virtual environment CPython3.7.13.final.0-64 in 1081ms creator CPython3Posix(dest=/content/reighns/venv_reighns, clear=False, no_vcs_ignore=False, global=False) seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv) added seed packages: pip==22.0.4, setuptools==62.1.0, wheel==0.37.1 activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator Requirement already satisfied: pip in ./venv_reighns/lib/python3.7/site-packages (22.0.4) Requirement already satisfied: setuptools in ./venv_reighns/lib/python3.7/site-packages (62.1.0) Requirement already satisfied: wheel in ./venv_reighns/lib/python3.7/site-packages (0.37.1) %% writefile { BASE_DIR } / requirements . txt numpy == 1.21.6 pandas == 1.3.5 matplotlib == 3.5.1 scikit - learn == 1.0.2 onnx == 1.11.0 onnxmltools == 1.11.0 skl2onnx == 1.11.1 holidays == 0.13 pytest == 7.1.1 pylint == 2.13.5 black == 22.3.0 flake8 == 4.0.1 Writing /content/reighns/requirements.txt %% writefile { BASE_DIR } / setup . py # setup.py # Setup installation for the application from pathlib import Path from setuptools import find_namespace_packages , setup BASE_DIR = Path ( __file__ ) . parent # Load packages from requirements.txt with open ( Path ( BASE_DIR , \"requirements.txt\" )) as file : required_packages = [ ln . strip () for ln in file . readlines ()] test_packages = [ \"coverage[toml]==6.0.2\" , \"great-expectations==0.13.14\" , \"pytest==6.0.2\" , \"pytest-cov==2.10.1\" , ] dev_packages = [ \"black==20.8b1\" , \"flake8==3.8.3\" , \"isort==5.5.3\" , \"jupyterlab==2.2.8\" , \"pre-commit==2.11.1\" , ] docs_packages = [ \"mkdocs==1.1.2\" , \"mkdocs-material==7.2.3\" , \"mkdocstrings==0.15.2\" , ] setup ( name = \"hongnan\" , version = \"0.1\" , license = \"MIT\" , description = \"MLOps guide.\" , author = \"Hongnan Gao\" , author_email = \"hongnang.sph@gmail.com\" , url = \"\" , keywords = [ \"machine-learning\" , \"artificial-intelligence\" ], classifiers = [ \"Development Status :: 3 - Alpha\" , \"Intended Audience :: Developers\" , \"Topic :: Software Development :: Build Tools\" , \"License :: OSI Approved :: MIT License\" , \"Programming Language :: Python :: 3\" , ], python_requires = \">=3.7\" , packages = find_namespace_packages (), install_requires = [ required_packages ], extras_require = { \"test\" : test_packages , \"dev\" : test_packages + dev_packages + docs_packages , \"docs\" : docs_packages , }, entry_points = { \"console_scripts\" : [ \"reighns = reighns.main:app\" , ], }, ) Writing /content/reighns/setup.py ! source venv_reighns / bin / activate ; python3 - m pip install - q - e . Preparing metadata (setup.py) ... done \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 15.7/15.7 MB 74.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.3/11.3 MB 83.2 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.2/11.2 MB 59.7 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24.8/24.8 MB 15.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.8/12.8 MB 83.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 302.4/302.4 KB 29.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 276.4/276.4 KB 22.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 172.1/172.1 KB 15.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 297.0/297.0 KB 28.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 437.6/437.6 KB 36.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 62.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.1/64.1 KB 8.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 843.7/843.7 KB 54.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 96.6/96.6 KB 12.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 69.7/69.7 KB 9.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.1/42.1 KB 5.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 247.7/247.7 KB 26.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 47.9/47.9 KB 6.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 930.9/930.9 KB 55.2 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40.8/40.8 KB 4.7 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 63.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.3/4.3 MB 41.9 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.5/98.5 KB 6.9 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 41.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 503.5/503.5 KB 28.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 250.7/250.7 KB 19.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 86.9/86.9 KB 7.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 103.4/103.4 KB 11.4 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 KB 6.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.7/98.7 KB 13.0 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 307.0/307.0 KB 29.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.1/38.1 MB 13.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.8/78.8 KB 8.9 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 57.8/57.8 KB 6.8 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 75.2/75.2 KB 9.5 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.4/5.4 MB 88.5 MB/s eta 0:00:00 Preparing metadata (setup.py) ... done Building wheel for pymeeus (setup.py) ... done","title":"Packaging and Setup"},{"location":"mlops_docs/developing/03.logging/#organization","text":"! mkdir - p config CONFIG_DIR = Path . joinpath ( BASE_DIR , \"config\" ) %% writefile { CONFIG_DIR } / config . py import logging import sys from dataclasses import dataclass from pathlib import Path from typing import List , Optional # Creating Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) SRC_DIR = Path ( BASE_DIR , \"src\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) TEST_DIR = Path ( BASE_DIR , \"tests\" ) ## Local stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) PROCESSED_DATA = Path ( DATA_DIR , \"processed\" ) ## Create dirs for d in [ CONFIG_DIR , LOGS_DIR , DATA_DIR , STORES_DIR , TEST_DIR , MODEL_REGISTRY , RAW_DATA , PROCESSED_DATA , ]: d . mkdir ( parents = True , exist_ok = True ) Writing /content/reighns/config/config.py %% writefile { BASE_DIR } / main . py from config import config Writing /content/reighns/main.py ! python3 main . py # call main to create the folders since it calls config!","title":"Organization"},{"location":"mlops_docs/developing/03.logging/#intuition","text":"Logging is the process of tracking and recording key events that occur in our applications. We want to log events so we can use them to inspect processes, fix issues, etc. They're a whole lot more powerful than print statements because they allow us to send specific pieces of information to specific locations, not to mention custom formatting, shared interface with other Python packages, etc. This makes logging a key proponent in being able to surface insightful information from the internal processes of our application.","title":"Intuition"},{"location":"mlops_docs/developing/03.logging/#components","text":"There are a few overarching concepts to be aware of first before we can create and use our loggers. Logger : the main object that emits the log messages from our application. Handler : used for sending log records to a specific location and specifications for that location (name, size, etc.). Formatter : used for style and layout of the log records. There is so much more to logging such as filters, exception logging, etc. but these basics will allows us to do everything we need for our application.","title":"Components"},{"location":"mlops_docs/developing/03.logging/#logging-levels","text":"The numeric values of logging levels are given in the following table. These are primarily of interest if you want to define your own levels, and need them to have specific values relative to the predefined levels. If you define a level with the same numeric value, it overwrites the predefined value; the predefined name is lost. Level Value DEBUG 10 INFO 20 WARN 30 ERROR 40 FATAL 50 Before we create our specialized, configured logger, let's look at what logged messages even look like by using a very basic configuration. import logging import sys from typing import Optional These are the basic levels of logging, where DEBUG is the lowest priority and CRITICAL is the highest. We defined our logger using basicConfig to emit log messages to our stdout console (we also could've written to any other stream or even a file) and to be sensitive to log messages starting from level DEBUG . This means that all of our logged messages will be displayed since DEBUG is the lowest level. Had we made the level ERROR , then only ERROR and CRITICAL log message would be displayed. (madewithml.com) In our first example, we set the level to be logging.DEBUG and all \\(5\\) messages are logged, as can be seen. # Create super basic logger logging . basicConfig ( stream = sys . stdout , level = logging . DEBUG ) # Logging levels (from lowest to highest priority) logging . debug ( \"Used for debugging your code.\" ) logging . info ( \"Informative messages from your code.\" ) logging . warning ( \"Everything works but there is something to be aware of.\" ) logging . error ( \"There's been a mistake with the process.\" ) logging . critical ( \"There is something terribly wrong and process may terminate.\" ) ERROR:root:There's been a mistake with the process. CRITICAL:root:There is something terribly wrong and process may terminate. In the next example, we set the level to be logging.ERROR , this means all messages lower than error is ignored, as can be seen here! Note, if you are working in google colab, one should factory reset the notebook so that the logger can be refreshed. # Create super basic logger logging . basicConfig ( stream = sys . stdout , level = logging . ERROR ) # Logging levels (from lowest to highest priority) logging . debug ( \"Used for debugging your code.\" ) logging . info ( \"Informative messages from your code.\" ) logging . warning ( \"Everything works but there is something to be aware of.\" ) logging . error ( \"There's been a mistake with the process.\" ) logging . critical ( \"There is something terribly wrong and process may terminate.\" ) ERROR:root:There's been a mistake with the process. CRITICAL:root:There is something terribly wrong and process may terminate.","title":"Logging Levels"},{"location":"mlops_docs/developing/03.logging/#custom-logger-function","text":"We will define a custom logger function for our purpose. If you encounter logger printing the same line multiple times, we should factory reset runtime. import logging import sys from pathlib import Path from typing import Optional We created a logging directory in the section Organization , however, for clarity, we create the LOGS_DIR again below (won't be overwritten). We will send all our logs to this directory. # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () LOGS_DIR = Path ( BASE_DIR , \"logs\" ) LOGS_DIR . mkdir ( parents = True , exist_ok = True ) # Logger def init_logger ( logs_dir : Path , default_level = logging . DEBUG , stream_level = logging . INFO , module_name : Optional [ str ] = None , ) -> logging . Logger : \"\"\"Initialize logger. Args: logs_dir (Path): Path to log directory. default_level (int, optional): Default logging level. Defaults to logging.DEBUG. stream_level (int, optional): Stream logging level. Defaults to logging.INFO. module_name (Optional[str]): Module name to be used in logger. Defaults to None. Returns: logging.Logger: The logger object. Example: >>> import logging >>> import sys >>> from pathlib import Path >>> from typing import Optional >>> # Creating Directories >>> BASE_DIR = Path(\"__file__\").parent.parent.absolute() >>> LOGS_DIR = Path(BASE_DIR, \"logs\") >>> LOGS_DIR.mkdir(parents=True, exist_ok=True) >>> train_logger = init_logger(LOGS_DIR, module_name=\"train\") >>> # Logging levels (from lowest to highest priority) >>> try: >>> train_logger.info(\"I am trying to divide by zero!\") >>> 1 / 0 >>> except ZeroDivisionError as e: >>> train_logger.error(e) # ERROR:root:division by zero >>> train_logger.critical(e, exc_info=True) # Logs error with stack trace \"\"\" if module_name is None : logger = logging . getLogger ( __name__ ) info_log_filepath = Path ( logs_dir , \"info.log\" ) error_log_filepath = Path ( logs_dir , \"error.log\" ) else : # get module name, useful for multi-module logging logger = logging . getLogger ( module_name ) info_log_filepath = Path ( logs_dir , f \" { module_name } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { module_name } _error.log\" ) logger . setLevel ( default_level ) stream_handler = logging . StreamHandler ( stream = sys . stdout ) stream_handler . setLevel ( stream_level ) stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) info_file_handler = logging . FileHandler ( filename = info_log_filepath ) info_file_handler . setLevel ( logging . INFO ) info_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) # add error file handler error_file_handler = logging . FileHandler ( filename = error_log_filepath ) error_file_handler . setLevel ( logging . ERROR ) error_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) logger . addHandler ( stream_handler ) logger . addHandler ( info_file_handler ) logger . addHandler ( error_file_handler ) logger . propagate = False return logger train_logger = init_logger ( logs_dir = LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"train\" ) # Logging levels (from lowest to highest priority) train_logger . debug ( \"Used for debugging your code.\" ) train_logger . info ( \"Informative messages from your code.\" ) train_logger . warning ( \"Everything works but there is something to be aware of.\" ) train_logger . error ( \"There's been a mistake with the process.\" ) train_logger . critical ( \"There is something terribly wrong and process may terminate.\" ) 2022-04-27 09:55:29 - train - INFO: Informative messages from your code. 2022-04-27 09:55:29 - train - WARNING: Everything works but there is something to be aware of. 2022-04-27 09:55:29 - train - ERROR: There's been a mistake with the process. 2022-04-27 09:55:29 - train - CRITICAL: There is something terribly wrong and process may terminate. Lo and behold, the train_logger is behaving properly: - console level: all messages above INFO are printed. - info file: all messages above INFO are logged in the file. - error file: all messages above DEBUG are logged in the file, in particular, messages of lower priority like .info and .debug are not logged. The reason of having \\(2\\) log files is that one file (info) logs almost everything, while the other (error) only logs the error messages etc. This avoids clutter and eases developer to pin-point errors when reviewing the code. For completeness sake, we define another logger called inference_logger and see that it behaves the same, except for the fact that it is logging messages for another module. inference_logger = init_logger ( logs_dir = LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"inference\" ) # Logging levels (from lowest to highest priority) inference_logger . debug ( \"Used for debugging your code.\" ) inference_logger . info ( \"Informative messages from your code.\" ) inference_logger . warning ( \"Everything works but there is something to be aware of.\" ) inference_logger . error ( \"There's been a mistake with the process.\" ) inference_logger . critical ( \"There is something terribly wrong and process may terminate.\" ) 2022-04-27 09:55:33 - inference - INFO: Informative messages from your code. 2022-04-27 09:55:33 - inference - WARNING: Everything works but there is something to be aware of. 2022-04-27 09:55:33 - inference - ERROR: There's been a mistake with the process. 2022-04-27 09:55:33 - inference - CRITICAL: There is something terribly wrong and process may terminate.","title":"Custom Logger Function"},{"location":"mlops_docs/developing/03.logging/#example-usage","text":"The below small example shows how one can log messages. In particular, in the except clause, we called logging.error(e) to log the error messages and logging.critical(e, exc_info=True) to log both the message and the stack trace. %% writefile { CONFIG_DIR } / config . py import logging import sys from dataclasses import dataclass from pathlib import Path from typing import List , Optional import datetime # Creating Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) SRC_DIR = Path ( BASE_DIR , \"src\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) TEST_DIR = Path ( BASE_DIR , \"tests\" ) ## Local stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) PROCESSED_DATA = Path ( DATA_DIR , \"processed\" ) ## Create dirs for d in [ CONFIG_DIR , LOGS_DIR , DATA_DIR , STORES_DIR , TEST_DIR , MODEL_REGISTRY , RAW_DATA , PROCESSED_DATA , ]: d . mkdir ( parents = True , exist_ok = True ) # Logger def init_logger ( logs_dir : Path , default_level = logging . DEBUG , stream_level = logging . INFO , module_name : Optional [ str ] = None , ) -> logging . Logger : \"\"\"Initialize logger. Args: logs_dir (Path): Path to log directory. default_level (int, optional): Default logging level. Defaults to logging.DEBUG. stream_level (int, optional): Stream logging level. Defaults to logging.INFO. module_name (Optional[str]): Module name to be used in logger. Defaults to None. Returns: logging.Logger: The logger object. Example: >>> import logging >>> import sys >>> from pathlib import Path >>> from typing import Optional >>> # Creating Directories >>> BASE_DIR = Path(\"__file__\").parent.parent.absolute() >>> LOGS_DIR = Path(BASE_DIR, \"logs\") >>> LOGS_DIR.mkdir(parents=True, exist_ok=True) >>> train_logger = init_logger(LOGS_DIR, module_name=\"train\") >>> # Logging levels (from lowest to highest priority) >>> try: >>> train_logger.info(\"I am trying to divide by zero!\") >>> 1 / 0 >>> except ZeroDivisionError as e: >>> train_logger.error(e) # ERROR:root:division by zero >>> train_logger.critical(e, exc_info=True) # Logs error with stack trace \"\"\" datetime_ = datetime . datetime . now () . strftime ( \"%Y-%m- %d _%H-%M-%S\" ) if module_name is None : logger = logging . getLogger ( __name__ ) info_log_filepath = Path ( logs_dir , f \" { datetime_ } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { datetime_ } _error.log\" ) else : # get module name, useful for multi-module logging logger = logging . getLogger ( module_name ) info_log_filepath = Path ( logs_dir , f \" { datetime_ } _ { module_name } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { datetime_ } _ { module_name } _error.log\" ) logger . setLevel ( default_level ) stream_handler = logging . StreamHandler ( stream = sys . stdout ) stream_handler . setLevel ( stream_level ) stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) info_file_handler = logging . FileHandler ( filename = info_log_filepath ) info_file_handler . setLevel ( logging . INFO ) info_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) # add error file handler error_file_handler = logging . FileHandler ( filename = error_log_filepath ) error_file_handler . setLevel ( logging . ERROR ) error_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) logger . addHandler ( stream_handler ) logger . addHandler ( info_file_handler ) logger . addHandler ( error_file_handler ) logger . propagate = False return logger Overwriting /content/reighns/config/config.py %% writefile { BASE_DIR } / main . py import logging from config import config def divide_by_zero ( logger : logging . Logger ): try : logger . info ( \"I am trying to divide by zero!\" ) 1 / 0 except ZeroDivisionError as e : logger . error ( e ) # ERROR:root:division by zero logger . critical ( e , exc_info = True ) # Logs error with stack trace if __name__ == \"__main__\" : train_logger = config . init_logger ( logs_dir = config . LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"train\" ) divide_by_zero ( train_logger ) Overwriting /content/reighns/main.py ! python main . py 2022-04-27 10:15:10 - train - INFO: I am trying to divide by zero! 2022-04-27 10:15:10 - train - ERROR: division by zero 2022-04-27 10:15:10 - train - CRITICAL: division by zero Traceback (most recent call last): File \"main.py\", line 7, in divide_by_zero 1 / 0 ZeroDivisionError: division by zero","title":"Example Usage"},{"location":"mlops_docs/developing/03.logging/#workflow","text":"","title":"Workflow"},{"location":"mlops_docs/developing/03.logging/#workflow-in-ide","text":"","title":"Workflow in IDE"},{"location":"mlops_docs/developing/03.logging/#workflow-in-google-colab","text":"%% writefile { CONFIG_DIR } / config . py import logging import sys from dataclasses import dataclass from pathlib import Path from typing import List , Optional # Creating Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) SRC_DIR = Path ( BASE_DIR , \"src\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) STORES_DIR = Path ( BASE_DIR , \"stores\" ) TEST_DIR = Path ( BASE_DIR , \"tests\" ) ## Local stores MODEL_REGISTRY = Path ( STORES_DIR , \"model\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) PROCESSED_DATA = Path ( DATA_DIR , \"processed\" ) ## Create dirs for d in [ CONFIG_DIR , LOGS_DIR , DATA_DIR , STORES_DIR , TEST_DIR , MODEL_REGISTRY , RAW_DATA , PROCESSED_DATA , ]: d . mkdir ( parents = True , exist_ok = True ) # Logger def init_logger ( logs_dir : Path , default_level = logging . DEBUG , stream_level = logging . INFO , module_name : Optional [ str ] = None , ) -> logging . Logger : \"\"\"Initialize logger. Args: logs_dir (Path): Path to log directory. default_level (int, optional): Default logging level. Defaults to logging.DEBUG. stream_level (int, optional): Stream logging level. Defaults to logging.INFO. module_name (Optional[str]): Module name to be used in logger. Defaults to None. Returns: logging.Logger: The logger object. Example: >>> import logging >>> import sys >>> from pathlib import Path >>> from typing import Optional >>> # Creating Directories >>> BASE_DIR = Path(\"__file__\").parent.parent.absolute() >>> LOGS_DIR = Path(BASE_DIR, \"logs\") >>> LOGS_DIR.mkdir(parents=True, exist_ok=True) >>> train_logger = init_logger(LOGS_DIR, module_name=\"train\") >>> # Logging levels (from lowest to highest priority) >>> try: >>> train_logger.info(\"I am trying to divide by zero!\") >>> 1 / 0 >>> except ZeroDivisionError as e: >>> train_logger.error(e) # ERROR:root:division by zero >>> train_logger.critical(e, exc_info=True) # Logs error with stack trace \"\"\" if module_name is None : logger = logging . getLogger ( __name__ ) info_log_filepath = Path ( logs_dir , \"info.log\" ) error_log_filepath = Path ( logs_dir , \"error.log\" ) else : # get module name, useful for multi-module logging logger = logging . getLogger ( module_name ) info_log_filepath = Path ( logs_dir , f \" { module_name } _info.log\" ) error_log_filepath = Path ( logs_dir , f \" { module_name } _error.log\" ) logger . setLevel ( default_level ) stream_handler = logging . StreamHandler ( stream = sys . stdout ) stream_handler . setLevel ( stream_level ) stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) info_file_handler = logging . FileHandler ( filename = info_log_filepath ) info_file_handler . setLevel ( logging . INFO ) info_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) # add error file handler error_file_handler = logging . FileHandler ( filename = error_log_filepath ) error_file_handler . setLevel ( logging . ERROR ) error_file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" , ) ) logger . addHandler ( stream_handler ) logger . addHandler ( info_file_handler ) logger . addHandler ( error_file_handler ) logger . propagate = False return logger Overwriting /content/reighns/config/config.py %% writefile { BASE_DIR } / main . py import logging from config import config def divide_by_zero ( logger : logging . Logger ): try : logger . info ( \"I am trying to divide by zero!\" ) 1 / 0 except ZeroDivisionError as e : logger . error ( e ) # ERROR:root:division by zero logger . critical ( e , exc_info = True ) # Logs error with stack trace if __name__ == \"__main__\" : train_logger = config . init_logger ( logs_dir = config . LOGS_DIR , default_level = logging . DEBUG , stream_level = logging . INFO , module_name = \"train\" ) divide_by_zero ( train_logger ) Overwriting /content/reighns/main.py ! python main . py 2022-04-27 10:15:10 - train - INFO: I am trying to divide by zero! 2022-04-27 10:15:10 - train - ERROR: division by zero 2022-04-27 10:15:10 - train - CRITICAL: division by zero Traceback (most recent call last): File \"main.py\", line 7, in divide_by_zero 1 / 0 ZeroDivisionError: division by zero","title":"Workflow in Google Colab"},{"location":"mlops_docs/developing/03.logging/#todo-log","text":"Each individual ML experiment should come with its own log file for clarity. That means, if we have a total of \\(3\\) experiments of a ML project, named exp_1, exp_2, exp_3 , then each of their log files should be separated accordingly as well. If we find ourself adding too many handlers to the function, then we may define a logging config like in https://madewithml.com/courses/mlops/logging/ . If can have one log file with multiple module references instead of multiple log files individually. Add timestamp prefix for logger (experiment). For more practices, one can refer to the references below.","title":"TODO Log"},{"location":"mlops_docs/developing/03.logging/#references","text":"https://docs.python.org/3/howto/logging-cookbook.html https://docs.python.org/3/library/logging.html# https://madewithml.com/courses/mlops/logging/ Using logging in multiple modules: https://docs.python.org/3/howto/logging-cookbook.html","title":"References"},{"location":"mlops_docs/developing/dev_tools/","text":"Customize Terminal Tip See Eric's zsh for better customization. Customize your terminal: Oh My ZSH + Powerlevel10k . The default settings put the virtual environments like conda or virtualenv to the right without parenthesis; we can change this by the following steps : Open the settings of Powerlevel10k by calling code ~/.p10k.zsh; To display conda environment with parentheses, find POWERLEVEL9K_ANACONDA_CONTENT_EXPANSION and change it like this: zsh typeset -g POWERLEVEL9K_ANACONDA_CONTENT_EXPANSION='${CONDA_PROMPT_MODIFIER% }' This can be applied to other types of virtual environments as well. To place the prompt to the left, just copy the ones you want from POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS to POWERLEVEL9K_LEFT_PROMPT_ELEMENTS. VSCode Extensions and Settings","title":"Dev tools"},{"location":"mlops_docs/developing/dev_tools/#customize-terminal","text":"Tip See Eric's zsh for better customization. Customize your terminal: Oh My ZSH + Powerlevel10k . The default settings put the virtual environments like conda or virtualenv to the right without parenthesis; we can change this by the following steps : Open the settings of Powerlevel10k by calling code ~/.p10k.zsh; To display conda environment with parentheses, find POWERLEVEL9K_ANACONDA_CONTENT_EXPANSION and change it like this: zsh typeset -g POWERLEVEL9K_ANACONDA_CONTENT_EXPANSION='${CONDA_PROMPT_MODIFIER% }' This can be applied to other types of virtual environments as well. To place the prompt to the left, just copy the ones you want from POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS to POWERLEVEL9K_LEFT_PROMPT_ELEMENTS.","title":"Customize Terminal"},{"location":"mlops_docs/developing/dev_tools/#vscode-extensions-and-settings","text":"","title":"VSCode Extensions and Settings"},{"location":"mlops_docs/developing/documentation/","text":"Writing/Blogging Etiquettes https://knowadays.com/blog/title-capitalisation-sentence-case-title-case/ I use title case to capitalize keywords and verbs in title. Do not capitalize after colon. https://github.com/jupyterlab-contrib/spellchecker Where should one place reference links? https://www.scribendi.com/academy/articles/what_to_capitalize_in_a_title.en.html https://www.dexplo.org/jupyter_to_medium/#publishing-to-medium-with-a-python-script Sphinx/Jupyter-Book https://jupyterbook.org/intro.html See my github repos for configuration and styles for sphinx and jupyter-book. Mkdocs Read up mkdocs's setup https://squidfunk.github.io/mkdocs-material/setup/ for customization. set up github icon: https://squidfunk.github.io/mkdocs-material/setup/adding-a-git-repository/ https://pawamoy.github.io/markdown-exec/usage/python/ General Mkdocs Footnotes should follow punctuation: https://www.bristol.ac.uk/arts/exercises/referencing/page_08.htm#:~:text=Footnote%20or%20endnote%20numbers%20in,the%20end%20of%20a%20sentence. \\({\\color{red} x} + {\\color{blue} y}\\) for color latex. Latex/Mathjax \\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}}\\] Info All the mathjax typesets are placed in mathjax.js . Label Equations We label equations by using \\(\\eqref{eq:sample}\\) , we find the value of an interesting integral: \\[ \\begin{equation} \\int_0^\\infty \\frac{x^3}{e^x-1}\\,dx = \\frac{\\pi^4}{15} \\label{eq:sample} \\end{equation} \\] Note that it is necessary to use the label command to label equations, and it is also important to give an indicative name for the label, so that the label can be used in references. This label method supports automatic numbering, as you can see in the example. tex: { tags: 'ams' } Color To add color code: loader: { load: ['[tex]/color'] }, tex: { packages: { '[+]': ['color'] } } \\({\\color{red} x} + {\\color{blue} y}\\) Warning Note that since tags: 'ams' is a key-pair inside tex , then we should instead write: loader: { load: ['[tex]/color'] }, tex: { packages: { '[+]': ['color'] }, tags: 'ams'} Setting up GitHub Pages and Mkdocs (Website) Update November 9th, 2021 You can skip most steps if you just fork this repository and follow the format. Welcome to my example website! This website uses MkDocs with the Material theme and an automated deployment workflow for publishing to GitHub Pages . This guide will help you create your own GitHub Pages website with this setup. If you're using Windows, you should run all the commands in this guide on a Windows Subsystem for Linux (WSL) terminal. Initializing your Website Repository First, create a new repository on GitHub. Make sure to skip the initialization step on the website \u2014 you will be initializing the contents of this repository with this template! Note down the Git remote for your new repository, you'll need it later when initializing your local copy of the repository. Next, download the website template and extract it: $ wget https://github.com/jansky/test-website/archive/refs/tags/template.tar.gz $ tar xvf template.tar.gz $ rm template.tar.gz Note If the wget command is not found, you can install it using apt-get: sudo apt-get install wget . This will create a new folder called test-website-template in your current directory with the website template contents. You may wish to rename this folder to something like my-website : $ mv test-website-template my-website Now you can initialize a Git repository with the website contents: $ cd my-website $ git init $ git remote add origin YOUR_GITHUB_REPOSITORY_REMOTE Website Configuration The configuration for your website is stored in mkdocs.yml in the repository root. You only need to change a few settings at the top of the file: 1 2 3 4 5 6 7 site_name : Example Website site_url : https://reighns92.github.io/test-website nav : - Home : index.md - About : about.md - Notebooks : notebooks_list.md ... First, update the site_name and site_url fields to be correct for your website. The URL format for GitHub pages websites is https://USERNAME.github.io/REPOSITORY-NAME As you add content to your website, you can also control the pages that appear on your website's navbar in the nav field. Each nav list element is of the form Link Text : filename.md For navbar links pointing to pages in your site, you should use a file path which is relative to the docs/ folder where all your website content is stored. You may also link to external pages and include sub-links. For more information, you can view the MkDocs nav documentation . GitHub Actions Configuration You also need to update the GitHub Actions deployment workflow with the name and e-mail address to use when the workflow pushes your built website to the gh-pages branch of your repository. In the file .github/workflows/deploy-website.yml , update lines 25 and 26 to reflect your account information: 22 23 24 25 26 27 ... - name : Push Build Website to gh-pages Branch run : | git config --global user.name 'YOUR NAME(Automated)' git config --global user.email 'YOUR-GITHUB-USERNAME@users.noreply.github.com' ... Setting Up Local Development MkDocs makes it easy to develop your website locally and see your changes in real time. To begin, set up and activate Python virtual environment for this project. Then, install the project dependencies: ( venv ) $ pip install -r requirements.txt MkDocs includes a small webserver that allows you to preview your website in your browser as you make changes. Whenever you save one of your source files, the website will be rebuilt and your browser will automatically refresh to show the new changes. You can start this development server using the serve command: ( venv ) $ mkdocs serve INFO - Building documentation... ... INFO - Documentation built in 0 .16 seconds INFO - [ 20 :09:07 ] Serving on http://127.0.0.1:8000/... ... If you copy and paste the URL given by MkDocs into your browser you will see your website preview. Adding Website Content Markdown files added under the docs/ folder will be converted to HTML pages on your website. This website template enables some helpful extensions for rendering of math in LaTeX style, admonitions such as notes and warnings, and code blocks with syntax highlighting. In addition, MkDocs also supports GitHub-flavored Markdown tables. To see examples of syntax for these elements, see the MkDocs website here . Deploying Your Changes When you are ready to deploy your website for the first time, make an initial commit and push to your GitHub remote: $ git add . $ git commit -a -m \"Initial Commit\" $ git push origin master -u Activate Workflow The GitHub Actions deployment workflow included with this template runs whenever you push to the master branch. This workflow will build your website using MkDocs and push the built website files to the gh-pages branch of your repository, which GitHub Pages can use to serve your website. Once you have pushed your changes, go to your repository page on GitHub and confirm that the GitHub Actions workflow has completed successfully (you should see a green checkmark next to the name of the most recent commit). Then, go to your repository settings page, and click on 'Pages'. You will see a section that will let you set the source for your GitHub Pages website. Click the box labelled 'None' and select the gh-pages branch. Leave the selectd folder at '/ (root)' and click 'Save'. Your website is now live! To push additional changes, simply commit and push to the master branch. The GitHub Actions deployment workflow will handle deploying your changes to GitHub Pages. Pandoc (Markdown Converter) More often than not, you will need to convert a jupyter notebook to markdown file for deployment (as markdown supports Admonitions in Mkdocs). Here is a way to do it, it is very convenient as it not only converts your notebook files to markdown, it also stores your output as images in a folder for you. This means any images rendered in notebook by matplotlib etc will now show up in markdown! brew install pandoc git clone https://github.com/jupyter/nbconvert.git cd nbconvert pip install -e . Then to convert, simply do the following: jupyter nbconvert --to markdown mynotebook.ipynb Pandoc (Wikitext to Markdown) From the solution here . We can do the following: pandoc --from mediawiki --to = markdown-definition_lists wiki.txt -o wiki.md where wiki.txt is a text file with wiki markup. Miscellaneous Problems Path Environment Often times, you will encounter a problem with the path environment when working with Windows especially. For example, when you do the following: jupyter nbconvert --to markdown mynotebook.ipynb then 'jupyter' is not recognized as an internal or external command, operable program or batch file. is the error message even though jupyter is installed. Usually, the shell will prompt a message to check PATH . Now go to Advanced System Settings and click on Environment Variables. You will see a list of environment variables. You can add a new environment variable by clicking on the plus sign in the System Variables . Add the path recommended by jupyter to the PATH variable. In my case, it is the obscure C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts . Mkdocs Full page width Use this custom css code to make page full width.","title":"Documentation"},{"location":"mlops_docs/developing/documentation/#writingblogging-etiquettes","text":"https://knowadays.com/blog/title-capitalisation-sentence-case-title-case/ I use title case to capitalize keywords and verbs in title. Do not capitalize after colon. https://github.com/jupyterlab-contrib/spellchecker Where should one place reference links? https://www.scribendi.com/academy/articles/what_to_capitalize_in_a_title.en.html https://www.dexplo.org/jupyter_to_medium/#publishing-to-medium-with-a-python-script","title":"Writing/Blogging Etiquettes"},{"location":"mlops_docs/developing/documentation/#sphinxjupyter-book","text":"https://jupyterbook.org/intro.html See my github repos for configuration and styles for sphinx and jupyter-book.","title":"Sphinx/Jupyter-Book"},{"location":"mlops_docs/developing/documentation/#mkdocs","text":"Read up mkdocs's setup https://squidfunk.github.io/mkdocs-material/setup/ for customization. set up github icon: https://squidfunk.github.io/mkdocs-material/setup/adding-a-git-repository/ https://pawamoy.github.io/markdown-exec/usage/python/","title":"Mkdocs"},{"location":"mlops_docs/developing/documentation/#general-mkdocs","text":"Footnotes should follow punctuation: https://www.bristol.ac.uk/arts/exercises/referencing/page_08.htm#:~:text=Footnote%20or%20endnote%20numbers%20in,the%20end%20of%20a%20sentence. \\({\\color{red} x} + {\\color{blue} y}\\) for color latex.","title":"General Mkdocs"},{"location":"mlops_docs/developing/documentation/#latexmathjax","text":"\\[\\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\v}{\\mathbf{v}} \\newcommand{\\a}{\\mathbf{a}} \\newcommand{\\b}{\\mathbf{b}} \\newcommand{\\c}{\\mathbf{c}} \\newcommand{\\w}{\\mathbf{w}} \\newcommand{\\u}{\\mathbf{u}} \\newcommand{\\0}{\\mathbf{0}} \\newcommand{\\1}{\\mathbf{1}}\\] Info All the mathjax typesets are placed in mathjax.js .","title":"Latex/Mathjax"},{"location":"mlops_docs/developing/documentation/#label-equations","text":"We label equations by using \\(\\eqref{eq:sample}\\) , we find the value of an interesting integral: \\[ \\begin{equation} \\int_0^\\infty \\frac{x^3}{e^x-1}\\,dx = \\frac{\\pi^4}{15} \\label{eq:sample} \\end{equation} \\] Note that it is necessary to use the label command to label equations, and it is also important to give an indicative name for the label, so that the label can be used in references. This label method supports automatic numbering, as you can see in the example. tex: { tags: 'ams' }","title":"Label Equations"},{"location":"mlops_docs/developing/documentation/#color","text":"To add color code: loader: { load: ['[tex]/color'] }, tex: { packages: { '[+]': ['color'] } } \\({\\color{red} x} + {\\color{blue} y}\\) Warning Note that since tags: 'ams' is a key-pair inside tex , then we should instead write: loader: { load: ['[tex]/color'] }, tex: { packages: { '[+]': ['color'] }, tags: 'ams'}","title":"Color"},{"location":"mlops_docs/developing/documentation/#setting-up-github-pages-and-mkdocs-website","text":"Update November 9th, 2021 You can skip most steps if you just fork this repository and follow the format. Welcome to my example website! This website uses MkDocs with the Material theme and an automated deployment workflow for publishing to GitHub Pages . This guide will help you create your own GitHub Pages website with this setup. If you're using Windows, you should run all the commands in this guide on a Windows Subsystem for Linux (WSL) terminal.","title":"Setting up GitHub Pages and Mkdocs (Website)"},{"location":"mlops_docs/developing/documentation/#initializing-your-website-repository","text":"First, create a new repository on GitHub. Make sure to skip the initialization step on the website \u2014 you will be initializing the contents of this repository with this template! Note down the Git remote for your new repository, you'll need it later when initializing your local copy of the repository. Next, download the website template and extract it: $ wget https://github.com/jansky/test-website/archive/refs/tags/template.tar.gz $ tar xvf template.tar.gz $ rm template.tar.gz Note If the wget command is not found, you can install it using apt-get: sudo apt-get install wget . This will create a new folder called test-website-template in your current directory with the website template contents. You may wish to rename this folder to something like my-website : $ mv test-website-template my-website Now you can initialize a Git repository with the website contents: $ cd my-website $ git init $ git remote add origin YOUR_GITHUB_REPOSITORY_REMOTE","title":"Initializing your Website Repository"},{"location":"mlops_docs/developing/documentation/#website-configuration","text":"The configuration for your website is stored in mkdocs.yml in the repository root. You only need to change a few settings at the top of the file: 1 2 3 4 5 6 7 site_name : Example Website site_url : https://reighns92.github.io/test-website nav : - Home : index.md - About : about.md - Notebooks : notebooks_list.md ... First, update the site_name and site_url fields to be correct for your website. The URL format for GitHub pages websites is https://USERNAME.github.io/REPOSITORY-NAME As you add content to your website, you can also control the pages that appear on your website's navbar in the nav field. Each nav list element is of the form Link Text : filename.md For navbar links pointing to pages in your site, you should use a file path which is relative to the docs/ folder where all your website content is stored. You may also link to external pages and include sub-links. For more information, you can view the MkDocs nav documentation .","title":"Website Configuration"},{"location":"mlops_docs/developing/documentation/#github-actions-configuration","text":"You also need to update the GitHub Actions deployment workflow with the name and e-mail address to use when the workflow pushes your built website to the gh-pages branch of your repository. In the file .github/workflows/deploy-website.yml , update lines 25 and 26 to reflect your account information: 22 23 24 25 26 27 ... - name : Push Build Website to gh-pages Branch run : | git config --global user.name 'YOUR NAME(Automated)' git config --global user.email 'YOUR-GITHUB-USERNAME@users.noreply.github.com' ...","title":"GitHub Actions Configuration"},{"location":"mlops_docs/developing/documentation/#setting-up-local-development","text":"MkDocs makes it easy to develop your website locally and see your changes in real time. To begin, set up and activate Python virtual environment for this project. Then, install the project dependencies: ( venv ) $ pip install -r requirements.txt MkDocs includes a small webserver that allows you to preview your website in your browser as you make changes. Whenever you save one of your source files, the website will be rebuilt and your browser will automatically refresh to show the new changes. You can start this development server using the serve command: ( venv ) $ mkdocs serve INFO - Building documentation... ... INFO - Documentation built in 0 .16 seconds INFO - [ 20 :09:07 ] Serving on http://127.0.0.1:8000/... ... If you copy and paste the URL given by MkDocs into your browser you will see your website preview.","title":"Setting Up Local Development"},{"location":"mlops_docs/developing/documentation/#adding-website-content","text":"Markdown files added under the docs/ folder will be converted to HTML pages on your website. This website template enables some helpful extensions for rendering of math in LaTeX style, admonitions such as notes and warnings, and code blocks with syntax highlighting. In addition, MkDocs also supports GitHub-flavored Markdown tables. To see examples of syntax for these elements, see the MkDocs website here .","title":"Adding Website Content"},{"location":"mlops_docs/developing/documentation/#deploying-your-changes","text":"When you are ready to deploy your website for the first time, make an initial commit and push to your GitHub remote: $ git add . $ git commit -a -m \"Initial Commit\" $ git push origin master -u","title":"Deploying Your Changes"},{"location":"mlops_docs/developing/documentation/#activate-workflow","text":"The GitHub Actions deployment workflow included with this template runs whenever you push to the master branch. This workflow will build your website using MkDocs and push the built website files to the gh-pages branch of your repository, which GitHub Pages can use to serve your website. Once you have pushed your changes, go to your repository page on GitHub and confirm that the GitHub Actions workflow has completed successfully (you should see a green checkmark next to the name of the most recent commit). Then, go to your repository settings page, and click on 'Pages'. You will see a section that will let you set the source for your GitHub Pages website. Click the box labelled 'None' and select the gh-pages branch. Leave the selectd folder at '/ (root)' and click 'Save'. Your website is now live! To push additional changes, simply commit and push to the master branch. The GitHub Actions deployment workflow will handle deploying your changes to GitHub Pages.","title":"Activate Workflow"},{"location":"mlops_docs/developing/documentation/#pandoc-markdown-converter","text":"More often than not, you will need to convert a jupyter notebook to markdown file for deployment (as markdown supports Admonitions in Mkdocs). Here is a way to do it, it is very convenient as it not only converts your notebook files to markdown, it also stores your output as images in a folder for you. This means any images rendered in notebook by matplotlib etc will now show up in markdown! brew install pandoc git clone https://github.com/jupyter/nbconvert.git cd nbconvert pip install -e . Then to convert, simply do the following: jupyter nbconvert --to markdown mynotebook.ipynb","title":"Pandoc (Markdown Converter)"},{"location":"mlops_docs/developing/documentation/#pandoc-wikitext-to-markdown","text":"From the solution here . We can do the following: pandoc --from mediawiki --to = markdown-definition_lists wiki.txt -o wiki.md where wiki.txt is a text file with wiki markup.","title":"Pandoc (Wikitext to Markdown)"},{"location":"mlops_docs/developing/documentation/#miscellaneous-problems","text":"","title":"Miscellaneous Problems"},{"location":"mlops_docs/developing/documentation/#path-environment","text":"Often times, you will encounter a problem with the path environment when working with Windows especially. For example, when you do the following: jupyter nbconvert --to markdown mynotebook.ipynb then 'jupyter' is not recognized as an internal or external command, operable program or batch file. is the error message even though jupyter is installed. Usually, the shell will prompt a message to check PATH . Now go to Advanced System Settings and click on Environment Variables. You will see a list of environment variables. You can add a new environment variable by clicking on the plus sign in the System Variables . Add the path recommended by jupyter to the PATH variable. In my case, it is the obscure C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts .","title":"Path Environment"},{"location":"mlops_docs/developing/documentation/#mkdocs_1","text":"","title":"Mkdocs"},{"location":"mlops_docs/developing/documentation/#full-page-width","text":"Use this custom css code to make page full width.","title":"Full page width"},{"location":"mlops_docs/developing/general_workflow/","text":"Git See my documentation here . Packaging and Setup We will be on branch setup_requirements for this section. Set Up Main Directory (IDE) Let us assume that we are residing in our root folder ~/gaohn and we want to create a new project called YOLO , we can do as follows: creating main directory 1 2 ~/gaohn $ mkdir YOLO && cd YOLO ~/gaohn/YOLO $ code . # (1) Open the project directory in Visual Studio Code. To change appropriately if using different IDE. If you are cloning a repository to your local folder YOLO , you can also do: cloning repository 1 ~/gaohn/YOLO $ git clone https://github.com/gao-hongnan/gaohn-yolov1-pytorch.git . where . means cloning to the current directory. README We can create a README.md file to describe the project. For example: README.md 1 ~/gaohn/YOLO $ touch README.md Typically, README.md includes instructions on how to install the project, how to use it, and how to contribute to it. Set Up Git Git is a version control system that is used to track changes to files. It is integral to the development process of any software. Here we initiate our main directory with git. The steps to setting up git can be found here . Be sure to have a read before proceeding. A typical git workflow on an empty repository is as follows: git 1 2 3 4 5 6 7 8 9 ~/gaohn/YOLO $ touch .gitignore ~/gaohn/YOLO $ git init ~/gaohn/YOLO $ git config --global user.name \"Your Name\" ~/gaohn/YOLO $ git config --global user.email \"your@email.com\" # (1) ~/gaohn/YOLO $ git add . ~/gaohn/YOLO $ git commit -a # (2) ~/gaohn/YOLO $ git remote add origin \"your-repo-http\" # (3) ~/gaohn/YOLO $ git remote set-url origin https:// [ token ] @github.com/ [ username ] / [ repository ] # (4) ~/gaohn/YOLO $ git push origin master -u # (5) important to set the email linked to the git account. write commit message. add remote origin. set the remote origin. push to remote origin. Set Up Virtual Environment Follow the steps below to set up a virtual environment for your development. Windows macOS M1 Linux venv 1 2 3 ~/gaohn/YOLO $ python -m venv <venv_name> # (1) ~/gaohn/YOLO $ <venv_name> \\S cripts \\a ctivate # (2) ~/gaohn/YOLO ( venv ) $ python -m pip install --upgrade pip setuptools wheel # (3) Create virtual environment named venv in the current directory. Activate virtual environment. Upgrade pip, setuptools and wheel. venv 1 2 3 4 ~/gaohn/YOLO $ pip3 install virtualenv ~/gaohn/YOLO $ virtualenv venv_name> ~/gaohn/YOLO $ source venv_name> \\b in \\a ctivate ~/gaohn/YOLO ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel venv 1 2 3 4 ~/gaohn/YOLO $ sudo apt install python3.8 python3.8-venv python3-venv ~/gaohn/YOLO $ python3 -m venv venv_name> ~/gaohn/YOLO $ source venv_name> \\b in \\a ctivate ~/gaohn/YOLO ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel You should see the following directory structure: main directory tree 1 2 \ud83d\udcc1 YOLO/ \ud83d\udcc4 \u2514\u2500\u2500 venv/ Requirements and Setup Requirements Note For small projects, we can have requirements.txt and just run pip install -r requirements.txt . For larger projects, we can add a setup.py file. In short, requirements.txt specifies the dependencies of your project, and setup.py file informs you about the module or package-dependencies you are about to install has been packaged and distributed with Distutils, which is the standard for distributing Python Modules. You can skip setup.py if you are just using requirements.txt to install dependencies. Refer to madewithml 's requirements and setup section for more details. create requirements 1 2 ~/gaohn/YOLO ( venv ) $ touch requirements.txt ~/gaohn/YOLO ( venv ) $ pip install -r requirements.txt insert into requirements.txt 1 2 3 4 5 6 7 8 9 10 ~/gaohn/YOLO ( venv ) $ cat > requirements.txt torch == 1 .10.0+cu113 torchaudio === 0 .10.0+cu113 torchvision == 0 .11.1+cu113 albumentations == 1 .1.0 matplotlib == 3 .2.2 pandas == 1 .3.1 torchinfo == 1 .7.1 tqdm == 4 .64.1 wandb == 0 .12.6 Tip Something worth taking note is when you download PyTorch Library, there is a dependency link since we are downloading cuda directly, you may execute as such: ~/gaohn/YOLO ( venv ) $ pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html Setup To use setup.py file, we first create a setup.py file and add the following: create setup.py 1 ~/gaohn/YOLO ( venv ) $ touch setup.py insert into setup.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ~/gaohn/YOLO ( venv ) $ cat > setup.py # Setup installation for the application from pathlib import Path from setuptools import setup BASE_DIR = Path ( __file__ ) .parent # Load packages from requirements.txt with open ( Path ( BASE_DIR, \"requirements.txt\" )) as file: required_packages = [ ln.strip () for ln in file.readlines ()] with open ( Path ( BASE_DIR, \"dev_requirements.txt\" )) as file: dev_packages = [ ln.strip () for ln in file.readlines ()] setup ( name = \"gaohn-yolov1\" , version = \"0.1\" , license = \"MIT\" , description = \"YOLOv1 Implementation in PyTorch.\" , author = \"Hongnan G\" , author_email = \"reighns.sjr.sjh.gr.twt.hrj@gmail.com\" , url = \"\" , keywords =[ \"machine-learning\" , \"deep-learning\" , \"object-detection\" ] , classifiers =[ \"Development Status :: 3 - Alpha\" , \"Intended Audience :: Developers\" , \"Topic :: Software Development :: Build Tools\" , \"License :: OSI Approved :: MIT License\" , \"Programming Language :: Python :: 3\" , ] , python_requires = \">=3.8\" , install_requires =[ required_packages ] , extras_require ={ \"dev\" : dev_packages } , dependency_links =[] , entry_points ={ \"console_scripts\" : [ \"gaohn_yolo= src.main:app\" , ] , } , ) In line 11-12 , we are loading the required packages from requirements.txt . So instead of calling pip install -r requirements.txt , we can call the below command: installing packages 1 ~/gaohn/YOLO ( venv ) $ pip install -e . -f https://download.pytorch.org/whl/torch_stable.html However, if you are using the code repository for further developing, then packages such as linters, formatters and testing frameworks should be installed as well. In line line 14-15 , we specify a variable dev_packages to load the packages from dev_requirements.txt . creating dev_requirements.txt 1 ~/gaohn/YOLO ( venv ) $ python -m pip install -e \".[dev]\" # installs required + dev packages dev_requirements.txt 1 2 3 4 5 6 7 8 9 10 11 12 # linters, pytest, type checking bandit == 1 .7.0 black == 21 .9b0 click == 7 .1.2 coverage == 5 .5 mypy == 0 .910 pylint == 2 .7.4 pytest == 6 .2.3 types-PyYAML == 5 .4.3 types-requests == 2 .25.0 types-setuptools == 57 .4.2 types-six == 0 .1.7 One main reason why we have a separate dev_requirements.txt is we can specify this set of requirements when doing CI/CD. Later on we will use GitHub Actions to define a set of workflows in .github/workflows directory, where we will directly install the packages from dev_requirements.txt . You should see the following directory structure: main directory tree 1 2 3 4 5 \ud83d\udcc1 YOLO/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc4 \u251c\u2500\u2500 dev_requirements.txt \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py Styling and Formatting We will be on branch styling for this section. We will be using a very popular blend of style and formatting conventions that makes some very opinionated decisions on our behalf (with configurable options) 1 . black : an in-place reformatter that (mostly) adheres to PEP8. isort : sorts and formats import statements inside Python scripts. flake8 : a code linter with stylistic conventions that adhere to PEP8. We also have pyproject.toml and .flake8 to configure our formatter and linter. create pyproject.toml and .flake8 1 2 ( venv ) $ echo > pyproject.toml ( venv ) $ echo > .flake8 For example, the configuration for black below tells us that our maximum line length should be \\(79\\) characters. We also want to exclude certain file extensions and in particular the virtual environment folder we created earlier. pyproject.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Black formatting [tool.black] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_* )/ ''' You can run black --check to check if your code is formatted correctly or black . to format your code. Scripting This is on branch scripting . See 06.makefile.ipynb for more details. create bash scripts 1 2 ~/gaohn/YOLO ( venv ) $ mkdir scripts ~/gaohn/YOLO ( venv ) $ touch scripts/linter.sh linter.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ~/gaohn/YOLO ( venv ) $ cat scripts/linter.sh #!/bin/sh MINLINTSCORE = 10 if ! ( pylint --fail-under = $MINLINTSCORE --ignore-paths = venv_* main.py src ) ; then echo \"PYLINT ERROR: score below required lint score\" exit 123 else echo \"PYLINT SUCCESS!!\" fi echo \"CHECKCODE: CONGRATULATIONS, ALL TESTS SUCCESSFUL!!\" where the flag --ignore-paths=venv_* tells pylint to ignore the virtual environment folder as we do not want to lint the packages installed in the virtual environment. We can also create a script for formatting: formatter.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ~/gaohn/YOLO ( venv ) $ touch scripts/formatter.sh ~/gaohn/YOLO ( venv ) $ cat scripts/formatter.sh #!/bin/sh -eu fail = \"0\" black --version if ! black --check --exclude = \"venv_yolo\" . ; then fail = \"1\" fi if [ \" $fail \" = \"1\" ] ; then echo \"BLACK ERROR: at least one file is poorly formatted\" exit 123 else echo \"BLACK SUCCESS!!\" fi echo \"CHECKCODE: CONGRATULATIONS, ALL TESTS SUCCESSFUL!!\" Script to download data #!/bin/bash # Download VOC128 dataset (first 128 images from VOC2007 trainval) # Example usage: bash data/scripts/get_voc128.sh # parent # \u251c\u2500\u2500 src # \u2514\u2500\u2500 datasets # \u2514\u2500\u2500 voc128 \u2190 downloads here # Download/unzip images and labels datasets_path = './datasets' # unzip directory dataset_name = 'pascal_voc_128' url = https://storage.googleapis.com/reighns/datasets/pascal_voc_128.zip zip_file = 'pascal_voc_128.zip' # zip file name echo 'Downloading' $url$zip_file ' ...' mkdir -p $d wget -P $datasets_path / $dataset_name $url unzip $datasets_path / $dataset_name / $zip_file -d $datasets_path / $dataset_name rm $datasets_path / $dataset_name / $zip_file wait # finish background tasks Pre-commit This is on branch pre-commit . install pre-commit 1 2 3 4 ~/gaohn/YOLO ( venv ) $ pip install pre-commit ~/gaohn/YOLO ( venv ) $ pre-commit install pre-commit installed at .git \\h ooks \\p re-commit ```bash title=\"pre-commit-config.yaml\" linenums=\"1 create pre-commit-config.yaml 1 ~/gaohn/YOLO ( venv ) $ touch .pre-commit-config.yaml .pre-commit-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ~/gaohn/YOLO (venv) $ cat .pre-commit-config.yaml repos : - repo : local hooks : - id : linter name : Run linter entry : bash args : [ \"./scripts/linter.sh\" ] language : system pass_filenames : false - id : check_format name : Run black code formatter entry : bash args : [ \"./scripts/formatter.sh\" ] language : system pass_filenames : false means we will run linter.sh and formatter.sh before every commit. Documentation Jupyter Book Setup packages required for jupyter-book 1 2 3 4 jupyter-book == 0 .13.1 sphinx-inline-tabs == 2021 .3.28b7 sphinx-proof == 0 .1.3 myst-nb == 0 .16.0 # remember to download manually create jupyter-book 1 ~/gaohn/YOLO ( venv ) $ mkdir content You populate the content folder with your notebooks and markdown files. To build the book, run: build jupyter-book 1 ~/gaohn/YOLO ( venv ) $ jupyter-book build content Then the book will be built in the _build folder. Lastly, to serve and deploy the book, run: serve and deploy jupyter-book 1 2 ~/gaohn/YOLO ( venv ) $ mkdir .github/workflows ~/gaohn/YOLO ( venv ) $ touch .github/workflows/deploy.yml deploy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ~/gaohn/YOLO ( venv ) $ cat .github/workflows/deploy.yml name: deploy on: # Trigger the workflow on push to main branch push: branches: - main - master # This job installs dependencies, build the book, and pushes it to `gh-pages` jobs: build-and-deploy-book: runs-on: ${ { matrix.os } } strategy: matrix: os: [ ubuntu-latest ] python-version: [ 3 .8 ] steps: - uses: actions/checkout@v2 # Install dependencies - name: Set up Python ${ { matrix.python-version } } uses: actions/setup-python@v1 with: python-version: ${ { matrix.python-version } } - name: Install dependencies run: | pip install -r requirements.txt # Build the book - name: Build the book run: | jupyter-book build content # Deploy the book's HTML to gh-pages branch - name: GitHub Pages action uses: peaceiris/actions-gh-pages@v3.6.1 with: github_token: ${ { secrets.GITHUB_TOKEN } } publish_dir: content/_build/html This will deploy the book to the gh-pages branch. Remember to enable GitHub Pages in the repository settings. Mkdocs Setup We will be using Mkdocs to generate our markdown documentation into a static website. The following requirements are necessary to run mkdocs : requirements.txt 1 2 3 4 mkdocs 1.3.0 mkdocs-material 8.2.13 mkdocs-material-extensions 1.0.3 mkdocstrings 0.18.1 Initialize default template by calling mkdocs new . where . refers to the current directory. The . can be replaced with a path to your directory as well. Subsequently, a folder docs alongside with mkdocs.yml file will be created. mkdocs folder structure 1 2 3 4 5 6 7 \ud83d\udcc1 pkd_exercise_counter/ \ud83d\udcc4 \u251c\u2500\u2500 venv_pkd_exercise_counter/ \ud83d\udcc4 \u251c\u2500\u2500 docs/ \ud83d\udcc4 \u2502 \u2514\u2500\u2500 index.md \ud83d\udcc4 \u251c\u2500\u2500 mkdocs.yml \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py We can specify the following configurations in mkdocs.yml : Show/Hide mkdocs.yml mkdocs.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 site_name: Hongnan G. PeekingDuck Exercise Counter site_url: \"\" nav: - Home: index.md - PeekingDuck: - Setup: workflows.md - Push-up Counter: pushup.md theme: name: material features: - content.code.annotate markdown_extensions: - attr_list - md_in_html - admonition - footnotes - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.superfences - pymdownx.snippets - pymdownx.details - pymdownx.arithmatex: generic: true extra_javascript: - javascript/mathjax.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js extra_css: - css/extra.css plugins: - search - mkdocstrings # plugins for mkdocstrings Some of the key features include: Code block Line Numbering ; Code block Annotations ; MathJax . One missing feature is the ability to toggle code blocks. Two workarounds are provided: Toggle Using Admonition Setting Up mkdir custom_hn_push_up_counter Toggle Using HTML Setting Up mkdir custom_hn_push_up_counter We added some custom CSS and JavaScript files. In particular, we added mathjax.js for easier latex integration. You can now call mkdocs serve to start the server at a local host to view your document. Tip To link to a section or header, you can do this: [link to Styling and Formatting by general_workflow.md#styling-and-formatting . Mkdocstrings We also can create docstrings as API reference using Mkdocstrings : Install mkdocstrings: pip install mkdocstrings Place plugings to mkdocs.yml : mkdocs.yml 1 2 3 plugins: - search - mkdocstrings In mkdocs.yml 's navigation tree: mkdocs.yml 1 2 - API Documentation: - Exercise Counter: api/exercise_counter_api.md For example you have a python file called exercise_counter.py and want to render it, create a file named api/exercise_counter_api.md and in this markdown file: api/exercise_counter_api.md 1 ::: custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter # package path. Tests Set up pytest for testing codes. Install pytest 1 2 pytest == 6 .0.2 pytest-cov == 2 .10.1 In general, Pytest expects our testing codes to be grouped under a folder called tests . We can configure in our pyproject.toml file to override this if we wish to ask pytest to check from a different directory. After specifying the folder holding the test codes, pytest will then look for python scripts starting with tests_*.py ; we can also change the extensions accordingly if you want pytest to look for other kinds of files (extensions) 2 . pyproject.toml 1 2 3 4 # Pytest [ tool.pytest.ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\" CI/CD (GitHub Actions) The following content is with reference to: MLOps Basics [Week 6]: CI/CD - GitHub Actions CI/CD for Machine Learning We will be using GitHub Actions to setup our mini CI/CD. Commit Checks Commit checks is to ensure the following: The requirements can be installed on various OS and python versions. Ensure code quality and adherence to PEP8 (or other coding standards). Ensure tests are passed. lint_test.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 name : Commit Checks # (1) on : [ push , pull_request ] # (2) jobs : # (3) check_code : # (4) runs-on : ${{ matrix.os }} # (5) strategy : # (6) fail-fast : false # (7) matrix : # (8) os : [ ubuntu-latest , windows-latest ] # (9) python-version : [ 3.8 , 3.9 ] # (10) steps : # (11) - name : Checkout code # (12) uses : actions/checkout@v2 # (13) - name : Setup Python # (14) uses : actions/setup-python@v2 # (15) with : # (16) python-version : ${{ matrix.python-version }} # (17) cache : \"pip\" # (18) - name : Install dependencies # (19) run : | # (20) python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Run Black Formatter # (21) run : black --check . # (22) # - name: Run flake8 Linter # run: flake8 . # look at my pyproject.toml file and see if there is a flake8 section, if so, run flake8 on the files in the flake8 section - name : Run Pytest # (23) run : python -m coverage run --source=custom_hn_exercise_counter -m pytest && python -m coverage report # (24) This is the name that will show up under the Actions tab in GitHub. Typically, we should name it appropriately like how we indicate the subject of an email. The list here indicates the workflow will be triggered whenever someone directly pushes or submits a PR to the main branch. Once an event is triggered, a set of jobs will run on a runner . In our example, we will run a job called check_code on a runner to check for formatting and linting errors as well as run the pytest tests. This is the name of the job that will run on the runner. We specify which OS system we want the code to be run on. We can simply say ubuntu-latest or windows-latest if we just want the code to be tested on a single OS. However, here we want to check if it works on both Ubuntu and Windows, and hence we define ${{ matrix.os }} where matrix.os is [ubuntu-latest, windows-latest] . A cartesian product is created for us and the job will run on both OSs. Strategy is a way to control how the jobs are run. In our example, we want the job to run as fast as possible, so we set strategy.fail-fast to false . If one job fails, then the whole workflow will fail, this is not ideal if we want to test multiple jobs, we can set fail-fast to false to allow the workflow to continue running on the remaining jobs. Matrix is a way to control how the jobs are run. In our example, we want to run the job on both Python 3.8 and 3.9, so we set matrix.python-version to [3.8, 3.9] . This list consists of the OS that the job will run on in cartesian product. This is the python version that the job will run on in cartesian product. We can simply say 3.8 or 3.9 if we just want the code to be tested on a single python version. However, here we want to check if it works on both python 3.8 and python 3.9, and hence we define ${{ matrix.python-version }} where matrix.python-version is [3.8, 3.9] . A cartesian product is created for us and the job will run on both python versions. This is a list of dictionaries that defines the steps that will be run. Name is the name of the step that will be run. It is important to specify @v2 as if unspecified, then the workflow will use the latest version from actions/checkout template, potentially causing libraries to break. The idea here is like your requirements.txt idea, if different versions then will break. Setup Python is a step that will be run before the job. Same as above, we specify @v2 as if unspecified, then the workflow will use the latest version from actions/setup-python template, potentially causing libraries to break. With is a way to pass parameters to the step. This is the python version that the job will run on in cartesian product and if run 1 python version then can define as just say 3.7 Cache is a way to control how the libraries are installed. Install dependencies is a step that will be run before the job. | is multi-line string that runs the below code, which sets up the libraries from setup.py file. Run Black Formatter is a step that will be run before the job. Runs black with configurations from pyproject.toml file. Run Pytest is a step that will be run before the job. Runs pytest, note that I specified python -m to resolve PATH issues. Deploy to Website The other workflow for this project is to deploy the website built from Mkdocsto gh-pages branch. Show/Hide content for deploy_website.yml deploy_website.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 name : Deploy Website to GitHub Pages on : push : branches : [ master ] paths : - \"docs/**\" - \"mkdocs.yml\" - \".github/workflows/deploy_website.yml\" permissions : write-all jobs : deploy : runs-on : ubuntu-latest name : Deploy Website steps : - uses : actions/checkout@v2 - name : Set Up Python uses : actions/setup-python@v2 with : python-version : 3.8 architecture : x64 - name : Install dependencies run : | # this symbol is called a multiline string python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Build Website run : | mkdocs build - name : Push Built Website to gh-pages Branch run : | git config --global user.name 'Hongnan G.' git config --global user.email 'reighns92@users.noreply.github.com' ghp-import \\ --no-jekyll \\ --force \\ --no-history \\ --push \\ --message \"Deploying ${{ github.sha }}\" \\ site Piecing it All Together download data 1 ~/gaohn/YOLO ( venv ) $ bash scripts/download_voc128.sh This part is extracted from madewithml . \u21a9 This part is extracted from madewithml . \u21a9","title":"General Workflow"},{"location":"mlops_docs/developing/general_workflow/#git","text":"See my documentation here .","title":"Git"},{"location":"mlops_docs/developing/general_workflow/#packaging-and-setup","text":"We will be on branch setup_requirements for this section.","title":"Packaging and Setup"},{"location":"mlops_docs/developing/general_workflow/#set-up-main-directory-ide","text":"Let us assume that we are residing in our root folder ~/gaohn and we want to create a new project called YOLO , we can do as follows: creating main directory 1 2 ~/gaohn $ mkdir YOLO && cd YOLO ~/gaohn/YOLO $ code . # (1) Open the project directory in Visual Studio Code. To change appropriately if using different IDE. If you are cloning a repository to your local folder YOLO , you can also do: cloning repository 1 ~/gaohn/YOLO $ git clone https://github.com/gao-hongnan/gaohn-yolov1-pytorch.git . where . means cloning to the current directory.","title":"Set Up Main Directory (IDE)"},{"location":"mlops_docs/developing/general_workflow/#readme","text":"We can create a README.md file to describe the project. For example: README.md 1 ~/gaohn/YOLO $ touch README.md Typically, README.md includes instructions on how to install the project, how to use it, and how to contribute to it.","title":"README"},{"location":"mlops_docs/developing/general_workflow/#set-up-git","text":"Git is a version control system that is used to track changes to files. It is integral to the development process of any software. Here we initiate our main directory with git. The steps to setting up git can be found here . Be sure to have a read before proceeding. A typical git workflow on an empty repository is as follows: git 1 2 3 4 5 6 7 8 9 ~/gaohn/YOLO $ touch .gitignore ~/gaohn/YOLO $ git init ~/gaohn/YOLO $ git config --global user.name \"Your Name\" ~/gaohn/YOLO $ git config --global user.email \"your@email.com\" # (1) ~/gaohn/YOLO $ git add . ~/gaohn/YOLO $ git commit -a # (2) ~/gaohn/YOLO $ git remote add origin \"your-repo-http\" # (3) ~/gaohn/YOLO $ git remote set-url origin https:// [ token ] @github.com/ [ username ] / [ repository ] # (4) ~/gaohn/YOLO $ git push origin master -u # (5) important to set the email linked to the git account. write commit message. add remote origin. set the remote origin. push to remote origin.","title":"Set Up Git"},{"location":"mlops_docs/developing/general_workflow/#set-up-virtual-environment","text":"Follow the steps below to set up a virtual environment for your development. Windows macOS M1 Linux venv 1 2 3 ~/gaohn/YOLO $ python -m venv <venv_name> # (1) ~/gaohn/YOLO $ <venv_name> \\S cripts \\a ctivate # (2) ~/gaohn/YOLO ( venv ) $ python -m pip install --upgrade pip setuptools wheel # (3) Create virtual environment named venv in the current directory. Activate virtual environment. Upgrade pip, setuptools and wheel. venv 1 2 3 4 ~/gaohn/YOLO $ pip3 install virtualenv ~/gaohn/YOLO $ virtualenv venv_name> ~/gaohn/YOLO $ source venv_name> \\b in \\a ctivate ~/gaohn/YOLO ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel venv 1 2 3 4 ~/gaohn/YOLO $ sudo apt install python3.8 python3.8-venv python3-venv ~/gaohn/YOLO $ python3 -m venv venv_name> ~/gaohn/YOLO $ source venv_name> \\b in \\a ctivate ~/gaohn/YOLO ( venv ) $ python3 -m pip install --upgrade pip setuptools wheel You should see the following directory structure: main directory tree 1 2 \ud83d\udcc1 YOLO/ \ud83d\udcc4 \u2514\u2500\u2500 venv/","title":"Set Up Virtual Environment"},{"location":"mlops_docs/developing/general_workflow/#requirements-and-setup","text":"","title":"Requirements and Setup"},{"location":"mlops_docs/developing/general_workflow/#requirements","text":"Note For small projects, we can have requirements.txt and just run pip install -r requirements.txt . For larger projects, we can add a setup.py file. In short, requirements.txt specifies the dependencies of your project, and setup.py file informs you about the module or package-dependencies you are about to install has been packaged and distributed with Distutils, which is the standard for distributing Python Modules. You can skip setup.py if you are just using requirements.txt to install dependencies. Refer to madewithml 's requirements and setup section for more details. create requirements 1 2 ~/gaohn/YOLO ( venv ) $ touch requirements.txt ~/gaohn/YOLO ( venv ) $ pip install -r requirements.txt insert into requirements.txt 1 2 3 4 5 6 7 8 9 10 ~/gaohn/YOLO ( venv ) $ cat > requirements.txt torch == 1 .10.0+cu113 torchaudio === 0 .10.0+cu113 torchvision == 0 .11.1+cu113 albumentations == 1 .1.0 matplotlib == 3 .2.2 pandas == 1 .3.1 torchinfo == 1 .7.1 tqdm == 4 .64.1 wandb == 0 .12.6 Tip Something worth taking note is when you download PyTorch Library, there is a dependency link since we are downloading cuda directly, you may execute as such: ~/gaohn/YOLO ( venv ) $ pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html","title":"Requirements"},{"location":"mlops_docs/developing/general_workflow/#setup","text":"To use setup.py file, we first create a setup.py file and add the following: create setup.py 1 ~/gaohn/YOLO ( venv ) $ touch setup.py insert into setup.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ~/gaohn/YOLO ( venv ) $ cat > setup.py # Setup installation for the application from pathlib import Path from setuptools import setup BASE_DIR = Path ( __file__ ) .parent # Load packages from requirements.txt with open ( Path ( BASE_DIR, \"requirements.txt\" )) as file: required_packages = [ ln.strip () for ln in file.readlines ()] with open ( Path ( BASE_DIR, \"dev_requirements.txt\" )) as file: dev_packages = [ ln.strip () for ln in file.readlines ()] setup ( name = \"gaohn-yolov1\" , version = \"0.1\" , license = \"MIT\" , description = \"YOLOv1 Implementation in PyTorch.\" , author = \"Hongnan G\" , author_email = \"reighns.sjr.sjh.gr.twt.hrj@gmail.com\" , url = \"\" , keywords =[ \"machine-learning\" , \"deep-learning\" , \"object-detection\" ] , classifiers =[ \"Development Status :: 3 - Alpha\" , \"Intended Audience :: Developers\" , \"Topic :: Software Development :: Build Tools\" , \"License :: OSI Approved :: MIT License\" , \"Programming Language :: Python :: 3\" , ] , python_requires = \">=3.8\" , install_requires =[ required_packages ] , extras_require ={ \"dev\" : dev_packages } , dependency_links =[] , entry_points ={ \"console_scripts\" : [ \"gaohn_yolo= src.main:app\" , ] , } , ) In line 11-12 , we are loading the required packages from requirements.txt . So instead of calling pip install -r requirements.txt , we can call the below command: installing packages 1 ~/gaohn/YOLO ( venv ) $ pip install -e . -f https://download.pytorch.org/whl/torch_stable.html However, if you are using the code repository for further developing, then packages such as linters, formatters and testing frameworks should be installed as well. In line line 14-15 , we specify a variable dev_packages to load the packages from dev_requirements.txt . creating dev_requirements.txt 1 ~/gaohn/YOLO ( venv ) $ python -m pip install -e \".[dev]\" # installs required + dev packages dev_requirements.txt 1 2 3 4 5 6 7 8 9 10 11 12 # linters, pytest, type checking bandit == 1 .7.0 black == 21 .9b0 click == 7 .1.2 coverage == 5 .5 mypy == 0 .910 pylint == 2 .7.4 pytest == 6 .2.3 types-PyYAML == 5 .4.3 types-requests == 2 .25.0 types-setuptools == 57 .4.2 types-six == 0 .1.7 One main reason why we have a separate dev_requirements.txt is we can specify this set of requirements when doing CI/CD. Later on we will use GitHub Actions to define a set of workflows in .github/workflows directory, where we will directly install the packages from dev_requirements.txt . You should see the following directory structure: main directory tree 1 2 3 4 5 \ud83d\udcc1 YOLO/ \ud83d\udcc4 \u251c\u2500\u2500 venv/ \ud83d\udcc4 \u251c\u2500\u2500 dev_requirements.txt \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py","title":"Setup"},{"location":"mlops_docs/developing/general_workflow/#styling-and-formatting","text":"We will be on branch styling for this section. We will be using a very popular blend of style and formatting conventions that makes some very opinionated decisions on our behalf (with configurable options) 1 . black : an in-place reformatter that (mostly) adheres to PEP8. isort : sorts and formats import statements inside Python scripts. flake8 : a code linter with stylistic conventions that adhere to PEP8. We also have pyproject.toml and .flake8 to configure our formatter and linter. create pyproject.toml and .flake8 1 2 ( venv ) $ echo > pyproject.toml ( venv ) $ echo > .flake8 For example, the configuration for black below tells us that our maximum line length should be \\(79\\) characters. We also want to exclude certain file extensions and in particular the virtual environment folder we created earlier. pyproject.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Black formatting [tool.black] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_* )/ ''' You can run black --check to check if your code is formatted correctly or black . to format your code.","title":"Styling and Formatting"},{"location":"mlops_docs/developing/general_workflow/#scripting","text":"This is on branch scripting . See 06.makefile.ipynb for more details. create bash scripts 1 2 ~/gaohn/YOLO ( venv ) $ mkdir scripts ~/gaohn/YOLO ( venv ) $ touch scripts/linter.sh linter.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ~/gaohn/YOLO ( venv ) $ cat scripts/linter.sh #!/bin/sh MINLINTSCORE = 10 if ! ( pylint --fail-under = $MINLINTSCORE --ignore-paths = venv_* main.py src ) ; then echo \"PYLINT ERROR: score below required lint score\" exit 123 else echo \"PYLINT SUCCESS!!\" fi echo \"CHECKCODE: CONGRATULATIONS, ALL TESTS SUCCESSFUL!!\" where the flag --ignore-paths=venv_* tells pylint to ignore the virtual environment folder as we do not want to lint the packages installed in the virtual environment. We can also create a script for formatting: formatter.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ~/gaohn/YOLO ( venv ) $ touch scripts/formatter.sh ~/gaohn/YOLO ( venv ) $ cat scripts/formatter.sh #!/bin/sh -eu fail = \"0\" black --version if ! black --check --exclude = \"venv_yolo\" . ; then fail = \"1\" fi if [ \" $fail \" = \"1\" ] ; then echo \"BLACK ERROR: at least one file is poorly formatted\" exit 123 else echo \"BLACK SUCCESS!!\" fi echo \"CHECKCODE: CONGRATULATIONS, ALL TESTS SUCCESSFUL!!\"","title":"Scripting"},{"location":"mlops_docs/developing/general_workflow/#script-to-download-data","text":"#!/bin/bash # Download VOC128 dataset (first 128 images from VOC2007 trainval) # Example usage: bash data/scripts/get_voc128.sh # parent # \u251c\u2500\u2500 src # \u2514\u2500\u2500 datasets # \u2514\u2500\u2500 voc128 \u2190 downloads here # Download/unzip images and labels datasets_path = './datasets' # unzip directory dataset_name = 'pascal_voc_128' url = https://storage.googleapis.com/reighns/datasets/pascal_voc_128.zip zip_file = 'pascal_voc_128.zip' # zip file name echo 'Downloading' $url$zip_file ' ...' mkdir -p $d wget -P $datasets_path / $dataset_name $url unzip $datasets_path / $dataset_name / $zip_file -d $datasets_path / $dataset_name rm $datasets_path / $dataset_name / $zip_file wait # finish background tasks","title":"Script to download data"},{"location":"mlops_docs/developing/general_workflow/#pre-commit","text":"This is on branch pre-commit . install pre-commit 1 2 3 4 ~/gaohn/YOLO ( venv ) $ pip install pre-commit ~/gaohn/YOLO ( venv ) $ pre-commit install pre-commit installed at .git \\h ooks \\p re-commit ```bash title=\"pre-commit-config.yaml\" linenums=\"1 create pre-commit-config.yaml 1 ~/gaohn/YOLO ( venv ) $ touch .pre-commit-config.yaml .pre-commit-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ~/gaohn/YOLO (venv) $ cat .pre-commit-config.yaml repos : - repo : local hooks : - id : linter name : Run linter entry : bash args : [ \"./scripts/linter.sh\" ] language : system pass_filenames : false - id : check_format name : Run black code formatter entry : bash args : [ \"./scripts/formatter.sh\" ] language : system pass_filenames : false means we will run linter.sh and formatter.sh before every commit.","title":"Pre-commit"},{"location":"mlops_docs/developing/general_workflow/#documentation","text":"","title":"Documentation"},{"location":"mlops_docs/developing/general_workflow/#jupyter-book-setup","text":"packages required for jupyter-book 1 2 3 4 jupyter-book == 0 .13.1 sphinx-inline-tabs == 2021 .3.28b7 sphinx-proof == 0 .1.3 myst-nb == 0 .16.0 # remember to download manually create jupyter-book 1 ~/gaohn/YOLO ( venv ) $ mkdir content You populate the content folder with your notebooks and markdown files. To build the book, run: build jupyter-book 1 ~/gaohn/YOLO ( venv ) $ jupyter-book build content Then the book will be built in the _build folder. Lastly, to serve and deploy the book, run: serve and deploy jupyter-book 1 2 ~/gaohn/YOLO ( venv ) $ mkdir .github/workflows ~/gaohn/YOLO ( venv ) $ touch .github/workflows/deploy.yml deploy.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ~/gaohn/YOLO ( venv ) $ cat .github/workflows/deploy.yml name: deploy on: # Trigger the workflow on push to main branch push: branches: - main - master # This job installs dependencies, build the book, and pushes it to `gh-pages` jobs: build-and-deploy-book: runs-on: ${ { matrix.os } } strategy: matrix: os: [ ubuntu-latest ] python-version: [ 3 .8 ] steps: - uses: actions/checkout@v2 # Install dependencies - name: Set up Python ${ { matrix.python-version } } uses: actions/setup-python@v1 with: python-version: ${ { matrix.python-version } } - name: Install dependencies run: | pip install -r requirements.txt # Build the book - name: Build the book run: | jupyter-book build content # Deploy the book's HTML to gh-pages branch - name: GitHub Pages action uses: peaceiris/actions-gh-pages@v3.6.1 with: github_token: ${ { secrets.GITHUB_TOKEN } } publish_dir: content/_build/html This will deploy the book to the gh-pages branch. Remember to enable GitHub Pages in the repository settings.","title":"Jupyter Book Setup"},{"location":"mlops_docs/developing/general_workflow/#mkdocs-setup","text":"We will be using Mkdocs to generate our markdown documentation into a static website. The following requirements are necessary to run mkdocs : requirements.txt 1 2 3 4 mkdocs 1.3.0 mkdocs-material 8.2.13 mkdocs-material-extensions 1.0.3 mkdocstrings 0.18.1 Initialize default template by calling mkdocs new . where . refers to the current directory. The . can be replaced with a path to your directory as well. Subsequently, a folder docs alongside with mkdocs.yml file will be created. mkdocs folder structure 1 2 3 4 5 6 7 \ud83d\udcc1 pkd_exercise_counter/ \ud83d\udcc4 \u251c\u2500\u2500 venv_pkd_exercise_counter/ \ud83d\udcc4 \u251c\u2500\u2500 docs/ \ud83d\udcc4 \u2502 \u2514\u2500\u2500 index.md \ud83d\udcc4 \u251c\u2500\u2500 mkdocs.yml \ud83d\udcc4 \u251c\u2500\u2500 requirements.txt \ud83d\udcc4 \u2514\u2500\u2500 setup.py We can specify the following configurations in mkdocs.yml : Show/Hide mkdocs.yml mkdocs.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 site_name: Hongnan G. PeekingDuck Exercise Counter site_url: \"\" nav: - Home: index.md - PeekingDuck: - Setup: workflows.md - Push-up Counter: pushup.md theme: name: material features: - content.code.annotate markdown_extensions: - attr_list - md_in_html - admonition - footnotes - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.superfences - pymdownx.snippets - pymdownx.details - pymdownx.arithmatex: generic: true extra_javascript: - javascript/mathjax.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js extra_css: - css/extra.css plugins: - search - mkdocstrings # plugins for mkdocstrings Some of the key features include: Code block Line Numbering ; Code block Annotations ; MathJax . One missing feature is the ability to toggle code blocks. Two workarounds are provided: Toggle Using Admonition Setting Up mkdir custom_hn_push_up_counter Toggle Using HTML Setting Up mkdir custom_hn_push_up_counter We added some custom CSS and JavaScript files. In particular, we added mathjax.js for easier latex integration. You can now call mkdocs serve to start the server at a local host to view your document. Tip To link to a section or header, you can do this: [link to Styling and Formatting by general_workflow.md#styling-and-formatting .","title":"Mkdocs Setup"},{"location":"mlops_docs/developing/general_workflow/#mkdocstrings","text":"We also can create docstrings as API reference using Mkdocstrings : Install mkdocstrings: pip install mkdocstrings Place plugings to mkdocs.yml : mkdocs.yml 1 2 3 plugins: - search - mkdocstrings In mkdocs.yml 's navigation tree: mkdocs.yml 1 2 - API Documentation: - Exercise Counter: api/exercise_counter_api.md For example you have a python file called exercise_counter.py and want to render it, create a file named api/exercise_counter_api.md and in this markdown file: api/exercise_counter_api.md 1 ::: custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter # package path.","title":"Mkdocstrings"},{"location":"mlops_docs/developing/general_workflow/#tests","text":"Set up pytest for testing codes. Install pytest 1 2 pytest == 6 .0.2 pytest-cov == 2 .10.1 In general, Pytest expects our testing codes to be grouped under a folder called tests . We can configure in our pyproject.toml file to override this if we wish to ask pytest to check from a different directory. After specifying the folder holding the test codes, pytest will then look for python scripts starting with tests_*.py ; we can also change the extensions accordingly if you want pytest to look for other kinds of files (extensions) 2 . pyproject.toml 1 2 3 4 # Pytest [ tool.pytest.ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\"","title":"Tests"},{"location":"mlops_docs/developing/general_workflow/#cicd-github-actions","text":"The following content is with reference to: MLOps Basics [Week 6]: CI/CD - GitHub Actions CI/CD for Machine Learning We will be using GitHub Actions to setup our mini CI/CD.","title":"CI/CD (GitHub Actions)"},{"location":"mlops_docs/developing/general_workflow/#commit-checks","text":"Commit checks is to ensure the following: The requirements can be installed on various OS and python versions. Ensure code quality and adherence to PEP8 (or other coding standards). Ensure tests are passed. lint_test.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 name : Commit Checks # (1) on : [ push , pull_request ] # (2) jobs : # (3) check_code : # (4) runs-on : ${{ matrix.os }} # (5) strategy : # (6) fail-fast : false # (7) matrix : # (8) os : [ ubuntu-latest , windows-latest ] # (9) python-version : [ 3.8 , 3.9 ] # (10) steps : # (11) - name : Checkout code # (12) uses : actions/checkout@v2 # (13) - name : Setup Python # (14) uses : actions/setup-python@v2 # (15) with : # (16) python-version : ${{ matrix.python-version }} # (17) cache : \"pip\" # (18) - name : Install dependencies # (19) run : | # (20) python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Run Black Formatter # (21) run : black --check . # (22) # - name: Run flake8 Linter # run: flake8 . # look at my pyproject.toml file and see if there is a flake8 section, if so, run flake8 on the files in the flake8 section - name : Run Pytest # (23) run : python -m coverage run --source=custom_hn_exercise_counter -m pytest && python -m coverage report # (24) This is the name that will show up under the Actions tab in GitHub. Typically, we should name it appropriately like how we indicate the subject of an email. The list here indicates the workflow will be triggered whenever someone directly pushes or submits a PR to the main branch. Once an event is triggered, a set of jobs will run on a runner . In our example, we will run a job called check_code on a runner to check for formatting and linting errors as well as run the pytest tests. This is the name of the job that will run on the runner. We specify which OS system we want the code to be run on. We can simply say ubuntu-latest or windows-latest if we just want the code to be tested on a single OS. However, here we want to check if it works on both Ubuntu and Windows, and hence we define ${{ matrix.os }} where matrix.os is [ubuntu-latest, windows-latest] . A cartesian product is created for us and the job will run on both OSs. Strategy is a way to control how the jobs are run. In our example, we want the job to run as fast as possible, so we set strategy.fail-fast to false . If one job fails, then the whole workflow will fail, this is not ideal if we want to test multiple jobs, we can set fail-fast to false to allow the workflow to continue running on the remaining jobs. Matrix is a way to control how the jobs are run. In our example, we want to run the job on both Python 3.8 and 3.9, so we set matrix.python-version to [3.8, 3.9] . This list consists of the OS that the job will run on in cartesian product. This is the python version that the job will run on in cartesian product. We can simply say 3.8 or 3.9 if we just want the code to be tested on a single python version. However, here we want to check if it works on both python 3.8 and python 3.9, and hence we define ${{ matrix.python-version }} where matrix.python-version is [3.8, 3.9] . A cartesian product is created for us and the job will run on both python versions. This is a list of dictionaries that defines the steps that will be run. Name is the name of the step that will be run. It is important to specify @v2 as if unspecified, then the workflow will use the latest version from actions/checkout template, potentially causing libraries to break. The idea here is like your requirements.txt idea, if different versions then will break. Setup Python is a step that will be run before the job. Same as above, we specify @v2 as if unspecified, then the workflow will use the latest version from actions/setup-python template, potentially causing libraries to break. With is a way to pass parameters to the step. This is the python version that the job will run on in cartesian product and if run 1 python version then can define as just say 3.7 Cache is a way to control how the libraries are installed. Install dependencies is a step that will be run before the job. | is multi-line string that runs the below code, which sets up the libraries from setup.py file. Run Black Formatter is a step that will be run before the job. Runs black with configurations from pyproject.toml file. Run Pytest is a step that will be run before the job. Runs pytest, note that I specified python -m to resolve PATH issues.","title":"Commit Checks"},{"location":"mlops_docs/developing/general_workflow/#deploy-to-website","text":"The other workflow for this project is to deploy the website built from Mkdocsto gh-pages branch. Show/Hide content for deploy_website.yml deploy_website.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 name : Deploy Website to GitHub Pages on : push : branches : [ master ] paths : - \"docs/**\" - \"mkdocs.yml\" - \".github/workflows/deploy_website.yml\" permissions : write-all jobs : deploy : runs-on : ubuntu-latest name : Deploy Website steps : - uses : actions/checkout@v2 - name : Set Up Python uses : actions/setup-python@v2 with : python-version : 3.8 architecture : x64 - name : Install dependencies run : | # this symbol is called a multiline string python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Build Website run : | mkdocs build - name : Push Built Website to gh-pages Branch run : | git config --global user.name 'Hongnan G.' git config --global user.email 'reighns92@users.noreply.github.com' ghp-import \\ --no-jekyll \\ --force \\ --no-history \\ --push \\ --message \"Deploying ${{ github.sha }}\" \\ site","title":"Deploy to Website"},{"location":"mlops_docs/developing/general_workflow/#piecing-it-all-together","text":"download data 1 ~/gaohn/YOLO ( venv ) $ bash scripts/download_voc128.sh This part is extracted from madewithml . \u21a9 This part is extracted from madewithml . \u21a9","title":"Piecing it All Together"},{"location":"mlops_docs/developing/packaging_and_setup/","text":"References General Workflow \u2022 Packaging and Setup \u2022 Made With ML","title":"Packing and Setup"},{"location":"mlops_docs/developing/packaging_and_setup/#references","text":"","title":"References"},{"location":"mlops_docs/developing/pre-commit/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Pre-commit Guide by Hongnan Gao Pre-commit Intuition Before performing a commit to our local repository, there are a lot of items on our mental todo list, ranging from styling, formatting, testing, etc. And it's very easy to forget some of these steps, especially when we want to \"push to quick fix\". To help us manage all these important steps, we can use pre-commit hooks, which will automatically be triggered when we try to perform a commit. Though we can add these checks directly in our CI/CD pipeline (ex. via GitHub actions), it's significantly faster to validate our commits before pushing to our remote host and waiting to see what needs to be fixed before submitting yet another PR. So the advantage of pre-commit over CI/CD checks is the speed of knowing your errors early, and to avoid excessive PR request should your commit fails. Installation We first install the package called pre-commit . Recall that we can put these in our requirements.txt as well. # Install pre-commit pip install pre-commit pre-commit install ! source venv_reighns / bin / activate ; python3 - m pip install - q pre - commit # colab # pip3 install -q pre-commit # IDE \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 197.8/197.8 KB 5.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.6/98.6 KB 8.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 596.3/596.3 KB 21.2 MB/s eta 0:00:00 We need to initialize a git repo first before running pre-commit install . Note this git can be local! ! git init Initialized empty Git repository in /content/reighns/.git/ ! source venv_reighns / bin / activate ; pre - commit install pre-commit installed at .git/hooks/pre-commit Config Similar to creating .flake8 and pyproject.toml file in our Styling Guide , we also have to create some settings for our pre-commit files. In other words, we need to tell pre-commit what kinds of checks to perform prior to committing. For a starter, we use the default config file provided by pre-commit and add on other config later. # Simple config ! source venv_reighns / bin / activate ; pre - commit sample - config > . pre - commit - config . yaml We can see from the default template that some default checks like trailing-whitespace are already in place. ! cat . pre - commit - config . yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v3.2.0 hooks: - id: trailing-whitespace - id: end-of-file-fixer - id: check-yaml - id: check-added-large-files Hooks What are Hooks? Let us read the documentation to get a general idea of what hooks are. Built-in Inside the sample configuration, we can see that pre-commit has added some default hooks from it's repository. It specifies the location of the repository, version as well as the specific hook ids to use. We can read about the function of these hooks and add even more by exploring pre-commit's built-in hooks . Many of them also have additional arguments that we can configure to customize the hook. Be sure to explore the many other built-in hooks because there are some really useful ones that we use in our project. For example, check-merge-conflict to see if there are any lingering merge conflict strings or detect-aws-credentials if we accidently left our credentials exposed in a file, and so much more. And we can also exclude certain files from being processed by the hooks by using the optional exclude key. There are many other optional keys we can configure for each hook ID. # Inside .pre-commit-config.yaml ... - id : check-yaml exclude : \"mkdocs.yml\" ... Custom We can also define custom hooks. For example, if we want to apply formatting checks with Black as a hook, we can leverage Black's pre-commit hook. # Inside .pre-commit-config.yaml ... - repo : https://github.com/psf/black rev : 20.8b1 hooks : - id : black args : [] files : . ... This specific hook is defined under a .pre-commit-hooks.yaml inside Black's repository, as are other custom hooks under their respective package repositories. Local Finally, we can define local hooks. Recall that in Makefile we defined: # Cleaning .PHONY : clean clean : find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf And so we want the pre-commit to run this clean from Makefile . The below commands will do, note that we need to define entry as make to tell the pre-commit that make is our local command. # Inside .pre-commit-config.yaml ... - repo : local hooks : - id : clean name : clean entry : make args : [ \"clean\" ] language : system pass_filenames : false Commit Our pre-commit hooks will automatically execute when we try to make a git commit . We'll be able to see if each hook passed or failed and make any changes. If any of the hooks failed, we have to fix the corresponding file or in many instances, reformatting will occur automatically. (venv_test) reighns@HONGNANs-MacBook-Air emergency_forecast % git commit -a Trim Trailing Whitespace.................................................Passed Check Yaml...............................................................Passed Check for added large files..............................................Passed Check python ast.....................................(no files to check)Skipped Check JSON...........................................(no files to check)Skipped Check for merge conflicts................................................Passed black................................................(no files to check)Skipped flake8...............................................(no files to check)Skipped isort................................................(no files to check)Skipped pyupgrade............................................(no files to check)Skipped clean....................................................................Passed If some hooks failed, the messages will show up accordingly, most of the times the failed hooks will be formatted automatically and therefore you just need to git commit again to ensure that all hooks are passed. Run Though pre-commit hooks are meant to run before (pre) a commit, we can manually trigger all or individual hooks on all or a set of files. # Run pre-commit run --all-files # run all hooks on all files pre-commit run <HOOK_ID> --all-files # run one hook on all files pre-commit run --files <PATH_TO_FILE> # run all hooks on a file pre-commit run <HOOK_ID> --files <PATH_TO_FILE> # run one hook on a file Update In our .pre-commit-config.yaml configuration files, we've had to specify the versions for each of the repositories so we can use their latest hooks. Pre-commit has an autoupdate CLI command which will update these versions as they become available. # Autoupdate pre-commit autoupdate We can also add this command to our Makefile to execute when a development environment is created so everything is up-to-date. # Makefile ... .PHONY : install - dev install-dev : python -m pip install -e \".[dev]\" --no-cache-dir pre-commit install pre-commit autoupdate ... Tips and Cautions Match the versions between your local development environment and the pre-commit hooks. For example: - repo : https://github.com/psf/black rev : 22.3.0 hooks : - id : black args : [] files : We want to use pre-commit hooks from black to check styling and re-format when necessary. This black version should necessarily be the same as the local development envrionment in requirements.txt . By local development, it can mean: - VSCode's black formatter; - Makefile commands to use black to format; The intuition is that different versions of black may have differing changes. The versioning difference issues can be resolved if in the Update section. When you see (no files to check) Skipped message during pre-commit checks. This is because the committed files do not have the file type that requires checkings. For example, your black check in pre-commit will say no files to check if the git commit has no .py files. pre-commit will pass a list of staged files which match types / files to the entry listed. Your commit shows \"no files to check\" because there were no python files in your commit. You probably want to run pre-commit run --all-files when first introducing new hooks. Ignore Pre-commit Use git commit -n flag to ignore pre commit runs. By default, the pre-commit and commit-msg hooks are run. When any of --no-verify or -n is given, these are bypassed. See also githooks[5]. Workflow Workflow in IDE Assuming Mac environment: pip3 install -q pre-commit # install pre-commit in your vm pre-commit install # make sure the dir is a git repo pre-commit sample-config > .pre-commit-config.yaml # create default config file and populate with custom configurations if need be pre-commit run --all-files # run all hooks on all files in the repo during first commit git add . # add all files to the repo git commit -a # commit all files and pre-commit will automatically run all hooks References Offical Pre-commit documentation Pre-commit madewithml Git Hooks","title":"Pre-commit"},{"location":"mlops_docs/developing/pre-commit/#pre-commit-guide","text":"by Hongnan Gao","title":"Pre-commit Guide"},{"location":"mlops_docs/developing/pre-commit/#pre-commit","text":"","title":"Pre-commit"},{"location":"mlops_docs/developing/pre-commit/#intuition","text":"Before performing a commit to our local repository, there are a lot of items on our mental todo list, ranging from styling, formatting, testing, etc. And it's very easy to forget some of these steps, especially when we want to \"push to quick fix\". To help us manage all these important steps, we can use pre-commit hooks, which will automatically be triggered when we try to perform a commit. Though we can add these checks directly in our CI/CD pipeline (ex. via GitHub actions), it's significantly faster to validate our commits before pushing to our remote host and waiting to see what needs to be fixed before submitting yet another PR. So the advantage of pre-commit over CI/CD checks is the speed of knowing your errors early, and to avoid excessive PR request should your commit fails.","title":"Intuition"},{"location":"mlops_docs/developing/pre-commit/#installation","text":"We first install the package called pre-commit . Recall that we can put these in our requirements.txt as well. # Install pre-commit pip install pre-commit pre-commit install ! source venv_reighns / bin / activate ; python3 - m pip install - q pre - commit # colab # pip3 install -q pre-commit # IDE \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 197.8/197.8 KB 5.1 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.6/98.6 KB 8.3 MB/s eta 0:00:00 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 596.3/596.3 KB 21.2 MB/s eta 0:00:00 We need to initialize a git repo first before running pre-commit install . Note this git can be local! ! git init Initialized empty Git repository in /content/reighns/.git/ ! source venv_reighns / bin / activate ; pre - commit install pre-commit installed at .git/hooks/pre-commit","title":"Installation"},{"location":"mlops_docs/developing/pre-commit/#config","text":"Similar to creating .flake8 and pyproject.toml file in our Styling Guide , we also have to create some settings for our pre-commit files. In other words, we need to tell pre-commit what kinds of checks to perform prior to committing. For a starter, we use the default config file provided by pre-commit and add on other config later. # Simple config ! source venv_reighns / bin / activate ; pre - commit sample - config > . pre - commit - config . yaml We can see from the default template that some default checks like trailing-whitespace are already in place. ! cat . pre - commit - config . yaml # See https://pre-commit.com for more information # See https://pre-commit.com/hooks.html for more hooks repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v3.2.0 hooks: - id: trailing-whitespace - id: end-of-file-fixer - id: check-yaml - id: check-added-large-files","title":"Config"},{"location":"mlops_docs/developing/pre-commit/#hooks","text":"What are Hooks? Let us read the documentation to get a general idea of what hooks are.","title":"Hooks"},{"location":"mlops_docs/developing/pre-commit/#built-in","text":"Inside the sample configuration, we can see that pre-commit has added some default hooks from it's repository. It specifies the location of the repository, version as well as the specific hook ids to use. We can read about the function of these hooks and add even more by exploring pre-commit's built-in hooks . Many of them also have additional arguments that we can configure to customize the hook. Be sure to explore the many other built-in hooks because there are some really useful ones that we use in our project. For example, check-merge-conflict to see if there are any lingering merge conflict strings or detect-aws-credentials if we accidently left our credentials exposed in a file, and so much more. And we can also exclude certain files from being processed by the hooks by using the optional exclude key. There are many other optional keys we can configure for each hook ID. # Inside .pre-commit-config.yaml ... - id : check-yaml exclude : \"mkdocs.yml\" ...","title":"Built-in"},{"location":"mlops_docs/developing/pre-commit/#custom","text":"We can also define custom hooks. For example, if we want to apply formatting checks with Black as a hook, we can leverage Black's pre-commit hook. # Inside .pre-commit-config.yaml ... - repo : https://github.com/psf/black rev : 20.8b1 hooks : - id : black args : [] files : . ... This specific hook is defined under a .pre-commit-hooks.yaml inside Black's repository, as are other custom hooks under their respective package repositories.","title":"Custom"},{"location":"mlops_docs/developing/pre-commit/#local","text":"Finally, we can define local hooks. Recall that in Makefile we defined: # Cleaning .PHONY : clean clean : find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf And so we want the pre-commit to run this clean from Makefile . The below commands will do, note that we need to define entry as make to tell the pre-commit that make is our local command. # Inside .pre-commit-config.yaml ... - repo : local hooks : - id : clean name : clean entry : make args : [ \"clean\" ] language : system pass_filenames : false","title":"Local"},{"location":"mlops_docs/developing/pre-commit/#commit","text":"Our pre-commit hooks will automatically execute when we try to make a git commit . We'll be able to see if each hook passed or failed and make any changes. If any of the hooks failed, we have to fix the corresponding file or in many instances, reformatting will occur automatically. (venv_test) reighns@HONGNANs-MacBook-Air emergency_forecast % git commit -a Trim Trailing Whitespace.................................................Passed Check Yaml...............................................................Passed Check for added large files..............................................Passed Check python ast.....................................(no files to check)Skipped Check JSON...........................................(no files to check)Skipped Check for merge conflicts................................................Passed black................................................(no files to check)Skipped flake8...............................................(no files to check)Skipped isort................................................(no files to check)Skipped pyupgrade............................................(no files to check)Skipped clean....................................................................Passed If some hooks failed, the messages will show up accordingly, most of the times the failed hooks will be formatted automatically and therefore you just need to git commit again to ensure that all hooks are passed.","title":"Commit"},{"location":"mlops_docs/developing/pre-commit/#run","text":"Though pre-commit hooks are meant to run before (pre) a commit, we can manually trigger all or individual hooks on all or a set of files. # Run pre-commit run --all-files # run all hooks on all files pre-commit run <HOOK_ID> --all-files # run one hook on all files pre-commit run --files <PATH_TO_FILE> # run all hooks on a file pre-commit run <HOOK_ID> --files <PATH_TO_FILE> # run one hook on a file","title":"Run"},{"location":"mlops_docs/developing/pre-commit/#update","text":"In our .pre-commit-config.yaml configuration files, we've had to specify the versions for each of the repositories so we can use their latest hooks. Pre-commit has an autoupdate CLI command which will update these versions as they become available. # Autoupdate pre-commit autoupdate We can also add this command to our Makefile to execute when a development environment is created so everything is up-to-date. # Makefile ... .PHONY : install - dev install-dev : python -m pip install -e \".[dev]\" --no-cache-dir pre-commit install pre-commit autoupdate ...","title":"Update"},{"location":"mlops_docs/developing/pre-commit/#tips-and-cautions","text":"Match the versions between your local development environment and the pre-commit hooks. For example: - repo : https://github.com/psf/black rev : 22.3.0 hooks : - id : black args : [] files : We want to use pre-commit hooks from black to check styling and re-format when necessary. This black version should necessarily be the same as the local development envrionment in requirements.txt . By local development, it can mean: - VSCode's black formatter; - Makefile commands to use black to format; The intuition is that different versions of black may have differing changes. The versioning difference issues can be resolved if in the Update section. When you see (no files to check) Skipped message during pre-commit checks. This is because the committed files do not have the file type that requires checkings. For example, your black check in pre-commit will say no files to check if the git commit has no .py files. pre-commit will pass a list of staged files which match types / files to the entry listed. Your commit shows \"no files to check\" because there were no python files in your commit. You probably want to run pre-commit run --all-files when first introducing new hooks.","title":"Tips and Cautions"},{"location":"mlops_docs/developing/pre-commit/#ignore-pre-commit","text":"Use git commit -n flag to ignore pre commit runs. By default, the pre-commit and commit-msg hooks are run. When any of --no-verify or -n is given, these are bypassed. See also githooks[5].","title":"Ignore Pre-commit"},{"location":"mlops_docs/developing/pre-commit/#workflow","text":"","title":"Workflow"},{"location":"mlops_docs/developing/pre-commit/#workflow-in-ide","text":"Assuming Mac environment: pip3 install -q pre-commit # install pre-commit in your vm pre-commit install # make sure the dir is a git repo pre-commit sample-config > .pre-commit-config.yaml # create default config file and populate with custom configurations if need be pre-commit run --all-files # run all hooks on all files in the repo during first commit git add . # add all files to the repo git commit -a # commit all files and pre-commit will automatically run all hooks","title":"Workflow in IDE"},{"location":"mlops_docs/developing/pre-commit/#references","text":"Offical Pre-commit documentation Pre-commit madewithml Git Hooks","title":"References"},{"location":"mlops_docs/developing/scripting/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Scripting References General Workflow \u2022 Scripting \u2022 Made With ML Shell Scripts Makefile Intuition Ranging from pip installing to doing styling checks using black . , we have just started and there are already so many different commands to keep track of. To help with this, we're going to use a Makefile which is a automation tool that organizes our commands. This makes it very easy for us to organize relevant commands as well as organize it for others who may be new to our application. If you are confused, just imagine Makefile as a file to keep track of the commands you want the users to run, organized in a manner that is easily called. Hop on in this tutorial to gain a hands on experience. https://github.com/drivendata/cookiecutter-data-science/blob/master/%7B%7B%20cookiecutter.repo_name%20%7D%7D/Makefile touch Makefile line 2 : SHELL := /bin/bash SHELL is the Makefile variable that sets the preferred shell to use. The default on is /bin/sh , so if you need Bash features, you might have to set this variable. What is Shell Bash? Choosing Shell in Makefile We first install the packages needed. ! make -- version GNU Make 4.1 Built for x86_64-pc-linux-gnu Copyright (C) 1988-2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. ! pip install - q black flake8 isort |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4 MB 4.3 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64 kB 2.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 57.8 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 843 kB 41.8 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 96 kB 6.1 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69 kB 7.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 832 kB/s ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible. flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.2 which is incompatible. We define a global variable: BASE_DIR which points to one level up, its root folder. We verify that it is /content (in google colab). from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () print ( BASE_DIR ) /content Write a python file named test.py into BASE_DIR . The file contains the function we talked about just now. We now write this function into the file test.py . %% writefile { BASE_DIR } / Makefile # Makefile SHELL := / bin / bash VM_NAME := venv_test say_hello : @echo \"Hello World\" . PHONY : help help : @echo \"Commands:\" @echo \"venv : creates development environment.\" @echo \"style : runs style formatting.\" @echo \"clean : cleans all unnecessary files.\" Writing /content/Makefile ! make help Commands: venv : creates development environment. style : runs style formatting. clean : cleans all unnecessary files. %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Writing /content/test.py As detailed in the earlier section, we set some configurations for the formatter black and write these in pyproject.toml file. Note that we excluded folders like the virtual environments venv_ae . As a reminder, we do not want our formatter and linter to check on every file in our code base. Even though this example here is not directly applicable, we should take note during production. %% writefile { BASE_DIR } / pyproject . toml # Black formatting [ tool . black ] line - length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_ae )/ ''' Writing /content/pyproject.toml Before we run the black formatter, we call %pycat to view the python file and take note that in line 2 , the line length definitely exceeded \\(79\\) . % pycat { BASE_DIR } / test . py To use these tools that we've configured, we could run these commands individually such as calling black . where . signifies that the configuration file for that package is in the current directory. ! black . reformatted test.py All done! \u2728 \ud83c\udf70 \u2728 1 file reformatted . We see that the console said the files are formatted. We can call %pycat once again to check the code is indeed formatted! % pycat { BASE_DIR } / test . py We can repeat the steps for our flake8 file. We will shorten the example here, but for completeness sake we re-initialize test.py and see what our flake8 has to say. %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Overwriting /content/test.py %% writefile { BASE_DIR } /. flake8 [ flake8 ] exclude = venv ignore = W503 , E226 # E501 max - line - length = 79 # E501: Line too long # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Overwriting /content/.flake8 ! flake8 ./test.py:2:80: E501 line too long (84 > 79 characters) ./test.py:5:21: W292 no newline at end of file In the original example, the author ignores E501: Line too long clause in order to avoid conflicts with black . However, I included it to show as an example. Even though flake8 highlights the issue, it will not automatically format the code! It also did not seem to have the uncessary else after return statement (perhaps this is under pylint and not flake8 ). We have seen the formatter and linter in action. I'll also provide a basic version if you are working in VSCode. cd \"to your desired directory\" code . # opens vscode touch test.py # touch is mac command to create a new file, upon creation, add in the code touch .flake8 # add in the configurations touch pyproject.toml # add in the configurations black . # runs black config from pyproject and formats code in-place isort . # runs isort config from pyproject and formats code in-place flake8 # runs flake8","title":"Scripting"},{"location":"mlops_docs/developing/scripting/#scripting","text":"","title":"Scripting"},{"location":"mlops_docs/developing/scripting/#references","text":"","title":"References"},{"location":"mlops_docs/developing/scripting/#general-workflow-scripting-made-with-ml","text":"","title":"General Workflow \n   \u2022 \n  Scripting\n   \u2022 \n  Made With ML"},{"location":"mlops_docs/developing/scripting/#shell-scripts","text":"","title":"Shell Scripts"},{"location":"mlops_docs/developing/scripting/#makefile","text":"","title":"Makefile"},{"location":"mlops_docs/developing/scripting/#intuition","text":"Ranging from pip installing to doing styling checks using black . , we have just started and there are already so many different commands to keep track of. To help with this, we're going to use a Makefile which is a automation tool that organizes our commands. This makes it very easy for us to organize relevant commands as well as organize it for others who may be new to our application. If you are confused, just imagine Makefile as a file to keep track of the commands you want the users to run, organized in a manner that is easily called. Hop on in this tutorial to gain a hands on experience. https://github.com/drivendata/cookiecutter-data-science/blob/master/%7B%7B%20cookiecutter.repo_name%20%7D%7D/Makefile touch Makefile line 2 : SHELL := /bin/bash SHELL is the Makefile variable that sets the preferred shell to use. The default on is /bin/sh , so if you need Bash features, you might have to set this variable. What is Shell Bash? Choosing Shell in Makefile We first install the packages needed. ! make -- version GNU Make 4.1 Built for x86_64-pc-linux-gnu Copyright (C) 1988-2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. ! pip install - q black flake8 isort |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4 MB 4.3 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64 kB 2.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 57.8 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 843 kB 41.8 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 96 kB 6.1 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69 kB 7.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 832 kB/s ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible. flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.2 which is incompatible. We define a global variable: BASE_DIR which points to one level up, its root folder. We verify that it is /content (in google colab). from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () print ( BASE_DIR ) /content Write a python file named test.py into BASE_DIR . The file contains the function we talked about just now. We now write this function into the file test.py . %% writefile { BASE_DIR } / Makefile # Makefile SHELL := / bin / bash VM_NAME := venv_test say_hello : @echo \"Hello World\" . PHONY : help help : @echo \"Commands:\" @echo \"venv : creates development environment.\" @echo \"style : runs style formatting.\" @echo \"clean : cleans all unnecessary files.\" Writing /content/Makefile ! make help Commands: venv : creates development environment. style : runs style formatting. clean : cleans all unnecessary files. %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Writing /content/test.py As detailed in the earlier section, we set some configurations for the formatter black and write these in pyproject.toml file. Note that we excluded folders like the virtual environments venv_ae . As a reminder, we do not want our formatter and linter to check on every file in our code base. Even though this example here is not directly applicable, we should take note during production. %% writefile { BASE_DIR } / pyproject . toml # Black formatting [ tool . black ] line - length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_ae )/ ''' Writing /content/pyproject.toml Before we run the black formatter, we call %pycat to view the python file and take note that in line 2 , the line length definitely exceeded \\(79\\) . % pycat { BASE_DIR } / test . py To use these tools that we've configured, we could run these commands individually such as calling black . where . signifies that the configuration file for that package is in the current directory. ! black . reformatted test.py All done! \u2728 \ud83c\udf70 \u2728 1 file reformatted . We see that the console said the files are formatted. We can call %pycat once again to check the code is indeed formatted! % pycat { BASE_DIR } / test . py We can repeat the steps for our flake8 file. We will shorten the example here, but for completeness sake we re-initialize test.py and see what our flake8 has to say. %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Overwriting /content/test.py %% writefile { BASE_DIR } /. flake8 [ flake8 ] exclude = venv ignore = W503 , E226 # E501 max - line - length = 79 # E501: Line too long # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Overwriting /content/.flake8 ! flake8 ./test.py:2:80: E501 line too long (84 > 79 characters) ./test.py:5:21: W292 no newline at end of file In the original example, the author ignores E501: Line too long clause in order to avoid conflicts with black . However, I included it to show as an example. Even though flake8 highlights the issue, it will not automatically format the code! It also did not seem to have the uncessary else after return statement (perhaps this is under pylint and not flake8 ). We have seen the formatter and linter in action. I'll also provide a basic version if you are working in VSCode. cd \"to your desired directory\" code . # opens vscode touch test.py # touch is mac command to create a new file, upon creation, add in the code touch .flake8 # add in the configurations touch pyproject.toml # add in the configurations black . # runs black config from pyproject and formats code in-place isort . # runs isort config from pyproject and formats code in-place flake8 # runs flake8","title":"Intuition"},{"location":"mlops_docs/developing/styling/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Styling Intuition Code is read more often than it is written. -- Guido Van Rossum (author of Python) When we write a piece of code, it's almost never the last time we see it or the last time it's edited. So we need to explain what's going on (via documentation) but also make it easy to read. One of the easiest ways to make code more readable is to follow consistent style and formatting conventions. There are many options when it comes to Python style conventions to adhere to, but most are based on PEP8 conventions. You'll notice that different teams follow different conventions and that's perfectly alright. The most important aspects are that everybody is consistently following the same convection and that there are pipelines in place to automatically and effortlessly ensure that consistency. Let's see what this looks like in our application. One prime example is standardizing the line length of each line of code within a team, this makes us easier to refer to \"a line number\" when discussing code reviews. Imagine the case where we have different standards on line length, then one man's line \\(30\\) may be other's line \\(50\\) . PEP8 PEP8 guide details many common conventions for coding. The list below is by no means exhaustive, but are some common coding styles that big organizations practice. Indentation level should be at \\(4\\) spaces (a tab), in general, if the indentation is fixed at \\(k\\) , the python code will work, but we follow the \\(4\\) spaces convention. Maximum line length should be at \\(79\\) maximum. This, however, is subjective and different organizations do it differently. The key is, to maintain consistency across teams. I believe the \\(79\\) number stems from multiple reasons, one of which is many web browsers do not offer dynamic wrapping, and a super long line of code will turn out extremely ugly. Variable namings, it should go without saying that namings are important. We are writing code for us and therefore readability is important. Consider the following example: x = \"hongnan\" # bad example name = \"hongnan\" . # good example The variable x represents a person's name, but it is vague as x can literally mean anything. Thus, changing x to name is a much better choice. Imports, there's many rich history on how importing should be, but one thing to bear in mind forever is stop wildcard imports . This is extremely bad for other developers. Consider the below example: from .src.main import * You are essentially importing all functions from src.main file. We will not really know which function is from where without digging deep. There are many more conventions, we can find them more in the guide in reference. Tools Formatter, Sorter and Linter We will be using a very popular blend of style and formatting conventions that makes some very opinionated decisions on our behalf (with configurable options). black : an in-place reformatter that (mostly) adheres to PEP8. isort : sorts and formats import statements inside Python scripts. flake8 : a code linter with stylistic conventions that adhere to PEP8. We have installed these libraries prior: \"black==20.8b1\" , \"flake8==3.8.3\" , \"isort==5.5.3\" , Difference between Linter and Formatter The difference might be nuanced and isn't clear. The tagline, linters for catching errors and quality, formatters to fix code formatting style can be demonstrated with an example: def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Our linter pylint will complain Unnecessary \"else\" after \"return\" (no-else-return) as this is not a good habit of using an else when you could have just return False without else . This is where our flake8 linter comes into play. Note that black won't catch this issue. Our linter and formatter will also see another glaring issue, that is the if line is too long, exceeding the PEP8 standard of \\(79\\) length. Both black and flake8 will tell us this, but black will perform an in-place treatment, formatting the code on the go for you, whereas flake8 will just tell you. Therefore, the coding world generally uses a formatter ( black ) and a linter ( flake8 ) in tandem. We can read the below articles for more info: https://sbarnea.com/lint/black/ Differences between code linters and formatters Format Code vs Lint Code Configuration Before we can properly use these tools, we'll have to configure them because they may have some discrepancies amongst them since they follow slightly different conventions that extend from PEP8. To configure the aforementioned tools such as black, we could just pass in options using the CLI method, but it's much more efficient (especially so others can easily find all our configurations) to do this through a file. So we'll need to create a pyproject.toml file and place some configurations we have. More specifically, we define the parameters and rules in which we want our linter and formatter to check in pyproject.toml . Configuring Formatter and Sorter with pyproject.toml We create a pyproject.toml file and put in the below. # Black formatting [tool.black] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_ae )/ ''' Here we're telling Black that our maximum line length should be \\(79\\) characters and to include and exclude certain file extensions. Take note that we should definitely exclude our virtual environment folder, in my example, my vm folder is called venv_ae and it should differ from users to users. We're going to follow the same configuration steps in our pyproject.toml file for configuring isort as well. Place the following configurations right below black 's configurations. # iSort [tool.isort] profile = \"black\" line_length = 79 multi_line_output = 3 include_trailing_comma = true skip_gitignore = true virtual_env = \"venv_ae\" Though there is a complete list of configuration options for isort, we've decided to set these explicitly so it works well with Black. Configuring Linter with .flake8 Lastly, we'll set up flake8 but this time we need to create a separate .flake8 file ( flake8 has its own config file) and place the following configurations: [flake8] exclude = venv ignore = E501, W503, E226 max-line-length = 79 # E501: Line too long # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Here we setting up some configurations like before but we're including an ignore option to ignore certain flake8 rules so everything works with our black and isort configurations. This is to prevent some conflicts from black and flake8 . Example Usage We include a reproducible example on google colab to help visualize the workflow. Google Colab Walkthrough We first install the packages needed. ! pip install - q black flake8 isort |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4 MB 5.3 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64 kB 2.4 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 61.2 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 96 kB 5.1 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 843 kB 57.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 782 kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69 kB 6.2 MB/s ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible. flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.2 which is incompatible. We define a global variable: BASE_DIR which points to one level up, its root folder. We verify that it is /content (in google colab). from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () print ( BASE_DIR ) /content Write a python file named test.py into BASE_DIR . The file contains the function we talked about just now. We now write this function into the file test.py . %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Writing /content/test.py As detailed in the earlier section, we set some configurations for the formatter black and write these in pyproject.toml file. Note that we excluded folders like the virtual environments venv_ae . As a reminder, we do not want our formatter and linter to check on every file in our code base. Even though this example here is not directly applicable, we should take note during production. %% writefile { BASE_DIR } / pyproject . toml # Black formatting [ tool . black ] line - length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_ae )/ ''' Writing /content/pyproject.toml Before we run the black formatter, we call %pycat to view the python file and take note that in line 2 , the line length definitely exceeded \\(79\\) . % pycat { BASE_DIR } / test . py To use these tools that we've configured, we could run these commands individually such as calling black . where . signifies that the configuration file for that package is in the current directory. ! black . reformatted test.py All done! \u2728 \ud83c\udf70 \u2728 1 file reformatted . We see that the console said the files are formatted. We can call %pycat once again to check the code is indeed formatted! % pycat { BASE_DIR } / test . py We can repeat the steps for our flake8 file. We will shorten the example here, but for completeness sake we re-initialize test.py and see what our flake8 has to say. %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Overwriting /content/test.py %% writefile { BASE_DIR } /. flake8 [ flake8 ] exclude = venv ignore = W503 , E226 # E501 max - line - length = 79 # E501: Line too long # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Overwriting /content/.flake8 ! flake8 ./test.py:5:21: W292 no newline at end of file In the original example, the author ignores E501: Line too long clause in order to avoid conflicts with black . However, I included it to show as an example. Even though flake8 highlights the issue, it will not automatically format the code! It also did not seem to have the uncessary else after return statement (perhaps this is under pylint and not flake8 ). Workflow Workflow in IDE Here is the command line if you are working in VSCode. cd \"to your desired directory\" # change dir to your working directory code . # open your VSCode touch test.py # touch is mac command to create a new file, upon creation, add in the code touch .flake8 # add in the configurations for flake8 touch pyproject.toml # add in the configurations for black and isort black . # runs black config from pyproject and formats code in-place isort . # runs isort config from pyproject and formats code in-place flake8 # runs flake8 config from .flake8 add flake8 ... if it hangs Workflow in Google Colab To fill in Next Steps Let us see what we can further do to automate this step. Branch Create a branch and make a tutorial on this styling. Makefile We will mention in Makefile on how to call these commands. Pre-commit We may sometimes forget to run these style checks after we finish development. We'll cover how to automate this process using pre-commit so that these checks are automatically executed whenever we want to commit our code. References PEP8 guide What really is pyproject.toml? MLOps madewithml https://sbarnea.com/lint/black/ Differences between code linters and formatters Pre-commits Styling","title":"Styling"},{"location":"mlops_docs/developing/styling/#styling","text":"","title":"Styling"},{"location":"mlops_docs/developing/styling/#intuition","text":"Code is read more often than it is written. -- Guido Van Rossum (author of Python) When we write a piece of code, it's almost never the last time we see it or the last time it's edited. So we need to explain what's going on (via documentation) but also make it easy to read. One of the easiest ways to make code more readable is to follow consistent style and formatting conventions. There are many options when it comes to Python style conventions to adhere to, but most are based on PEP8 conventions. You'll notice that different teams follow different conventions and that's perfectly alright. The most important aspects are that everybody is consistently following the same convection and that there are pipelines in place to automatically and effortlessly ensure that consistency. Let's see what this looks like in our application. One prime example is standardizing the line length of each line of code within a team, this makes us easier to refer to \"a line number\" when discussing code reviews. Imagine the case where we have different standards on line length, then one man's line \\(30\\) may be other's line \\(50\\) .","title":"Intuition"},{"location":"mlops_docs/developing/styling/#pep8","text":"PEP8 guide details many common conventions for coding. The list below is by no means exhaustive, but are some common coding styles that big organizations practice. Indentation level should be at \\(4\\) spaces (a tab), in general, if the indentation is fixed at \\(k\\) , the python code will work, but we follow the \\(4\\) spaces convention. Maximum line length should be at \\(79\\) maximum. This, however, is subjective and different organizations do it differently. The key is, to maintain consistency across teams. I believe the \\(79\\) number stems from multiple reasons, one of which is many web browsers do not offer dynamic wrapping, and a super long line of code will turn out extremely ugly. Variable namings, it should go without saying that namings are important. We are writing code for us and therefore readability is important. Consider the following example: x = \"hongnan\" # bad example name = \"hongnan\" . # good example The variable x represents a person's name, but it is vague as x can literally mean anything. Thus, changing x to name is a much better choice. Imports, there's many rich history on how importing should be, but one thing to bear in mind forever is stop wildcard imports . This is extremely bad for other developers. Consider the below example: from .src.main import * You are essentially importing all functions from src.main file. We will not really know which function is from where without digging deep. There are many more conventions, we can find them more in the guide in reference.","title":"PEP8"},{"location":"mlops_docs/developing/styling/#tools","text":"","title":"Tools"},{"location":"mlops_docs/developing/styling/#formatter-sorter-and-linter","text":"We will be using a very popular blend of style and formatting conventions that makes some very opinionated decisions on our behalf (with configurable options). black : an in-place reformatter that (mostly) adheres to PEP8. isort : sorts and formats import statements inside Python scripts. flake8 : a code linter with stylistic conventions that adhere to PEP8. We have installed these libraries prior: \"black==20.8b1\" , \"flake8==3.8.3\" , \"isort==5.5.3\" ,","title":"Formatter, Sorter and Linter"},{"location":"mlops_docs/developing/styling/#difference-between-linter-and-formatter","text":"The difference might be nuanced and isn't clear. The tagline, linters for catching errors and quality, formatters to fix code formatting style can be demonstrated with an example: def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Our linter pylint will complain Unnecessary \"else\" after \"return\" (no-else-return) as this is not a good habit of using an else when you could have just return False without else . This is where our flake8 linter comes into play. Note that black won't catch this issue. Our linter and formatter will also see another glaring issue, that is the if line is too long, exceeding the PEP8 standard of \\(79\\) length. Both black and flake8 will tell us this, but black will perform an in-place treatment, formatting the code on the go for you, whereas flake8 will just tell you. Therefore, the coding world generally uses a formatter ( black ) and a linter ( flake8 ) in tandem. We can read the below articles for more info: https://sbarnea.com/lint/black/ Differences between code linters and formatters Format Code vs Lint Code","title":"Difference between Linter and Formatter"},{"location":"mlops_docs/developing/styling/#configuration","text":"Before we can properly use these tools, we'll have to configure them because they may have some discrepancies amongst them since they follow slightly different conventions that extend from PEP8. To configure the aforementioned tools such as black, we could just pass in options using the CLI method, but it's much more efficient (especially so others can easily find all our configurations) to do this through a file. So we'll need to create a pyproject.toml file and place some configurations we have. More specifically, we define the parameters and rules in which we want our linter and formatter to check in pyproject.toml .","title":"Configuration"},{"location":"mlops_docs/developing/styling/#configuring-formatter-and-sorter-with-pyprojecttoml","text":"We create a pyproject.toml file and put in the below. # Black formatting [tool.black] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_ae )/ ''' Here we're telling Black that our maximum line length should be \\(79\\) characters and to include and exclude certain file extensions. Take note that we should definitely exclude our virtual environment folder, in my example, my vm folder is called venv_ae and it should differ from users to users. We're going to follow the same configuration steps in our pyproject.toml file for configuring isort as well. Place the following configurations right below black 's configurations. # iSort [tool.isort] profile = \"black\" line_length = 79 multi_line_output = 3 include_trailing_comma = true skip_gitignore = true virtual_env = \"venv_ae\" Though there is a complete list of configuration options for isort, we've decided to set these explicitly so it works well with Black.","title":"Configuring Formatter and Sorter with pyproject.toml"},{"location":"mlops_docs/developing/styling/#configuring-linter-with-flake8","text":"Lastly, we'll set up flake8 but this time we need to create a separate .flake8 file ( flake8 has its own config file) and place the following configurations: [flake8] exclude = venv ignore = E501, W503, E226 max-line-length = 79 # E501: Line too long # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Here we setting up some configurations like before but we're including an ignore option to ignore certain flake8 rules so everything works with our black and isort configurations. This is to prevent some conflicts from black and flake8 .","title":"Configuring Linter with .flake8"},{"location":"mlops_docs/developing/styling/#example-usage","text":"We include a reproducible example on google colab to help visualize the workflow.","title":"Example Usage"},{"location":"mlops_docs/developing/styling/#google-colab-walkthrough","text":"We first install the packages needed. ! pip install - q black flake8 isort |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4 MB 5.3 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64 kB 2.4 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 61.2 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 96 kB 5.1 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 843 kB 57.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 782 kB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69 kB 6.2 MB/s ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible. flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.2 which is incompatible. We define a global variable: BASE_DIR which points to one level up, its root folder. We verify that it is /content (in google colab). from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () print ( BASE_DIR ) /content Write a python file named test.py into BASE_DIR . The file contains the function we talked about just now. We now write this function into the file test.py . %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Writing /content/test.py As detailed in the earlier section, we set some configurations for the formatter black and write these in pyproject.toml file. Note that we excluded folders like the virtual environments venv_ae . As a reminder, we do not want our formatter and linter to check on every file in our code base. Even though this example here is not directly applicable, we should take note during production. %% writefile { BASE_DIR } / pyproject . toml # Black formatting [ tool . black ] line - length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_ae )/ ''' Writing /content/pyproject.toml Before we run the black formatter, we call %pycat to view the python file and take note that in line 2 , the line length definitely exceeded \\(79\\) . % pycat { BASE_DIR } / test . py To use these tools that we've configured, we could run these commands individually such as calling black . where . signifies that the configuration file for that package is in the current directory. ! black . reformatted test.py All done! \u2728 \ud83c\udf70 \u2728 1 file reformatted . We see that the console said the files are formatted. We can call %pycat once again to check the code is indeed formatted! % pycat { BASE_DIR } / test . py We can repeat the steps for our flake8 file. We will shorten the example here, but for completeness sake we re-initialize test.py and see what our flake8 has to say. %% writefile { BASE_DIR } / test . py def shhq ( shhq_member : str = \"hn\" ): if shhq_member in [ \"hn\" , \"cw\" , \"jun\" , \"lh\" , \"lz\" , \"mj\" , \"sz\" , \"wj\" , \"yj\" , \"zj\" ]: return True else : return False Overwriting /content/test.py %% writefile { BASE_DIR } /. flake8 [ flake8 ] exclude = venv ignore = W503 , E226 # E501 max - line - length = 79 # E501: Line too long # W503: Line break occurred before binary operator # E226: Missing white space around arithmetic operator Overwriting /content/.flake8 ! flake8 ./test.py:5:21: W292 no newline at end of file In the original example, the author ignores E501: Line too long clause in order to avoid conflicts with black . However, I included it to show as an example. Even though flake8 highlights the issue, it will not automatically format the code! It also did not seem to have the uncessary else after return statement (perhaps this is under pylint and not flake8 ).","title":"Google Colab Walkthrough"},{"location":"mlops_docs/developing/styling/#workflow","text":"","title":"Workflow"},{"location":"mlops_docs/developing/styling/#workflow-in-ide","text":"Here is the command line if you are working in VSCode. cd \"to your desired directory\" # change dir to your working directory code . # open your VSCode touch test.py # touch is mac command to create a new file, upon creation, add in the code touch .flake8 # add in the configurations for flake8 touch pyproject.toml # add in the configurations for black and isort black . # runs black config from pyproject and formats code in-place isort . # runs isort config from pyproject and formats code in-place flake8 # runs flake8 config from .flake8 add flake8 ... if it hangs","title":"Workflow in IDE"},{"location":"mlops_docs/developing/styling/#workflow-in-google-colab","text":"To fill in","title":"Workflow in Google Colab"},{"location":"mlops_docs/developing/styling/#next-steps","text":"Let us see what we can further do to automate this step.","title":"Next Steps"},{"location":"mlops_docs/developing/styling/#branch","text":"Create a branch and make a tutorial on this styling.","title":"Branch"},{"location":"mlops_docs/developing/styling/#makefile","text":"We will mention in Makefile on how to call these commands.","title":"Makefile"},{"location":"mlops_docs/developing/styling/#pre-commit","text":"We may sometimes forget to run these style checks after we finish development. We'll cover how to automate this process using pre-commit so that these checks are automatically executed whenever we want to commit our code.","title":"Pre-commit"},{"location":"mlops_docs/developing/styling/#references","text":"PEP8 guide What really is pyproject.toml? MLOps madewithml https://sbarnea.com/lint/black/ Differences between code linters and formatters Pre-commits Styling","title":"References"},{"location":"mlops_docs/git/git/","text":"Git Tutorial (Based on Atlassian) A typical workflow git init git add . git commit -m \"Initial commit\" git remote add origin <> git push -u origin master Git Branches For Personal Projects Guidelines Website and Blogging Assume that I wrote a notebook named blog.ipynb and I want to publish it to my personal blog. Here are the general guidelines. Say that I put the notebook blog.ipynb in the notebooks folder in master branch. Then before I publish, I have a collaborator named Joe who is also a collaborator of my personal blog (who acts as my editor). I will create a new branch named blog and push the notebook to the branch. Then I will commit the notebook to the branch and create a pull request for Joe to review. If Joe approves the pull request, then I will merge the branch to the master branch using squash and merge or merge commit . If Joe proposes some small changes, I will then go my branch blog and make the changes locally, and commit them with a pull request for Joe again. Once he approved, I will merge the branch to the master branch. After the merge, I will delete the branch blog as the updates are reflected in the master branch. The above can be outlined in pseudo code below: # Place the notebook `blog.ipynb` in the `notebooks` folder in master branch. # Create a new branch named `blog` and push the notebook to the branch. git checkout -b <new-branch-name> blog git status # to see the status of the current branch. # Convert the notebook to markdown if needed. jupyter nbconvert --to markdown mynotebook.ipynb # Commit the notebook to the branch. git add . git commit -a git push origin branch_name -u # Go to the pull request link, and select reviewer for review. # If no changes proposed, merge the branch to master. # If there are changes proposed, edit the codes in the branch locally and push the changes to the branch for review again. After that go to the same link and click on re-review. After review and approved by the reviewer, you have 3 merge options. Squash and Merge: Take all the commits during the review and merge them into one commit. The only downside is you cannot go back to one unique commit but you will see all commit messages tho and changes. Create a merge commit: Create a merge commit with the changes from the review. You can use this if you want git to track every single commit (messages). The merge will mean it is updated in master branch also. Then delete the branch if not in use. Handling Merge Conflicts ... Commit Courtesy Tip Files that are changed in sync should be committed together. As an example, you made changes to three files, a, b and c, if a and b are related and c is unrelated to both of them, then it is logical to do the following: git add a b git commit -a \"Commit related files\" git add c git commit -a \"Commit c file\" and https://github.com/aisingapore/PeekingDuck/blob/main/CONTRIBUTING.md GIT Readme Profile Instructions here and template .","title":"Git"},{"location":"mlops_docs/git/git/#git-branches-for-personal-projects-guidelines","text":"","title":"Git Branches For Personal Projects Guidelines"},{"location":"mlops_docs/git/git/#website-and-blogging","text":"Assume that I wrote a notebook named blog.ipynb and I want to publish it to my personal blog. Here are the general guidelines. Say that I put the notebook blog.ipynb in the notebooks folder in master branch. Then before I publish, I have a collaborator named Joe who is also a collaborator of my personal blog (who acts as my editor). I will create a new branch named blog and push the notebook to the branch. Then I will commit the notebook to the branch and create a pull request for Joe to review. If Joe approves the pull request, then I will merge the branch to the master branch using squash and merge or merge commit . If Joe proposes some small changes, I will then go my branch blog and make the changes locally, and commit them with a pull request for Joe again. Once he approved, I will merge the branch to the master branch. After the merge, I will delete the branch blog as the updates are reflected in the master branch. The above can be outlined in pseudo code below: # Place the notebook `blog.ipynb` in the `notebooks` folder in master branch. # Create a new branch named `blog` and push the notebook to the branch. git checkout -b <new-branch-name> blog git status # to see the status of the current branch. # Convert the notebook to markdown if needed. jupyter nbconvert --to markdown mynotebook.ipynb # Commit the notebook to the branch. git add . git commit -a git push origin branch_name -u # Go to the pull request link, and select reviewer for review. # If no changes proposed, merge the branch to master. # If there are changes proposed, edit the codes in the branch locally and push the changes to the branch for review again. After that go to the same link and click on re-review. After review and approved by the reviewer, you have 3 merge options. Squash and Merge: Take all the commits during the review and merge them into one commit. The only downside is you cannot go back to one unique commit but you will see all commit messages tho and changes. Create a merge commit: Create a merge commit with the changes from the review. You can use this if you want git to track every single commit (messages). The merge will mean it is updated in master branch also. Then delete the branch if not in use.","title":"Website and Blogging"},{"location":"mlops_docs/git/git/#handling-merge-conflicts","text":"...","title":"Handling Merge Conflicts"},{"location":"mlops_docs/git/git/#commit-courtesy","text":"Tip Files that are changed in sync should be committed together. As an example, you made changes to three files, a, b and c, if a and b are related and c is unrelated to both of them, then it is logical to do the following: git add a b git commit -a \"Commit related files\" git add c git commit -a \"Commit c file\" and https://github.com/aisingapore/PeekingDuck/blob/main/CONTRIBUTING.md","title":"Commit Courtesy"},{"location":"mlops_docs/git/git/#git-readme-profile","text":"Instructions here and template .","title":"GIT Readme Profile"},{"location":"mlops_docs/git/git_helpsheet/","text":"Helpsheet Tips rebase onto upstream/dev 1 2 3 $ git checkout main $ git delete -D <branch-name> $ git checkout <branch-name> This solves the issues of sometimes having the local repo between two computers not fully updating properly using git pull or git fetch . Make sure the <branch-name> in question is fully up to date on the remote repo first before deleting it locally.","title":"Helpsheet"},{"location":"mlops_docs/git/git_helpsheet/#helpsheet","text":"","title":"Helpsheet"},{"location":"mlops_docs/git/git_helpsheet/#tips","text":"rebase onto upstream/dev 1 2 3 $ git checkout main $ git delete -D <branch-name> $ git checkout <branch-name> This solves the issues of sometimes having the local repo between two computers not fully updating properly using git pull or git fetch . Make sure the <branch-name> in question is fully up to date on the remote repo first before deleting it locally.","title":"Tips"},{"location":"mlops_docs/git/install_git/","text":"Installation and Configuration Installing Git We have to ensure that we have Git installed on our machine. Different operating systems have different ways of installing Git. The following command downloads Git to macOS: install git 1 ~/gaohn $ brew install git # (1) Use Homebrew to install git manager. Once Git is installed, we can check the version of Git we have installed: check git version 1 ~/gaohn $ git --version Read More Visit Atlassian's Install Git Tutorial for more information on installing Git. Configure Name and Email In your terminal, configure your Git username and email using the following commands: configure git 1 2 ~/gaohn $ git config --global user.name <your name> ~/gaohn $ git config --global user.email <your email> replacing <your name> with your own name, and set the user.email to the email address you used to sign up for GitHub. These details will be associated with any commits that you create! Note This step is particularly important if you are working with multiple git accounts. This is because setting the config globally will affect your commits. For example, if I have two git accounts called a1 and a2 with emails a1@gmail.com and a2@gmail.com respectively. If I set my global config of the user.email to a1@gmail.com , then any commits made by a2 will be credited to a1 as contributor. Read More Visit Atlassian's Configuring Git Tutorial for more details.","title":"Install Git"},{"location":"mlops_docs/git/install_git/#installation-and-configuration","text":"","title":"Installation and Configuration"},{"location":"mlops_docs/git/install_git/#installing-git","text":"We have to ensure that we have Git installed on our machine. Different operating systems have different ways of installing Git. The following command downloads Git to macOS: install git 1 ~/gaohn $ brew install git # (1) Use Homebrew to install git manager. Once Git is installed, we can check the version of Git we have installed: check git version 1 ~/gaohn $ git --version Read More Visit Atlassian's Install Git Tutorial for more information on installing Git.","title":"Installing Git"},{"location":"mlops_docs/git/install_git/#configure-name-and-email","text":"In your terminal, configure your Git username and email using the following commands: configure git 1 2 ~/gaohn $ git config --global user.name <your name> ~/gaohn $ git config --global user.email <your email> replacing <your name> with your own name, and set the user.email to the email address you used to sign up for GitHub. These details will be associated with any commits that you create! Note This step is particularly important if you are working with multiple git accounts. This is because setting the config globally will affect your commits. For example, if I have two git accounts called a1 and a2 with emails a1@gmail.com and a2@gmail.com respectively. If I set my global config of the user.email to a1@gmail.com , then any commits made by a2 will be credited to a1 as contributor. Read More Visit Atlassian's Configuring Git Tutorial for more details.","title":"Configure Name and Email"},{"location":"mlops_docs/git/introduction/","text":"This git documentation is mostly based on Atlassian's Git Tutorial and Colt Steele's Learn Git in 15 Minutes . I have since changed my GitHub username from reigHns92 to gao-hongnan , so do not be alarmed to see the former in the documentation. Dummy Repository I created two dummy repositories to demonstrate the git workflow. The first repository is called git-tutorial and the second is called git-sample-workflow . The git-tutorial repository demonstrates the basic git commands. The git-sample-workflow demonstrates the git workflow for personal projects and how to collaborate with other people. Note that this repository is created by my alternate account. git-tutorial (reighns92) master branch 27378ad b9d3256 13e956d git-fetch branch 27378ad b9d3256 13e956d f3d4f25 git-branch branch Branch out from git-fetch branch. 27378ad b9d3256 13e956d f3d4f25 git-sample-workflow (ghnreigns) master 96e7501 2d47f2d 32f5bbd dev 96e7501 2d47f2d 32f5bbd 7e8a124 1fa405d (merged dummy PR from reighns92) e17c6f2 (additional commit for CM) git-sample-workflow (reighns92) master 96e7501 2d47f2d 32f5bbd dev 96e7501 2d47f2d 32f5bbd 7e8a124 1fa405d","title":"Introduction"},{"location":"mlops_docs/git/introduction/#dummy-repository","text":"I created two dummy repositories to demonstrate the git workflow. The first repository is called git-tutorial and the second is called git-sample-workflow . The git-tutorial repository demonstrates the basic git commands. The git-sample-workflow demonstrates the git workflow for personal projects and how to collaborate with other people. Note that this repository is created by my alternate account.","title":"Dummy Repository"},{"location":"mlops_docs/git/introduction/#git-tutorial-reighns92","text":"","title":"git-tutorial (reighns92)"},{"location":"mlops_docs/git/introduction/#master-branch","text":"27378ad b9d3256 13e956d","title":"master branch"},{"location":"mlops_docs/git/introduction/#git-fetch-branch","text":"27378ad b9d3256 13e956d f3d4f25","title":"git-fetch branch"},{"location":"mlops_docs/git/introduction/#git-branch-branch","text":"Branch out from git-fetch branch. 27378ad b9d3256 13e956d f3d4f25","title":"git-branch branch"},{"location":"mlops_docs/git/introduction/#git-sample-workflow-ghnreigns","text":"","title":"git-sample-workflow (ghnreigns)"},{"location":"mlops_docs/git/introduction/#master","text":"96e7501 2d47f2d 32f5bbd","title":"master"},{"location":"mlops_docs/git/introduction/#dev","text":"96e7501 2d47f2d 32f5bbd 7e8a124 1fa405d (merged dummy PR from reighns92) e17c6f2 (additional commit for CM)","title":"dev"},{"location":"mlops_docs/git/introduction/#git-sample-workflow-reighns92","text":"","title":"git-sample-workflow (reighns92)"},{"location":"mlops_docs/git/introduction/#master_1","text":"96e7501 2d47f2d 32f5bbd","title":"master"},{"location":"mlops_docs/git/introduction/#dev_1","text":"96e7501 2d47f2d 32f5bbd 7e8a124 1fa405d","title":"dev"},{"location":"mlops_docs/git/advanced_tips/merging_vs_rebasing/","text":"The documentation on merging vs rebasing written by Atlassian is a great resource for understanding the differences between the two approaches. Tip If you intend to use git rebase instead of git merge , then the below is a very generic workflow that you can follow. git rebase workflow $ git checkout feature $ git checkout -b temp-branch $ git rebase -i dev $ git checkout feature $ git merge temp-branch The workflow assumes that you have a dev branch that is in sync with the upstream dev branch. You are working on a feature branch called feature and after you have done developing, you want to check that your changes are compatible with the latest changes in the dev branch. You can then use interactive rebase to rebase your feature branch on top of the dev branch. I created a temp-branch to make sure that I don't mess up my history in the feature branch if anything goes wrong.","title":"Merging vs Rebasing"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/","text":"This workflow described in this section is a generic workflow, for other detailed models of workflows, refer to Atlassian's Git Workflows for more details. Fork the Repository We will first start off by going to the repository you are interested in and click on the Fork button on the top right. I will use a public repository named git-sample-workflow from my other account. This will replicate his entire repository into my own GitHub account. Remember to uncheck the box Copy the main branch only if you decide to work on the forked repository's branches as well. Clone your Forked Repository I have a fork of the test repository on my GitHub server, but to start developing, I will have to clone this forked repository locally on my computer. Depending on HTTPS or SSH key, one might copy different URLs for the forked repository. Git Clone Forked Repo Outputs git clone forked repo ~/gaohn $ git clone https://github.com/reigHns92/git-sample-workflow.git Cloning into 'git-sample-workflow'... remote: Enumerating objects: 6, done. remote: Counting objects: 100% (6/6), done. remote: Compressing objects: 100% (3/3), done. remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (6/6), done. For a more detailed step-by-step guide, GitHub Docs's Quickstart: Fork a repo provides a templated method to do fork and clone. Sync Forked Repository with Upstream Before we go on, we define two key terms in the context of GitHub Forks : origin : this refers to one's own repository (i.e. the forked and cloned repository on your personal account); upstream : generally refers to the original repository that you have forked. In general, it is also useful to understand upstreams and downstreams . Back to where we left off, you can continue to develop on the forked repository but the original repository you forked from (the upstream ) will not automatically sync with your origin . In other words, if the upstream repository made 10 new commits, your origin will have no information on those 10 new commits. To be able to sync changes, we will have to first configure a remote for the fork, and fetch from upstream to sync. Configuring a Remote for a Forked Repository The steps are referenced from GitHub Docs . They have included steps for different OS. We will reproduce the steps on macOS. Open the terminal; A handy command to check what remote repository your forked repo has. Note that one can have many remotes! list out all remotes 1 2 3 4 5 ~/gaohn $ cd git-sample-workflow ~/gaohn/git-sample-workflow $ git remote -v > origin https://github.com/reigHns92/git-sample-workflow.git ( fetch ) > origin https://github.com/reigHns92/git-sample-workflow.git ( push ) Specify a new remote upstream repository that will be synced with the fork. To do so, go to the original repository (the one you forked from) and copy the URLs (HTTPS or SSH) to add to remote (upstream). configuring remote for forked repo 1 2 3 4 ~/gaohn/git-sample-workflow $ git remote add upstream https://github.com/ghnreigns/git-sample-workflow.git > origin https://github.com/reigHns92/git-sample-workflow.git ( fetch ) > origin https://github.com/reigHns92/git-sample-workflow.git ( push ) You can verify the new upstream repository is added by checking git remove -v again. list out all remotes 1 2 3 4 5 6 ~/gaohn/git-sample-workflow $ git remote -v > origin https://github.com/reigHns92/git-sample-workflow.git ( fetch ) > origin https://github.com/reigHns92/git-sample-workflow.git ( push ) > upstream https://github.com/ghnreigns/git-sample-workflow.git ( fetch ) > upstream https://github.com/ghnreigns/git-sample-workflow.git ( push ) Syncing the Forked Repository with Upstream Now I will use my other account to make a new commit 32f5bbd on the repository's main branch (i.e the upstream) and now my forked repository will be 1 commit behind. Origin is 1 commit behind Upstream To keep my repository up to date, we will refer to GitHub Docs section on syncing a fork branch from the command line. In the same terminal, we fetch all branches and commites from the upstream repository. fetching from upstream repo 1 2 3 4 5 6 7 8 9 ~/gaohn/git-sample-workflow $ git fetch upstream > remote: Enumerating objects: 5 , done . > remote: Counting objects: 100 % ( 5 /5 ) , done . > remote: Compressing objects: 100 % ( 2 /2 ) , done . > remote: Total 3 ( delta 1 ) , reused 0 ( delta 0 ) , pack-reused 0 > Unpacking objects: 100 % ( 3 /3 ) , 711 bytes | 71 .00 KiB/s, done . > From https://github.com/ghnreigns/git-sample-workflow > * [ new branch ] main -> upstream/main Switch to your fork's main branch (or the branch you want the changes to be synced and merged). ~/gaohn/git-sample-workflow $ git checkout main Before you merge, you can use git log from the previous section to check what was committed on the upstream repository. checking what was committed on upstream 1 ~/gaohn/git-sample-workflow $ git log --oneline upstream/main..main and it indeed returns the message 32f5bbd (upstream/main) git: update README.md for new commit which was indeed the commit 32f5bbd done on my other account. Merge the changes from the upstream default branch - in this case, upstream/main - into your local default branch. This brings your fork's default branch into sync with the upstream repository, without losing your local changes. merge with upstream 1 2 3 4 5 6 ~/gaohn/git-sample-workflow $ git merge upstream/main > Updating 2d47f2d..32f5bbd > Fast-forward > README.md | 2 ++ > 1 file changed, 2 insertions ( + ) Now it is time to push the commits to the origin repository, a quick git status will tell you that you are 1 commit ahead of the origin repository. checking status 1 2 3 4 5 ~/gaohn/git-sample-workflow $ git status > On branch main > Your branch is ahead of 'origin/main' by 1 commit. > ( use \"git push\" to publish your local commits ) and we will use git push to push the commits to the origin repository. pushing to origin 1 2 3 4 5 6 ~/gaohn/git-sample-workflow $ git push -u origin main > Total 0 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 > To https://github.com/reigHns92/git-sample-workflow.git > 2d47f2d..32f5bbd main -> main > Branch 'main' set up to track remote branch 'main' from 'origin' . After this step, the origin (forked) repo of mine will be in sync with the upstream as can be verified by the message \"This branch is up to date with ghnreigns/git-sample-workflow:main.\" on your main branch github. Syncing the Forked Repository with Upstream's Other Branches Let's say our upstream repository created a new branch dev and we want to sync our forked repository with it (note that our forked repository origin does not have this branch yet). We can first git fetch upstream to fetch all branches and commits from the upstream repository. We see that git tells us the following: From https://github.com/ghnreigns/git-sample-workflow * [ new branch ] dev -> upstream/dev which means that the upstream repository has a new branch dev and it \"downloads (fetch)\" it to our local repository. Notice that the dev branch does not automatically appear in your local git (i.e. calling git checkout dev does not work). The dev branch exists in both origin and upstream as can be seen by git branch -a . There are two ways to \"fetch\" the dev repo to your local: git checkout -b dev upstream/dev will create a branch named dev that pulls the information from the upstream/dev branch; Alternatively, you can git checkout -b dev origin/main which essentially fetches the dev branch on your forked repo and then we can do git merge upstream/dev to be in sync. Creating Pull Request Now on my local dev branch, I made some changes and committed to 7e8a124 using git push -u origin dev . I now want to create a pull request for the upstream repository owner to see my changes, and to merge into his repo if necessary. We first go to our origin (forked) repository and see that there is a new popup on \"compare and pull request\". We click it and the following interface will appear, that is for you to write a message to the upstream author. Pull Request 1 Once you created pull request, you can wait for the upstream author to decide and reply. Since I am the owner of the other account, I went in and see the following: Pull Request 2 and proceeded to approve the merge request. We used the default option to merge the pull request, which is to merge into the dev branch of the upstream repository and create a new commit (SHA 1fa405d). We can have different types of merge which we can read up more here . Consequently, you will need to repeat the following steps to sync your forked repository's dev branch with the upstream repository's dev branch: syncing forked repo with upstream 1 2 3 4 ~/gaohn/git-sample-workflow $ git checkout dev ~/gaohn/git-sample-workflow $ git fetch upstream ~/gaohn/git-sample-workflow $ git merge upstream/dev ~/gaohn/git-sample-workflow $ git push -u origin dev even though the point is slightly moot since this step is exactly the same as the previous commit. Git Rebase (My Workflow) vs Git Merge We discuss how we can use git rebase to clean up commits before merging into the upstream repository. This workflow can be extremely versatile, so I will only be discussing the workflow that I use. Git Rebase Let's say you are on your origin/dev branch and decides to create a new feature. You can create a new branch feat-hello-world and do your work on it. creating new branch 1 ~/gaohn/git-sample-workflow ( git: dev ) $ git checkout -b feat-hello-world You then create a new script hello_world.py which prints \"Hello World!\" and commit it to your local repository. creating new script 1 2 3 4 5 6 7 ~/gaohn/git-sample-workflow $ touch hello_world.py # add print(\"Hello World!\") ~/gaohn/git-sample-workflow $ git add hello_world.py ~/gaohn/git-sample-workflow $ git commit -m \"feat: add new feature hello_world that prints 'Hello World!'\" [ feat-hello-world 0ee86be ] feat: add new feature hello_world that prints \"Hello World!\" 1 file changed, 1 insertion ( + ) create mode 100644 hello_world.py You then continue developing and added a new function print_message() to the hello_world script such that the function can take in a string and print it to console. adding new function 1 2 3 4 ~/gaohn/git-sample-workflow $ git commit -m \"feat: add new function to print any incoming string as message\" [ feat-hello-world ef731ee ] feat: add new function to print any incoming string as message. 1 file changed, 7 insertions ( + ) , 1 deletion ( - ) After you are done with the feature, I packaged the script hello_world into a folder named src . packaging script 1 2 3 4 5 6 7 8 ~/gaohn/git-sample-workflow $ mkdir src ~/gaohn/git-sample-workflow $ mv hello_world.py src/ ~/gaohn/git-sample-workflow $ git add src/ ~/gaohn/git-sample-workflow $ git commit -m \"build: package hello_world.py into src folder for modularity.\" [ feat-hello-world 99180da ] build: package hello_world.py into src folder for modularity. 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) rename hello_world.py = > src/hello_world.py ( 100 % ) Now I am ready to merge my feat-hello-world branch into the dev branch of the upstream repository. Before that, I want to clean up my commits because commits 0ee86be and ef731ee are quite similar in what they do, both adding new features to the hello_world script and can be merged into one commit. We can do this by using git rebase -i to interactively rebase my commits. But since we want to rebase on top of the dev branch of the upstream repository, we need to first fetch the upstream repository (if you have not done so already). Subsequently, we can rebase our feat-hello-world branch onto the dev branch of the upstream repository as follows. rebase onto upstream/dev 1 2 ~/gaohn/git-sample-workflow $ git fetch upstream # this ensures that we are up to date with the upstream repository ~/gaohn/git-sample-workflow $ git rebase -i upstream/dev This will open up a text editor with the following content, shown in the gif below. Git Rebase Note that we replaced pick of ef731ee with fixup to squash the commit into the previous commit. We did not use squash because we do not want to keep the commit message of ef731ee . Therefore, fixup will keep the commit message of 0ee86be and discard the commit message of ef731ee while squashing the commit. Now we will see a message Successfully rebased and updated refs/heads/feat-hello-world. and we can force push our branch to the remote repository. force push to remote 1 ~/gaohn/git-sample-workflow $ git push -f origin feat-hello-world We are now ready to create a pull request to merge our feat-hello-world branch into the dev branch. Note We have successfully cleaned up our commits by using git rebase -i to squash two commits into one.","title":"Contributing to Open Source Projects, A Generic Sample Workflow"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#fork-the-repository","text":"We will first start off by going to the repository you are interested in and click on the Fork button on the top right. I will use a public repository named git-sample-workflow from my other account. This will replicate his entire repository into my own GitHub account. Remember to uncheck the box Copy the main branch only if you decide to work on the forked repository's branches as well.","title":"Fork the Repository"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#clone-your-forked-repository","text":"I have a fork of the test repository on my GitHub server, but to start developing, I will have to clone this forked repository locally on my computer. Depending on HTTPS or SSH key, one might copy different URLs for the forked repository. Git Clone Forked Repo Outputs git clone forked repo ~/gaohn $ git clone https://github.com/reigHns92/git-sample-workflow.git Cloning into 'git-sample-workflow'... remote: Enumerating objects: 6, done. remote: Counting objects: 100% (6/6), done. remote: Compressing objects: 100% (3/3), done. remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (6/6), done. For a more detailed step-by-step guide, GitHub Docs's Quickstart: Fork a repo provides a templated method to do fork and clone.","title":"Clone your Forked Repository"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#sync-forked-repository-with-upstream","text":"Before we go on, we define two key terms in the context of GitHub Forks : origin : this refers to one's own repository (i.e. the forked and cloned repository on your personal account); upstream : generally refers to the original repository that you have forked. In general, it is also useful to understand upstreams and downstreams . Back to where we left off, you can continue to develop on the forked repository but the original repository you forked from (the upstream ) will not automatically sync with your origin . In other words, if the upstream repository made 10 new commits, your origin will have no information on those 10 new commits. To be able to sync changes, we will have to first configure a remote for the fork, and fetch from upstream to sync.","title":"Sync Forked Repository with Upstream"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#configuring-a-remote-for-a-forked-repository","text":"The steps are referenced from GitHub Docs . They have included steps for different OS. We will reproduce the steps on macOS. Open the terminal; A handy command to check what remote repository your forked repo has. Note that one can have many remotes! list out all remotes 1 2 3 4 5 ~/gaohn $ cd git-sample-workflow ~/gaohn/git-sample-workflow $ git remote -v > origin https://github.com/reigHns92/git-sample-workflow.git ( fetch ) > origin https://github.com/reigHns92/git-sample-workflow.git ( push ) Specify a new remote upstream repository that will be synced with the fork. To do so, go to the original repository (the one you forked from) and copy the URLs (HTTPS or SSH) to add to remote (upstream). configuring remote for forked repo 1 2 3 4 ~/gaohn/git-sample-workflow $ git remote add upstream https://github.com/ghnreigns/git-sample-workflow.git > origin https://github.com/reigHns92/git-sample-workflow.git ( fetch ) > origin https://github.com/reigHns92/git-sample-workflow.git ( push ) You can verify the new upstream repository is added by checking git remove -v again. list out all remotes 1 2 3 4 5 6 ~/gaohn/git-sample-workflow $ git remote -v > origin https://github.com/reigHns92/git-sample-workflow.git ( fetch ) > origin https://github.com/reigHns92/git-sample-workflow.git ( push ) > upstream https://github.com/ghnreigns/git-sample-workflow.git ( fetch ) > upstream https://github.com/ghnreigns/git-sample-workflow.git ( push )","title":"Configuring a Remote for a Forked Repository"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#syncing-the-forked-repository-with-upstream","text":"Now I will use my other account to make a new commit 32f5bbd on the repository's main branch (i.e the upstream) and now my forked repository will be 1 commit behind. Origin is 1 commit behind Upstream To keep my repository up to date, we will refer to GitHub Docs section on syncing a fork branch from the command line. In the same terminal, we fetch all branches and commites from the upstream repository. fetching from upstream repo 1 2 3 4 5 6 7 8 9 ~/gaohn/git-sample-workflow $ git fetch upstream > remote: Enumerating objects: 5 , done . > remote: Counting objects: 100 % ( 5 /5 ) , done . > remote: Compressing objects: 100 % ( 2 /2 ) , done . > remote: Total 3 ( delta 1 ) , reused 0 ( delta 0 ) , pack-reused 0 > Unpacking objects: 100 % ( 3 /3 ) , 711 bytes | 71 .00 KiB/s, done . > From https://github.com/ghnreigns/git-sample-workflow > * [ new branch ] main -> upstream/main Switch to your fork's main branch (or the branch you want the changes to be synced and merged). ~/gaohn/git-sample-workflow $ git checkout main Before you merge, you can use git log from the previous section to check what was committed on the upstream repository. checking what was committed on upstream 1 ~/gaohn/git-sample-workflow $ git log --oneline upstream/main..main and it indeed returns the message 32f5bbd (upstream/main) git: update README.md for new commit which was indeed the commit 32f5bbd done on my other account. Merge the changes from the upstream default branch - in this case, upstream/main - into your local default branch. This brings your fork's default branch into sync with the upstream repository, without losing your local changes. merge with upstream 1 2 3 4 5 6 ~/gaohn/git-sample-workflow $ git merge upstream/main > Updating 2d47f2d..32f5bbd > Fast-forward > README.md | 2 ++ > 1 file changed, 2 insertions ( + ) Now it is time to push the commits to the origin repository, a quick git status will tell you that you are 1 commit ahead of the origin repository. checking status 1 2 3 4 5 ~/gaohn/git-sample-workflow $ git status > On branch main > Your branch is ahead of 'origin/main' by 1 commit. > ( use \"git push\" to publish your local commits ) and we will use git push to push the commits to the origin repository. pushing to origin 1 2 3 4 5 6 ~/gaohn/git-sample-workflow $ git push -u origin main > Total 0 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 > To https://github.com/reigHns92/git-sample-workflow.git > 2d47f2d..32f5bbd main -> main > Branch 'main' set up to track remote branch 'main' from 'origin' . After this step, the origin (forked) repo of mine will be in sync with the upstream as can be verified by the message \"This branch is up to date with ghnreigns/git-sample-workflow:main.\" on your main branch github.","title":"Syncing the Forked Repository with Upstream"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#syncing-the-forked-repository-with-upstreams-other-branches","text":"Let's say our upstream repository created a new branch dev and we want to sync our forked repository with it (note that our forked repository origin does not have this branch yet). We can first git fetch upstream to fetch all branches and commits from the upstream repository. We see that git tells us the following: From https://github.com/ghnreigns/git-sample-workflow * [ new branch ] dev -> upstream/dev which means that the upstream repository has a new branch dev and it \"downloads (fetch)\" it to our local repository. Notice that the dev branch does not automatically appear in your local git (i.e. calling git checkout dev does not work). The dev branch exists in both origin and upstream as can be seen by git branch -a . There are two ways to \"fetch\" the dev repo to your local: git checkout -b dev upstream/dev will create a branch named dev that pulls the information from the upstream/dev branch; Alternatively, you can git checkout -b dev origin/main which essentially fetches the dev branch on your forked repo and then we can do git merge upstream/dev to be in sync.","title":"Syncing the Forked Repository with Upstream's Other Branches"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#creating-pull-request","text":"Now on my local dev branch, I made some changes and committed to 7e8a124 using git push -u origin dev . I now want to create a pull request for the upstream repository owner to see my changes, and to merge into his repo if necessary. We first go to our origin (forked) repository and see that there is a new popup on \"compare and pull request\". We click it and the following interface will appear, that is for you to write a message to the upstream author. Pull Request 1 Once you created pull request, you can wait for the upstream author to decide and reply. Since I am the owner of the other account, I went in and see the following: Pull Request 2 and proceeded to approve the merge request. We used the default option to merge the pull request, which is to merge into the dev branch of the upstream repository and create a new commit (SHA 1fa405d). We can have different types of merge which we can read up more here . Consequently, you will need to repeat the following steps to sync your forked repository's dev branch with the upstream repository's dev branch: syncing forked repo with upstream 1 2 3 4 ~/gaohn/git-sample-workflow $ git checkout dev ~/gaohn/git-sample-workflow $ git fetch upstream ~/gaohn/git-sample-workflow $ git merge upstream/dev ~/gaohn/git-sample-workflow $ git push -u origin dev even though the point is slightly moot since this step is exactly the same as the previous commit.","title":"Creating Pull Request"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#git-rebase-my-workflow-vs-git-merge","text":"We discuss how we can use git rebase to clean up commits before merging into the upstream repository. This workflow can be extremely versatile, so I will only be discussing the workflow that I use.","title":"Git Rebase (My Workflow) vs Git Merge"},{"location":"mlops_docs/git/collaborating/contributing_to_open_source_projects/#git-rebase","text":"Let's say you are on your origin/dev branch and decides to create a new feature. You can create a new branch feat-hello-world and do your work on it. creating new branch 1 ~/gaohn/git-sample-workflow ( git: dev ) $ git checkout -b feat-hello-world You then create a new script hello_world.py which prints \"Hello World!\" and commit it to your local repository. creating new script 1 2 3 4 5 6 7 ~/gaohn/git-sample-workflow $ touch hello_world.py # add print(\"Hello World!\") ~/gaohn/git-sample-workflow $ git add hello_world.py ~/gaohn/git-sample-workflow $ git commit -m \"feat: add new feature hello_world that prints 'Hello World!'\" [ feat-hello-world 0ee86be ] feat: add new feature hello_world that prints \"Hello World!\" 1 file changed, 1 insertion ( + ) create mode 100644 hello_world.py You then continue developing and added a new function print_message() to the hello_world script such that the function can take in a string and print it to console. adding new function 1 2 3 4 ~/gaohn/git-sample-workflow $ git commit -m \"feat: add new function to print any incoming string as message\" [ feat-hello-world ef731ee ] feat: add new function to print any incoming string as message. 1 file changed, 7 insertions ( + ) , 1 deletion ( - ) After you are done with the feature, I packaged the script hello_world into a folder named src . packaging script 1 2 3 4 5 6 7 8 ~/gaohn/git-sample-workflow $ mkdir src ~/gaohn/git-sample-workflow $ mv hello_world.py src/ ~/gaohn/git-sample-workflow $ git add src/ ~/gaohn/git-sample-workflow $ git commit -m \"build: package hello_world.py into src folder for modularity.\" [ feat-hello-world 99180da ] build: package hello_world.py into src folder for modularity. 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) rename hello_world.py = > src/hello_world.py ( 100 % ) Now I am ready to merge my feat-hello-world branch into the dev branch of the upstream repository. Before that, I want to clean up my commits because commits 0ee86be and ef731ee are quite similar in what they do, both adding new features to the hello_world script and can be merged into one commit. We can do this by using git rebase -i to interactively rebase my commits. But since we want to rebase on top of the dev branch of the upstream repository, we need to first fetch the upstream repository (if you have not done so already). Subsequently, we can rebase our feat-hello-world branch onto the dev branch of the upstream repository as follows. rebase onto upstream/dev 1 2 ~/gaohn/git-sample-workflow $ git fetch upstream # this ensures that we are up to date with the upstream repository ~/gaohn/git-sample-workflow $ git rebase -i upstream/dev This will open up a text editor with the following content, shown in the gif below. Git Rebase Note that we replaced pick of ef731ee with fixup to squash the commit into the previous commit. We did not use squash because we do not want to keep the commit message of ef731ee . Therefore, fixup will keep the commit message of 0ee86be and discard the commit message of ef731ee while squashing the commit. Now we will see a message Successfully rebased and updated refs/heads/feat-hello-world. and we can force push our branch to the remote repository. force push to remote 1 ~/gaohn/git-sample-workflow $ git push -f origin feat-hello-world We are now ready to create a pull request to merge our feat-hello-world branch into the dev branch. Note We have successfully cleaned up our commits by using git rebase -i to squash two commits into one.","title":"Git Rebase"},{"location":"mlops_docs/git/collaborating/branching_and_merging/branching_and_merging/","text":"A branch could be interpreted as an individual timeline of our project commits. With Git, we can create many of these alternative environments (i.e. we can create different branches ) so other versions of our project code can exist and be tracked in parallel. That allows us to add new (experimental, unfinished, and potentially buggy) features in separate branches, without touching the ' official' stable version of our project code (which is usually kept on the master branch). When we initialize a repository and start making commits, they are saved to the master branch by default. Read More Visit Atlassian's git branch tutorial for more details.","title":"Overview"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/","text":"How Branch Works Quote A branch represents an independent line of development. Branches serve as an abstraction for the edit/stage/commit process. You can think of them as a way to request a brand new working directory, staging area, and project history. New commits are recorded in the history for the current branch, which results in a fork in the history of the project. The git branch command lets you create, list, rename, and delete branches. It doesn\u2019t let you switch between branches or put a forked history back together again. For this reason, git branch is tightly integrated with the git checkout and git merge commands. Creating New Branch You can create a new branch using the following command: git branch 1 ~/gaohn/git_tutorial $ git branch <branch-name> The new branch that gets created will be the reference to the current state of your repository. This does not switch you to the new branch. To switch to the new branch, use the git checkout or git switch command. For our purpose we create a dummy branch which we will delete later: git branch 1 ~/gaohn/git_tutorial $ git branch dummy Note It's a good idea to create a development branch where you can work on improving your code, adding new experimental features, and similar. After development and testing these new features to make sure they don't have any bugs and that they can be used, you can merge them to the master branch. Listing Branches To list all the branches in your repository, use the following command: List Git Branches Outputs git branch 1 ~/gaohn/git_tutorial $ git branch --list git branch 1 2 3 dummy * git-fetch master This will list all the branches in your repository. The branch that you are currently on will be marked with an asterisk. Delete Branches Delete Local Branch To delete a branch, you can run the following command: delete local branch 1 ~/gaohn/git_tutorial $ git branch -d <branch-name> and in our case our <branch-name> is dummy and running the above command will tell us Deleted branch dummy (was f3d4f25). You can replace -d with -D to force delete the branch if there are unmerged changes. Note that you must not be on the branch you want to delete. Delete Remote Branch To delete a remote branch, you can run the following command: delete remote branch 1 ~/gaohn/git_tutorial $ git push <remote-name> --delete <branch-name> Switching Branches To switch to a different branch, you use the git checkout command: git checkout 1 ~/gaohn/git_tutorial $ git checkout <branch-name> With that, you switch to a different isolated timeline of your project by changing branches. Note For example, you could be working on different features in your code and have a separate branch for each feature. When you switch to a branch, you can commit code changes which only affect that particular branch. Then, you can switch to another branch to work on a different feature, which won't be affected by the changes and commits made from the previous branch. To create a new branch and change to it at the same time, you can use the -b flag: git checkout -b 1 ~/gaohn/git_tutorial $ git checkout -b <branch-name> In our example, let us create a new branch called git-branch and switch to it: git checkout -b 1 ~/gaohn/git_tutorial $ git checkout -b git-branch and let's push this branch to remote: git push 1 ~/gaohn/git_tutorial $ git push -u origin git-branch","title":"git branch"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/#how-branch-works","text":"Quote A branch represents an independent line of development. Branches serve as an abstraction for the edit/stage/commit process. You can think of them as a way to request a brand new working directory, staging area, and project history. New commits are recorded in the history for the current branch, which results in a fork in the history of the project. The git branch command lets you create, list, rename, and delete branches. It doesn\u2019t let you switch between branches or put a forked history back together again. For this reason, git branch is tightly integrated with the git checkout and git merge commands.","title":"How Branch Works"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/#creating-new-branch","text":"You can create a new branch using the following command: git branch 1 ~/gaohn/git_tutorial $ git branch <branch-name> The new branch that gets created will be the reference to the current state of your repository. This does not switch you to the new branch. To switch to the new branch, use the git checkout or git switch command. For our purpose we create a dummy branch which we will delete later: git branch 1 ~/gaohn/git_tutorial $ git branch dummy Note It's a good idea to create a development branch where you can work on improving your code, adding new experimental features, and similar. After development and testing these new features to make sure they don't have any bugs and that they can be used, you can merge them to the master branch.","title":"Creating New Branch"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/#listing-branches","text":"To list all the branches in your repository, use the following command: List Git Branches Outputs git branch 1 ~/gaohn/git_tutorial $ git branch --list git branch 1 2 3 dummy * git-fetch master This will list all the branches in your repository. The branch that you are currently on will be marked with an asterisk.","title":"Listing Branches"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/#delete-branches","text":"","title":"Delete Branches"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/#delete-local-branch","text":"To delete a branch, you can run the following command: delete local branch 1 ~/gaohn/git_tutorial $ git branch -d <branch-name> and in our case our <branch-name> is dummy and running the above command will tell us Deleted branch dummy (was f3d4f25). You can replace -d with -D to force delete the branch if there are unmerged changes. Note that you must not be on the branch you want to delete.","title":"Delete Local Branch"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/#delete-remote-branch","text":"To delete a remote branch, you can run the following command: delete remote branch 1 ~/gaohn/git_tutorial $ git push <remote-name> --delete <branch-name>","title":"Delete Remote Branch"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_branch/#switching-branches","text":"To switch to a different branch, you use the git checkout command: git checkout 1 ~/gaohn/git_tutorial $ git checkout <branch-name> With that, you switch to a different isolated timeline of your project by changing branches. Note For example, you could be working on different features in your code and have a separate branch for each feature. When you switch to a branch, you can commit code changes which only affect that particular branch. Then, you can switch to another branch to work on a different feature, which won't be affected by the changes and commits made from the previous branch. To create a new branch and change to it at the same time, you can use the -b flag: git checkout -b 1 ~/gaohn/git_tutorial $ git checkout -b <branch-name> In our example, let us create a new branch called git-branch and switch to it: git checkout -b 1 ~/gaohn/git_tutorial $ git checkout -b git-branch and let's push this branch to remote: git push 1 ~/gaohn/git_tutorial $ git push -u origin git-branch","title":"Switching Branches"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_merge/","text":"Merging branches (NOT DONE YET!) You can merge branches in situations where you want to implement the code changes that you made in an individual branch to a different branch. For example, after you fully implemented and tested a new feature in your code, you would want to merge those changes to the stable branch of your project (which is usually the default master branch). To merge the changes from a different branch into your current branch, you can use this command: git merge <branch-name> You would replace <branch-name> with the branch that you want to integrate into your current branch.","title":"Git merge"},{"location":"mlops_docs/git/collaborating/branching_and_merging/git_merge/#merging-branches","text":"(NOT DONE YET!) You can merge branches in situations where you want to implement the code changes that you made in an individual branch to a different branch. For example, after you fully implemented and tested a new feature in your code, you would want to merge those changes to the stable branch of your project (which is usually the default master branch). To merge the changes from a different branch into your current branch, you can use this command: git merge <branch-name> You would replace <branch-name> with the branch that you want to integrate into your current branch.","title":"Merging branches"},{"location":"mlops_docs/git/collaborating/syncing/git_fetch/","text":"Atlassian Git Fetch The git fetch command downloads commits, files, and refs from a remote repository into your local repo. Fetching is what you do when you want to see what everybody else has been working on. It lets you see how the central history has progressed, but it doesn\u2019t force you to actually merge the changes into your repository. Git isolates fetched content from existing local content; it has absolutely no effect on your local development work. Fetched content has to be explicitly checked out using the git checkout command. This makes fetching a safe way to review commits before integrating them with your local repository. When downloading content from a remote repo, git pull and git fetch commands are available to accomplish the task. You can consider git fetch the 'safe' version of the two commands. It will download the remote content but not update your local repo's working state, leaving your current work intact. git pull is the more aggressive alternative; it will download the remote content for the active local branch and immediately execute git merge to create a merge commit for the new remote content. If you have pending changes in progress this will cause conflicts and kick-off the merge conflict resolution flow. Read More Visit Atlassian's git fetch tutorial for more details. We will add more content with examples later, for now we will see a simplified example, continuing from the previous section git push . Generic Git Fetch Workflow Let's say I have two local machines A and B. I develop on machine A at home and machine B at work. So far, commits 1-3 [27378ad, b9d3256, 13e956d] were developed on machine A. Subsequently, I went to work and will work on machine B. Let's say I created a new branch git-fetch on machine B at commit SHA 13e956d , meaning to say this branch has a snapshot of the remote repository at commit SHA 13e956d . Note that this branch is pushed to the remote repository from machine B . When I get back to machine A, I want to synchronize my local repository with the remote repository (i.e. I want the branch git-fetch created on machine B to be available on machine A). I can call the command git fetch origin : Git Fetch Git Fetch Outputs git fetch 1 ~/gaohn/git_tutorial $ git fetch origin git fetch output 1 2 From https://github.com/reigHns92/git-tutorial * [ new branch ] git-fetch -> origin/git-fetch This will \"download\" the remote repository's git-fetch branch to my local repository and I now have a local branch git-fetch that is tracking the remote branch git-fetch . To further understand this, let's say I made some changes to the README.md on branch git-fetch at commit hash 13e956d and pushed it to the remote repository from machine B at commit f3d4f25 . Consequently, my local repo at machine A will be one commit behind the remote repository at branch git-fetch . Git Fetch Git Fetch Outputs git fetch 1 ~/gaohn/git_tutorial $ git fetch origin git fetch output 1 2 3 4 5 6 7 8 remote: Enumerating objects: 5 , done . remote: Counting objects: 100 % ( 5 /5 ) , done . remote: Compressing objects: 100 % ( 3 /3 ) , done . remote: Total 3 ( delta 1 ) , reused 0 ( delta 0 ) , pack-reused 0 Unpacking objects: 100 % ( 3 /3 ) , 742 bytes | 123 .00 KiB/s, done . From https://github.com/reigHns92/git-tutorial * branch git-fetch -> FETCH_HEAD 13e956d..f3d4f25 git-fetch -> origin/git-fetch Running git status will also show that the local repository at machine A is one commit behind the remote repository at branch git-fetch . To see what commits have been added to the remote origin branch git-fetch , you can run a git log using origin/git-fetch as a filter: git log 1 ~/gaohn/git_tutorial $ git log --oneline master..origin/git-fetch which will a message f3d4f25 (origin/git-fetch) git: update README . Finally, you can merge the changes made on git-fetch to your local repository at machine A: Git Merge Git Merge Outputs git merge 1 ~/gaohn/git_tutorial $ git merge origin/git-fetch git merge output 1 2 3 4 Updating 13e956d..f3d4f25 Fast-forward README.md | 1 + 1 file changed, 1 insertion ( + ) Now, you can see that the local repository at machine A is up-to-date with the remote repository at branch git-fetch .","title":"git fetch"},{"location":"mlops_docs/git/collaborating/syncing/git_fetch/#generic-git-fetch-workflow","text":"Let's say I have two local machines A and B. I develop on machine A at home and machine B at work. So far, commits 1-3 [27378ad, b9d3256, 13e956d] were developed on machine A. Subsequently, I went to work and will work on machine B. Let's say I created a new branch git-fetch on machine B at commit SHA 13e956d , meaning to say this branch has a snapshot of the remote repository at commit SHA 13e956d . Note that this branch is pushed to the remote repository from machine B . When I get back to machine A, I want to synchronize my local repository with the remote repository (i.e. I want the branch git-fetch created on machine B to be available on machine A). I can call the command git fetch origin : Git Fetch Git Fetch Outputs git fetch 1 ~/gaohn/git_tutorial $ git fetch origin git fetch output 1 2 From https://github.com/reigHns92/git-tutorial * [ new branch ] git-fetch -> origin/git-fetch This will \"download\" the remote repository's git-fetch branch to my local repository and I now have a local branch git-fetch that is tracking the remote branch git-fetch . To further understand this, let's say I made some changes to the README.md on branch git-fetch at commit hash 13e956d and pushed it to the remote repository from machine B at commit f3d4f25 . Consequently, my local repo at machine A will be one commit behind the remote repository at branch git-fetch . Git Fetch Git Fetch Outputs git fetch 1 ~/gaohn/git_tutorial $ git fetch origin git fetch output 1 2 3 4 5 6 7 8 remote: Enumerating objects: 5 , done . remote: Counting objects: 100 % ( 5 /5 ) , done . remote: Compressing objects: 100 % ( 3 /3 ) , done . remote: Total 3 ( delta 1 ) , reused 0 ( delta 0 ) , pack-reused 0 Unpacking objects: 100 % ( 3 /3 ) , 742 bytes | 123 .00 KiB/s, done . From https://github.com/reigHns92/git-tutorial * branch git-fetch -> FETCH_HEAD 13e956d..f3d4f25 git-fetch -> origin/git-fetch Running git status will also show that the local repository at machine A is one commit behind the remote repository at branch git-fetch . To see what commits have been added to the remote origin branch git-fetch , you can run a git log using origin/git-fetch as a filter: git log 1 ~/gaohn/git_tutorial $ git log --oneline master..origin/git-fetch which will a message f3d4f25 (origin/git-fetch) git: update README . Finally, you can merge the changes made on git-fetch to your local repository at machine A: Git Merge Git Merge Outputs git merge 1 ~/gaohn/git_tutorial $ git merge origin/git-fetch git merge output 1 2 3 4 Updating 13e956d..f3d4f25 Fast-forward README.md | 1 + 1 file changed, 1 insertion ( + ) Now, you can see that the local repository at machine A is up-to-date with the remote repository at branch git-fetch .","title":"Generic Git Fetch Workflow"},{"location":"mlops_docs/git/collaborating/syncing/git_pull/","text":"Atlassian Git Pull The git pull command is a combination of git fetch and git merge . It downloads the remote content and merges it with your local content. Read More Visit Atlassian's git pull tutorial for more details.","title":"git pull"},{"location":"mlops_docs/git/collaborating/syncing/git_push/","text":"Atlassian Git Push The git push command is used to upload local repository content to a remote repository. Pushing is how you transfer commits from your local repository to a remote repo. It's the counterpart to git fetch, but whereas fetching imports commits to local branches, pushing exports commits to remote branches. Remote branches are configured using the git remote command. Pushing has the potential to overwrite changes, caution should be taken when pushing. These issues are discussed below. Read More Visit Atlassian's git push tutorial for more details. Generic Git Push Workflow Continuing from the previous section on git remote , we will now push our local repository to the remote repository. git push 1 ~/gaohn/git_tutorial $ git push <remote name> <branch name> where <remote name> is the name of the remote repository and <branch name> is the name of the branch. In our example, we will be pushing to the origin remote repository and the master / main branch. Git Push Git Push Outputs git push 1 ~/gaohn/git_tutorial $ git push origin master git push output 1 2 3 4 5 6 7 8 9 10 Enumerating objects: 8 , done . Counting objects: 100 % ( 8 /8 ) , done . Delta compression using up to 12 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 8 /8 ) , 614 bytes | 614 .00 KiB/s, done . Total 8 ( delta 1 ) , reused 0 ( delta 0 ) , pack-reused 0 remote: Resolving deltas: 100 % ( 1 /1 ) , done . To https://github.com/reigHns92/git-tutorial.git * [ new branch ] master -> master Branch 'master' set up to track remote branch 'master' from 'origin' . This will now push our local repository to the remote repository and can be further confirmed by git remote -v . Non-Fast-Forward Push This terminology was mentioned a few times in Atlassian's Git tutorial. Basically, if Git detects that the remote repository is ahead of the local repository, it will refuse to push. This is common, let's see the example: Prior to this section, we pushed our local repository to the remote repository master with commit SHA b9d3256 . Shortly after, my friend Joe synced up with the remote repository at commit SHA b9d3256 and he updated README.md and pushed with commit SHA 13e956d to master . Now if I run git push -u origin master again, Git will detect that the remote repository is ahead of the local repository and refuse to push, giving me the following errors. git push 1 2 3 4 5 6 7 8 To https://github.com/reigHns92/git-tutorial.git ! [ rejected ] master -> master ( fetch first ) error: failed to push some refs to 'https://github.com/reigHns92/git-tutorial.git' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: ( e.g., 'git pull ...' ) before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details 4. To resolve this, I need to git pull origin first and then run git push -u origin master . 5. However, this is an over-simplified example because I made an assumption that I did not have any local changes/developments on at the commit b9d3256 . If I did, then I may have to resolve merge conflicts, which we will see in future tutorials.","title":"git push"},{"location":"mlops_docs/git/collaborating/syncing/git_push/#generic-git-push-workflow","text":"Continuing from the previous section on git remote , we will now push our local repository to the remote repository. git push 1 ~/gaohn/git_tutorial $ git push <remote name> <branch name> where <remote name> is the name of the remote repository and <branch name> is the name of the branch. In our example, we will be pushing to the origin remote repository and the master / main branch. Git Push Git Push Outputs git push 1 ~/gaohn/git_tutorial $ git push origin master git push output 1 2 3 4 5 6 7 8 9 10 Enumerating objects: 8 , done . Counting objects: 100 % ( 8 /8 ) , done . Delta compression using up to 12 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 8 /8 ) , 614 bytes | 614 .00 KiB/s, done . Total 8 ( delta 1 ) , reused 0 ( delta 0 ) , pack-reused 0 remote: Resolving deltas: 100 % ( 1 /1 ) , done . To https://github.com/reigHns92/git-tutorial.git * [ new branch ] master -> master Branch 'master' set up to track remote branch 'master' from 'origin' . This will now push our local repository to the remote repository and can be further confirmed by git remote -v .","title":"Generic Git Push Workflow"},{"location":"mlops_docs/git/collaborating/syncing/git_push/#non-fast-forward-push","text":"This terminology was mentioned a few times in Atlassian's Git tutorial. Basically, if Git detects that the remote repository is ahead of the local repository, it will refuse to push. This is common, let's see the example: Prior to this section, we pushed our local repository to the remote repository master with commit SHA b9d3256 . Shortly after, my friend Joe synced up with the remote repository at commit SHA b9d3256 and he updated README.md and pushed with commit SHA 13e956d to master . Now if I run git push -u origin master again, Git will detect that the remote repository is ahead of the local repository and refuse to push, giving me the following errors. git push 1 2 3 4 5 6 7 8 To https://github.com/reigHns92/git-tutorial.git ! [ rejected ] master -> master ( fetch first ) error: failed to push some refs to 'https://github.com/reigHns92/git-tutorial.git' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: ( e.g., 'git pull ...' ) before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details 4. To resolve this, I need to git pull origin first and then run git push -u origin master . 5. However, this is an over-simplified example because I made an assumption that I did not have any local changes/developments on at the commit b9d3256 . If I did, then I may have to resolve merge conflicts, which we will see in future tutorials.","title":"Non-Fast-Forward Push"},{"location":"mlops_docs/git/collaborating/syncing/git_remote/","text":"So far, everything done above is still in local repository, if we want to push our content to a remote repository for collaboration, we will need to harness the power of git remote . Read More Visit Atlassian's git remote tutorial for more details. Generic Git Remote Workflow To add a remote to our local repository, we first need to create a remote repository. For our purpose we will be using GitHub. Follow the steps here to create a new repository on GitHub. Once the remote repository is created, we will copy the URL for the remote repository. In this tutorial, we will only focus on the HTTPS URL. git remote add 1 ~/gaohn/git_tutorial $ git remote add <remote name> <remote url> where <remote name> is the name of the remote repository and <remote url> is the URL of the remote repository. After creating a remote repository on GitHub, we call that remote repository origin . Therefore, our <remote name> is origin and the <remote url> is the HTTPS URL of the remote repository. git remote add 1 ~/gaohn/git_tutorial $ git remote add origin https://github.com/reigHns92/git-tutorial.git Tip I often encounter permission denied error when pushing to remote when I have two github accounts on the same local machine, one solution is to use SSH, the other is the one listed below: Create a personal access token for the second account and tick all access . Add the remote origin for the second account. git remote add <remote name> <remote url> git remote set-url <remote name> <https:// [ token ] @github.com/ [ username ] / [ repository_name ] > Push the changes to the second account and a prompt opens for you to key in token access. Useful Git Remote Commands git remote commands 1 2 3 ~/gaohn/git_tutorial $ git remote -v # (1) ~/gaohn/git_tutorial $ git remote rm <remote name> # (2) ~/gaohn/git_tutorial $ git remote rename <old name> <new name> # (3) List all the remote repositories that are added to our local repository. Remove a remote repository from our local repository. Rename a remote repository.","title":"git remote"},{"location":"mlops_docs/git/collaborating/syncing/git_remote/#generic-git-remote-workflow","text":"To add a remote to our local repository, we first need to create a remote repository. For our purpose we will be using GitHub. Follow the steps here to create a new repository on GitHub. Once the remote repository is created, we will copy the URL for the remote repository. In this tutorial, we will only focus on the HTTPS URL. git remote add 1 ~/gaohn/git_tutorial $ git remote add <remote name> <remote url> where <remote name> is the name of the remote repository and <remote url> is the URL of the remote repository. After creating a remote repository on GitHub, we call that remote repository origin . Therefore, our <remote name> is origin and the <remote url> is the HTTPS URL of the remote repository. git remote add 1 ~/gaohn/git_tutorial $ git remote add origin https://github.com/reigHns92/git-tutorial.git Tip I often encounter permission denied error when pushing to remote when I have two github accounts on the same local machine, one solution is to use SSH, the other is the one listed below: Create a personal access token for the second account and tick all access . Add the remote origin for the second account. git remote add <remote name> <remote url> git remote set-url <remote name> <https:// [ token ] @github.com/ [ username ] / [ repository_name ] > Push the changes to the second account and a prompt opens for you to key in token access.","title":"Generic Git Remote Workflow"},{"location":"mlops_docs/git/collaborating/syncing/git_remote/#useful-git-remote-commands","text":"git remote commands 1 2 3 ~/gaohn/git_tutorial $ git remote -v # (1) ~/gaohn/git_tutorial $ git remote rm <remote name> # (2) ~/gaohn/git_tutorial $ git remote rename <old name> <new name> # (3) List all the remote repositories that are added to our local repository. Remove a remote repository from our local repository. Rename a remote repository.","title":"Useful Git Remote Commands"},{"location":"mlops_docs/git/collaborating/syncing/syncing/","text":"So far the tutorial has covered the basics of Git, and is limited to your local machine. In order to collaborate with others, you will need to push your changes to a remote repository. The following sections will cover how to do that.","title":"Overview"},{"location":"mlops_docs/git/getting_started/inspecting_repository/git_status/","text":"","title":"git status"},{"location":"mlops_docs/git/getting_started/inspecting_repository/inspecting_repository/","text":"Visit Atlassian's inspecting a repository tutorial for more details.","title":"Overview"},{"location":"mlops_docs/git/getting_started/rewriting_history/git_rebase/","text":"See https://christoph-rumpel.com/2015/05/clean-up-your-commits-for-a-pull-request/ for documentation.","title":"Git rebase"},{"location":"mlops_docs/git/getting_started/rewriting_history/rewriting_history/","text":"Visit Atlassian's rewriting history tutorial for more details.","title":"Overview"},{"location":"mlops_docs/git/getting_started/saving_changes/git_add/","text":"","title":"git add"},{"location":"mlops_docs/git/getting_started/saving_changes/git_commit/","text":"","title":"git commit"},{"location":"mlops_docs/git/getting_started/saving_changes/git_diff/","text":"Comparing Changes with git diff Visit Atlassian's git diff tutorial for more details.","title":"git diff"},{"location":"mlops_docs/git/getting_started/saving_changes/git_diff/#comparing-changes-with-git-diff","text":"Visit Atlassian's git diff tutorial for more details.","title":"Comparing Changes with git diff"},{"location":"mlops_docs/git/getting_started/saving_changes/git_stash/","text":"Stashing Changes with git stash Visit Atlassian's git stash tutorial for more details.","title":"git stash"},{"location":"mlops_docs/git/getting_started/saving_changes/git_stash/#stashing-changes-with-git-stash","text":"Visit Atlassian's git stash tutorial for more details.","title":"Stashing Changes with git stash"},{"location":"mlops_docs/git/getting_started/saving_changes/saving_changes/","text":"Committing is the process in which the changes are officially added to the Git repository. In Git, we can consider commits to be checkpoints, or snapshots of your project at its current state. In other words, we basically save the current version of our code in a commit. We can create as many commits as we need in the commit history, and we can go back and forth between commits to see the different revisions of our project code. That allows us to efficiently manage our progress and track the project as it gets developed. Commits are usually created at logical points as we develop our project, usually after adding in specific contents, features or modifications (like new functionalities or bug fixes, for example). Creating gitignore Usually, I recommend creating .gitignore at this step, because you want to list down files that you do not wish git to track. This is important because some secret keys should never be pushed to the remote server for people to see. Furthermore, some large files should be kept in a storage as git cannot store too large files. To ignore files that you don't want to be tracked or added to the staging area, you can create a file called .gitignore in your main project folder. Inside of that file, you can list all the file and folder names that you definitely do not want to track (each ignored file and folder should go to a new line inside the .gitignore file). create .gitignore 1 ~/gaohn/git_tutorial $ touch .gitignore For example, if I have a file called secrets.py that contains my secret key, I can add it to the .gitignore file. create .gitignore 1 2 ~/gaohn/git_tutorial $ touch secrets.py ~/gaohn/git_tutorial $ echo \"secrets.py\" >> .gitignore Read More Visit the Atlassian's gitignore tutorial for more details. Checking Status: git status While located inside the project folder in our terminal, we can type the following command to check the status of our repository: git status 1 ~/gaohn/git_tutorial $ git status which will return something like this: git status output 1 2 3 4 5 6 7 8 9 10 11 On branch master No commits yet Untracked files: ( use \"git add <file>...\" to include in what will be committed ) .gitignore README.md secrets.py nothing added to commit but untracked files present ( use \"git add\" to track ) This is a command that is very often used when working with Git. It shows us which files have been changed, which files are tracked, etc. We can add the untracked project files using git add to the staging area based on the information from the git status command. At a later point, git status will report any modifications that we made to our tracked files before we decide to add them to the staging area again. Adding Files to the Staging Area: git add The git add command adds a change in the working directory to the staging area. It tells Git that you want to include updates to a particular file in the next commit. However, git add doesn't really affect the repository in any significant way\u2014changes are not actually recorded until you run git commit. In conjunction with these commands, you'll also need git status to view the state of the working directory and the staging area. Next we will add all our current changes to the staging area: git add all files in cwd 1 ~/gaohn/git_tutorial $ git add . # (1) add all files in the current directory You can also choose to add specific files to the staging area. git add specific files 1 ~/gaohn/git_tutorial $ git add <file> Read More Visit the Atlassian's git add tutorial for more details. Making commits: git commit A commit is a snapshot of our code at a particular time, which we are saving to the commit history of our repository. After adding all the files that we want to track to the staging area with the git add command, we are ready to make a commit. To commit the files from the staging area, we use the following command: git commit 1 2 ~/gaohn/git_tutorial $ git config --global core.editor \"code --wait\" # (1) ~/gaohn/git_tutorial $ git commit -a # (2) use this code if popup editor does not appear. a pop up editor should appear for you to type the message. The commit message should be a descriptive summary of the changes that you are committing to the repository. For the first commit, I usually type \"Initial Commit\" as the commit message. After making the commit, you will see messages like the following: git commit output 1 2 3 4 5 [ master ( root-commit ) 3dccc05 ] Initial Commit 3 files changed, 3 insertions ( + ) create mode 100644 .gitignore create mode 100644 README.md create mode 100644 secrets.py Read More Visit the Atlassian's git commit tutorial for more details. Commit History: git log To see all the commits that were made for our project, you can use the following command: git log 1 ~/gaohn/git_tutorial $ git log The logs will show details for each commit, like the author name, the generated hash for the commit, date and time of the commit, and the commit message that we provided. To go back to a previous state of your project code that you committed, you can use the following command: git checkout specific commit 1 ~/gaohn/git_tutorial $ git checkout <commit_hash> Replace <commit-hash> with the actual hash for the specific commit that you want to visit, which is listed with the git log command. To go back to the latest commit (the newest version of our project code), you can type this command: ~/gaohn/git_tutorial $ git checkout master Update/Amend Commit To update the commit message for the latest commit, you can use the following command: git commit --amend 1 ~/gaohn/git_tutorial $ git commit --amend This is useful if you forgot to add something to the commit message. Read More There are much more nuance to this and visit the Atlassian's rewriting history tutorial for more details on how to change committed files.","title":"Overview"},{"location":"mlops_docs/git/getting_started/saving_changes/saving_changes/#creating-gitignore","text":"Usually, I recommend creating .gitignore at this step, because you want to list down files that you do not wish git to track. This is important because some secret keys should never be pushed to the remote server for people to see. Furthermore, some large files should be kept in a storage as git cannot store too large files. To ignore files that you don't want to be tracked or added to the staging area, you can create a file called .gitignore in your main project folder. Inside of that file, you can list all the file and folder names that you definitely do not want to track (each ignored file and folder should go to a new line inside the .gitignore file). create .gitignore 1 ~/gaohn/git_tutorial $ touch .gitignore For example, if I have a file called secrets.py that contains my secret key, I can add it to the .gitignore file. create .gitignore 1 2 ~/gaohn/git_tutorial $ touch secrets.py ~/gaohn/git_tutorial $ echo \"secrets.py\" >> .gitignore Read More Visit the Atlassian's gitignore tutorial for more details.","title":"Creating gitignore"},{"location":"mlops_docs/git/getting_started/saving_changes/saving_changes/#checking-status-git-status","text":"While located inside the project folder in our terminal, we can type the following command to check the status of our repository: git status 1 ~/gaohn/git_tutorial $ git status which will return something like this: git status output 1 2 3 4 5 6 7 8 9 10 11 On branch master No commits yet Untracked files: ( use \"git add <file>...\" to include in what will be committed ) .gitignore README.md secrets.py nothing added to commit but untracked files present ( use \"git add\" to track ) This is a command that is very often used when working with Git. It shows us which files have been changed, which files are tracked, etc. We can add the untracked project files using git add to the staging area based on the information from the git status command. At a later point, git status will report any modifications that we made to our tracked files before we decide to add them to the staging area again.","title":"Checking Status: git status"},{"location":"mlops_docs/git/getting_started/saving_changes/saving_changes/#adding-files-to-the-staging-area-git-add","text":"The git add command adds a change in the working directory to the staging area. It tells Git that you want to include updates to a particular file in the next commit. However, git add doesn't really affect the repository in any significant way\u2014changes are not actually recorded until you run git commit. In conjunction with these commands, you'll also need git status to view the state of the working directory and the staging area. Next we will add all our current changes to the staging area: git add all files in cwd 1 ~/gaohn/git_tutorial $ git add . # (1) add all files in the current directory You can also choose to add specific files to the staging area. git add specific files 1 ~/gaohn/git_tutorial $ git add <file> Read More Visit the Atlassian's git add tutorial for more details.","title":"Adding Files to the Staging Area: git add"},{"location":"mlops_docs/git/getting_started/saving_changes/saving_changes/#making-commits-git-commit","text":"A commit is a snapshot of our code at a particular time, which we are saving to the commit history of our repository. After adding all the files that we want to track to the staging area with the git add command, we are ready to make a commit. To commit the files from the staging area, we use the following command: git commit 1 2 ~/gaohn/git_tutorial $ git config --global core.editor \"code --wait\" # (1) ~/gaohn/git_tutorial $ git commit -a # (2) use this code if popup editor does not appear. a pop up editor should appear for you to type the message. The commit message should be a descriptive summary of the changes that you are committing to the repository. For the first commit, I usually type \"Initial Commit\" as the commit message. After making the commit, you will see messages like the following: git commit output 1 2 3 4 5 [ master ( root-commit ) 3dccc05 ] Initial Commit 3 files changed, 3 insertions ( + ) create mode 100644 .gitignore create mode 100644 README.md create mode 100644 secrets.py Read More Visit the Atlassian's git commit tutorial for more details.","title":"Making commits: git commit"},{"location":"mlops_docs/git/getting_started/saving_changes/saving_changes/#commit-history-git-log","text":"To see all the commits that were made for our project, you can use the following command: git log 1 ~/gaohn/git_tutorial $ git log The logs will show details for each commit, like the author name, the generated hash for the commit, date and time of the commit, and the commit message that we provided. To go back to a previous state of your project code that you committed, you can use the following command: git checkout specific commit 1 ~/gaohn/git_tutorial $ git checkout <commit_hash> Replace <commit-hash> with the actual hash for the specific commit that you want to visit, which is listed with the git log command. To go back to the latest commit (the newest version of our project code), you can type this command: ~/gaohn/git_tutorial $ git checkout master","title":"Commit History: git log"},{"location":"mlops_docs/git/getting_started/saving_changes/saving_changes/#updateamend-commit","text":"To update the commit message for the latest commit, you can use the following command: git commit --amend 1 ~/gaohn/git_tutorial $ git commit --amend This is useful if you forgot to add something to the commit message. Read More There are much more nuance to this and visit the Atlassian's rewriting history tutorial for more details on how to change committed files.","title":"Update/Amend Commit"},{"location":"mlops_docs/git/getting_started/setting_up_repository/git_clone/","text":"","title":"git clone"},{"location":"mlops_docs/git/getting_started/setting_up_repository/git_config/","text":"","title":"git config"},{"location":"mlops_docs/git/getting_started/setting_up_repository/git_init/","text":"","title":"git init"},{"location":"mlops_docs/git/getting_started/setting_up_repository/setting_up_repository/","text":"We will closely follow the Atlassian's Getting Started Tutorial . As stated in their tutorial, this section's high level overview: Initializing a new Git repo Cloning an existing Git repo Committing a modified version of a file to the repo Configuring a Git repo for remote collaboration Common Git version control commands Initialize an Entirely New Repository: git init To create a new repo, you'll use the git init command; this a one-time command you use during the initial setup of a new repo. Executing this command will create a new .git subdirectory in your current working directory. This will also create a new main/master branch. initialize a new repo 1 2 3 4 5 ~/gaohn $ mkdir { git_tutorial } ~/gaohn $ cd { git_tutorial } ~/gaohn/git_tutorial $ git init > Initialized empty Git repository in /Users/gaohn/git_tutorial/.git/ This command will generate a hidden .git directory for your project, where Git stores all internal tracking data for the current repository. ~/gaohn/git_tutorial $ ls -a Read More Visit the Atlassian's git init tutorial for more details. Initialize a Repository from an Existing Project: git clone If a project has already been set up in a remote repository , the clone command is the most common way for users to obtain a local development clone. Like git init , cloning is generally a one-time operation. Once a developer has obtained a working copy, all version control operations are managed through their local repository. clone a repo 1 ~/gaohn $ git clone <repo url> For example, I have a repository stored on GitHub called https://github.com/ghnreigns/ github-test.git and I can clone it to my local machine by filling <repo url> with the url of the repository. clone a repo 1 2 3 4 5 6 7 8 ~/gaohn $ git clone https://github.com/ghnreigns/github-test.git > Cloning into 'github-test' ... > remote: Enumerating objects: 6 , done . > remote: Counting objects: 100 % ( 6 /6 ) , done . > remote: Compressing objects: 100 % ( 2 /2 ) , done . > remote: Total 6 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 > Receiving objects: 100 % ( 6 /6 ) , done . Then you will see a new directory called github-test in your current working directory and you do not need to run git init on this directory. These two commands cover the ground of initializing a repository locally. Read More Visit the Atlassian's git clone tutorial for more details. Initialize Remote Repository Generally stored outside of your isolated local system, usually on a remote server. It's especially useful when working in teams - this is the place where you can share your project code, see other people's code and integrate it into your local version of the project, and also push your changes to the remote repository. For our purpose, we will be using GitHub as our remote repository, you can initialize one following the GitHub's tutorial . Tip Many people suggest one to first create a remote repository and then clone it to their local machine for development, where you need not run git init on the local directory.","title":"Overview"},{"location":"mlops_docs/git/getting_started/setting_up_repository/setting_up_repository/#initialize-an-entirely-new-repository-git-init","text":"To create a new repo, you'll use the git init command; this a one-time command you use during the initial setup of a new repo. Executing this command will create a new .git subdirectory in your current working directory. This will also create a new main/master branch. initialize a new repo 1 2 3 4 5 ~/gaohn $ mkdir { git_tutorial } ~/gaohn $ cd { git_tutorial } ~/gaohn/git_tutorial $ git init > Initialized empty Git repository in /Users/gaohn/git_tutorial/.git/ This command will generate a hidden .git directory for your project, where Git stores all internal tracking data for the current repository. ~/gaohn/git_tutorial $ ls -a Read More Visit the Atlassian's git init tutorial for more details.","title":"Initialize an Entirely New Repository: git init"},{"location":"mlops_docs/git/getting_started/setting_up_repository/setting_up_repository/#initialize-a-repository-from-an-existing-project-git-clone","text":"If a project has already been set up in a remote repository , the clone command is the most common way for users to obtain a local development clone. Like git init , cloning is generally a one-time operation. Once a developer has obtained a working copy, all version control operations are managed through their local repository. clone a repo 1 ~/gaohn $ git clone <repo url> For example, I have a repository stored on GitHub called https://github.com/ghnreigns/ github-test.git and I can clone it to my local machine by filling <repo url> with the url of the repository. clone a repo 1 2 3 4 5 6 7 8 ~/gaohn $ git clone https://github.com/ghnreigns/github-test.git > Cloning into 'github-test' ... > remote: Enumerating objects: 6 , done . > remote: Counting objects: 100 % ( 6 /6 ) , done . > remote: Compressing objects: 100 % ( 2 /2 ) , done . > remote: Total 6 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 > Receiving objects: 100 % ( 6 /6 ) , done . Then you will see a new directory called github-test in your current working directory and you do not need to run git init on this directory. These two commands cover the ground of initializing a repository locally. Read More Visit the Atlassian's git clone tutorial for more details.","title":"Initialize a Repository from an Existing Project: git clone"},{"location":"mlops_docs/git/getting_started/setting_up_repository/setting_up_repository/#initialize-remote-repository","text":"Generally stored outside of your isolated local system, usually on a remote server. It's especially useful when working in teams - this is the place where you can share your project code, see other people's code and integrate it into your local version of the project, and also push your changes to the remote repository. For our purpose, we will be using GitHub as our remote repository, you can initialize one following the GitHub's tutorial . Tip Many people suggest one to first create a remote repository and then clone it to their local machine for development, where you need not run git init on the local directory.","title":"Initialize Remote Repository"},{"location":"mlops_docs/git/getting_started/undoing_changes/undoing_changes/","text":"This section is important as it teaches you how to \"undo\" changes that you made to your repository. Atlassian Undoing Changes In this section, we will discuss the available 'undo' Git strategies and commands. It is first important to note that Git does not have a traditional 'undo' system like those found in a word processing application. It will be beneficial to refrain from mapping Git operations to any traditional 'undo' mental model. Additionally, Git has its own nomenclature for 'undo' operations that it is best to leverage in a discussion. This nomenclature includes terms like reset, revert, checkout, clean, and more. A fun metaphor is to think of Git as a timeline management utility. Commits are snapshots of a point in time or points of interest along the timeline of a project's history. Additionally, multiple timelines can be managed through the use of branches. When 'undoing' in Git, you are usually moving back in time, or to another timeline where mistakes didn't happen. This tutorial provides all of the necessary skills to work with previous revisions of a software project. First, it shows you how to explore old commits, then it explains the difference between reverting public commits in the project history vs. resetting unpublished changes on your local machine. Read More Visit Atlassian's undoing changes tutorial for more details.","title":"Overview"},{"location":"mlops_docs/testing/testing/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import numpy as np import pytest import torch Intuition Tests are a way for us to ensure that something works as intended. We're incentivized to implement tests and discover sources of error as early in the development cycle as possible so that we can reduce increasing downstream costs and wasted time. Once we've designed our tests, we can automatically execute them every time we implement a change to our system and continue to build on them over time. In this lesson, we'll learn how to test machine learning code, data and models to construct a system that we can reliably iterate on. Types of tests There are many four majors types of tests which are utilized at different points in the development cycle: Unit tests: tests on individual components that each have a single responsibility (ex. function that filters a list). Integration tests: tests on the combined functionality of individual components (ex. data processing). System tests: tests on the design of a system for expected outputs given inputs (ex. training, inference, etc.). Acceptance tests: tests to verify that requirements have been met, usually referred to as User Acceptance Testing (UAT). Regression tests: testing errors we've seen before to ensure new changes don't reintroduce them. How should we test? The framework to use when composing tests is the Arrange Act Assert methodology . Arrange: set up the different inputs to test on. Act: apply the inputs on the component we want to test. Assert: confirm that we received the expected output. What should we be testing for? An example: When arranging our inputs and asserting our expected outputs, what are some aspects of our inputs and outputs that we should be testing for? inputs: data types, format, length, edge cases (min/max, small/large, etc.) outputs: data types, formats, exceptions, intermediary and final outputs Best practices Regardless of the framework we use, it's important to strongly tie testing into the development process. atomic: when creating unit components, we need to ensure that they have a single responsibility so that we can easily test them. If not, we'll need to split them into more granular units. compose: when we create new components, we want to compose tests to validate their functionality. It's a great way to ensure reliability and catch errors early on. regression: we want to account for new errors we come across with a regression test so we can ensure we don't reintroduce the same errors in the future. coverage: we want to ensure that 100% of our codebase has been accounter for. This doesn't mean writing a test for every single line of code but rather accounting for every single line (more on this in the coverage section below). automate: in the event we forget to run our tests before committing to a repository, we want to auto run tests for every commit. We'll learn how to do this locally using pre-commit hooks and remotely (ie. main branch) via GitHub actions in subsequent lessons. Test-driven development or Otherwise? Test-driven development (TDD) is the process where you write a test before completely writing the functionality to ensure that tests are always written. This is in contrast to writing functionality first and then composing tests afterwards. Here are my thoughts on this: good to write tests as we progress, but it's not the representation of correctness. initial time should be spent on design before ever getting into the code or tests. using a test as guide doesn't mean that our functionality is error free. Perfect coverage doesn't mean that our application is error free if those tests aren't meaningful and don't encompass the field of possible inputs, intermediates and outputs. Therefore, we should work towards better design and agility when facing errors, quickly resolving them and writing test cases around them to avoid them next time. Pytest We're going to be using pytest as our testing framework for it's powerful builtin features such as parametrization, fixtures, markers, etc. Configuration Pytest expects tests to be organized under a tests directory by default. However, we can also use our pyproject.toml file to configure any other test path directories as well. Once in the directory, pytest looks for python scripts starting with tests_*.py but we can configure it to read any other file patterns as well. # Pytest [tool.pytest.ini_options] testpaths = [ \"tests\" ] python_files = \"test_*.py\" Assertions Simple assertion testing example. from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () SRC_DIR = Path . joinpath ( BASE_DIR , \"src\" ) TEST_DIR = Path . joinpath ( BASE_DIR , \"tests\" ) SRC_DIR . mkdir ( parents = True , exist_ok = True ) TEST_DIR . mkdir ( parents = True , exist_ok = True ) %% writefile { BASE_DIR } / pyproject . toml # Pytest [ tool . pytest . ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\" Writing /content/pyproject.toml %% writefile { SRC_DIR } / __init__ . py \"init file\" Writing /content/src/__init__.py %% writefile { SRC_DIR } / fruits . py def is_crisp ( fruit ): if fruit : fruit = fruit . lower () if fruit in [ \"apple\" , \"watermelon\" , \"cherries\" ]: return True elif fruit in [ \"orange\" , \"mango\" , \"strawberry\" ]: return False else : raise ValueError ( f \" { fruit } not in known list of fruits.\" ) return False Writing /content/src/fruits.py %% writefile { TEST_DIR } / test_fruits . py import pytest import sys sys . path . append ( \"/content\" ) # append to import properly. from src.fruits import is_crisp def test_is_crisp (): assert is_crisp ( fruit = \"apple\" ) # or == True assert is_crisp ( fruit = \"Apple\" ) assert not is_crisp ( fruit = \"orange\" ) with pytest . raises ( ValueError ): is_crisp ( fruit = None ) is_crisp ( fruit = \"pear\" ) Writing /content/tests/test_fruits.py ! pytest # all tests ! pytest tests / # tests under a directory ! pytest tests / test_fruits . py # tests for a single file ! pytest tests / test_fruits . py :: test_is_crisp # tests for a single function ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.02 seconds =========================== ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.01 seconds =========================== ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.01 seconds =========================== ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.01 seconds =========================== Classes See examples from madewithml repo to understand better. Interfaces See madewithml interface section . Parametrize So far, in our tests, we've had to create individual assert statements to validate different combinations of inputs and expected outputs. However, there's a bit of redundancy here because the inputs always feed into our functions as arguments and the outputs are compared with our expected outputs. To remove this redundancy, pytest has the @pytest.mark.parametrize decorator which allows us to represent our inputs and outputs as parameters. Let us create a new python file test_fruits_parametrize.py to test it out. %% writefile { TEST_DIR } / test_fruits_parametrize . py import pytest import sys sys . path . append ( \"/content\" ) # append to import properly. from src.fruits import is_crisp @pytest . mark . parametrize ( \"fruit, crisp\" , [ ( \"apple\" , True ), ( \"Apple\" , True ), ( \"orange\" , False ), ], ) def test_is_crisp_parametrize ( fruit , crisp ): assert is_crisp ( fruit = fruit ) == crisp @pytest . mark . parametrize ( \"fruit, exception\" , [ ( \"pear\" , ValueError ), ], ) def test_is_crisp_exceptions ( fruit , exception ): with pytest . raises ( exception ): is_crisp ( fruit = fruit ) Overwriting /content/tests/test_fruits_parametrize.py To fix line number, but for now the line number starts from the decorator @pytest.mark.parametrize . [Line 2]: define the names of the parameters under the decorator, ex. \"fruit, crisp\" (note that this is one string). Note that this string names should correspond to the function defined under the decorator. [Lines 3-7]: provide a list of combinations of values for the parameters from Step 1. [Line 9]: pass in parameter names to the test function. [Line 10]: include necessary assert statements which will be executed for each of the combinations in the list from Step 2. [Line 12-20]: this tests exception handling as well if you pass in as such. ! pytest tests / test_fruits_parametrize . py # tests for a single function ============================= test session starts ============================== platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 4 items tests/test_fruits_parametrize.py .... [100%] =========================== 4 passed in 0.01 seconds =========================== Fixtures What's the benefits of using fixtures? One obvious reason that I know of is about reducing the redundancies of re-defining inputs every time. import numpy as np def add ( nums_list ): return np . sum ( nums_list ) def mul ( nums_list ): return np . prod ( nums_list ) def test_add (): nums_list = [ 1 , 2 , 3 , 4 , 5 ] assert add ( nums_list ) == 15 def test_mul (): nums_list = [ 1 , 2 , 3 , 4 , 5 ] assert add ( nums_list ) == 120 Notice that you defined nums_list twice when we want to test different functions with the same inputs . So to reduce this redundancy, we can do: import pytest @pytest . fixture def sample_nums_list (): nums_list = [ 1 , 2 , 3 , 4 , 5 ] return nums_list Unit Test Mock Readings: https://realpython.com/python-mock-library/ https://docs.python.org/3/library/unittest.mock.html (READ THE API) PeekingDuck draw.poses test suites. Example Walkthrough The example walkthrough assumes you have a basic understanding of pytests. Problem Setup In the field of object detection, given a query image, our task is to localize and classify . Localization needs labels and they are in the form of bounding boxes . As an example, we take the image from albumentations , this image shows us a cat with a bounding box drawn around it. The coordinates are marked accordingly. One thing to note is that the image coordinates' origin starts at the top-left corner, so it is like a cartesian coordinate but rotated clockwise by 90 degrees. The bounding boxes can be represented in various different formats. Most notably, the Pascal-VOC, COCO and the YOLO format. More information can be found here . Our task is two fold, one is to create some utility functions that transform from one format to another, and the other is to write some unit testing to ensure the correctness of our codes. For our purpose, we will deal with two formats, given a bounding box of Pascal-VOC format, we want to convert it to YOLO format, and vice versa. First, we briefly quote from albumentations on the two formats. Pascal-VOC is a format used by the Pascal VOC dataset. Coordinates of a bounding box are encoded with four values in pixels: [x_min, y_min, x_max, y_max] . x_min and y_min are coordinates of the top-left corner of the bounding box. x_max and y_max are coordinates of bottom-right corner of the bounding box. For our purpose, we will call them xyxy for abbreviation. YOLO format's bounding box is represented by four values [x_center, y_center, width, height] . x_center and y_center are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. width and height represent the width and the height of the bounding box. They are normalized as well. For our purpose, we will call them xywhn for abbreviation. More concretely, if an image is of heigth of 480 and width 640 with its bounding box in Pascal-VOC format with coordinates xyxy = [ 98 , 345 , 420 , 462 ] # x1 y1 x2 y2 we want to transform it to the equivalent YOLO coordinates xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] # x y w h and vice versa. As of now, we have verified by hand that [98, 345, 420, 462] indeed convert correctly to [0.4046875, 0.840625, 0.503125, 0.24375] with the given height and width. But as programmers, our task is to reduce manual work, let's write out the functions to convert the bounding boxes in between formats. Step 0. Setting up Folders/Scripts %%bash mkdir -p bbox % cd / content / bbox /content/bbox from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () SRC_DIR = Path . joinpath ( BASE_DIR , \"src\" ) TEST_DIR = Path . joinpath ( BASE_DIR , \"tests\" ) SRC_DIR . mkdir ( parents = True , exist_ok = True ) TEST_DIR . mkdir ( parents = True , exist_ok = True ) %% writefile { SRC_DIR } / __init__ . py \"init file\" Overwriting /content/bbox/src/__init__.py %% writefile { BASE_DIR } / pyproject . toml # Pytest [ tool . pytest . ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\" Overwriting /content/bbox/pyproject.toml Step 1. Writing our functions We will create two functions xyxy2xywhn and xywhn2xyxy , the former takes in inputs of Pascal-VOC style bounding box and transforms it to its equivalent YOLO format, while the latter does the opposite. We will write our functions into src/bbox_utils.py . %% writefile { SRC_DIR } / bbox_utils . py from typing import Union import numpy as np import torch BboxType = Union [ np . ndarray , torch . Tensor ] def cast_to_float ( inputs : BboxType ) -> BboxType : if isinstance ( inputs , torch . Tensor ): return inputs . float () return inputs . astype ( np . float32 ) def clone ( inputs : BboxType ) -> BboxType : if isinstance ( inputs , torch . Tensor ): return inputs . clone () return inputs . copy () def xyxy2xywhn ( inputs : BboxType , height : float , width : float ) -> BboxType : outputs = clone ( inputs ) outputs = cast_to_float ( outputs ) outputs [ ... , [ 0 , 2 ]] /= width outputs [ ... , [ 1 , 3 ]] /= height outputs [ ... , 2 ] -= outputs [ ... , 0 ] outputs [ ... , 3 ] -= outputs [ ... , 1 ] outputs [ ... , 0 ] += outputs [ ... , 2 ] / 2 outputs [ ... , 1 ] += outputs [ ... , 3 ] / 2 return outputs def xywhn2xyxy ( inputs : BboxType , height : float , width : float ) -> BboxType : outputs = clone ( inputs ) outputs = cast_to_float ( outputs ) outputs [ ... , [ 0 , 2 ]] *= width outputs [ ... , [ 1 , 3 ]] *= height outputs [ ... , 0 ] -= outputs [ ... , 2 ] / 2 outputs [ ... , 1 ] -= outputs [ ... , 3 ] / 2 outputs [ ... , 2 ] += outputs [ ... , 0 ] outputs [ ... , 3 ] += outputs [ ... , 1 ] return outputs Overwriting /content/bbox/src/bbox_utils.py After writing the functions, we can simply just test the correctness of the transformation by passing in our pre-defined ground truths defined earlier. For example, when I pass in xyxy = [98, 345, 420, 462] to voc2yolo , I expect it voc2yolo(xyxy, 480, 640) to output xywhn = [0.4046875, 0.840625, 0.503125, 0.24375] . Without using any library, we can simply do something like the following, using an assertion %% writefile -- append { SRC_DIR } / bbox_utils . py xyxy = np . asarray ([ 98 , 345 , 420 , 462 ]) xywhn = np . asarray ([ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ]) assert np . allclose ( xyxy2xywhn ( xyxy , height = 480 , width = 640 ), xywhn , rtol = 1e-05 , atol = 1e-08 ) assert np . allclose ( xywhn2xyxy ( xywhn , height = 480 , width = 640 ), xyxy , rtol = 1e-05 , atol = 1e-08 ) Appending to /content/bbox/src/bbox_utils.py %%bash python src/bbox_utils.py The assertion passed! This may seem fine, but it is very hard to scale up when you add in more transformations, imagine having 10 pairs of transformation functions, you will need to do the assertion 20 times. Furthermore, functions like these often have implicit assumptions that need to be rigourously tested as well. For example, we defined BboxType = Union[np.ndarray, torch.Tensor] and type hinted our functions' inputs and outputs to be both of this type. In particular, when a user pass in an array of type torch.Tensor , I expect the output to be of the same type as in the input. This is important as many operations performed on torch.Tensor does not carry forward to their np.ndarray counterpart. Our assert statement above does not check this, and this will be a problem. See example below. def xywhn2xyxy ( inputs : BboxType , height : float , width : float ) -> BboxType : outputs = clone ( inputs ) outputs = cast_to_float ( outputs ) outputs [ ... , [ 0 , 2 ]] *= width outputs [ ... , [ 1 , 3 ]] *= height outputs [ ... , 0 ] -= outputs [ ... , 2 ] / 2 outputs [ ... , 1 ] -= outputs [ ... , 3 ] / 2 outputs [ ... , 2 ] += outputs [ ... , 0 ] outputs [ ... , 3 ] += outputs [ ... , 1 ] if isinstance ( outputs , torch . Tensor ): outputs = outputs . detach () . cpu () . numpy () return outputs Imagine if we coded our xywhn2xyxy as such, where we hard coded a conversion of outputs to numpy . Then the user will experience errors down stream. xywhn_tensor = torch . tensor ([ 0.1 , 0.2 , 0.3 , 0.4 ]) xyxy_tensor = yolo2voc ( yolo ) unsqueeze_voc = voc . unsqueeze ( 0 ) The user trusted that when he passed in an input of type torch.Tensor , the outputs he get will also be the same type, in which he performed an operation unsqueeze , unique to torch. An error ensues, since now his yolo became a np.ndarray instead. Things get a bit more complicated when I also want to check that the input dimension is the same as the output dimension. For example, if I pass in a 3d-array as input, I expect the same dimension for its outputs. Our assert statement above does not check this. This is where PyTests come in Step 2. Writing our tests This section will be written into tests/test_bbox_utils_before_refactor.py . Defining Global Variables We start first by importing the libraries and define some global constants. %% writefile { TEST_DIR } / test_bbox_utils_before_refactor . py import sys from typing import Union import numpy as np import numpy.testing as npt import pytest import torch sys . path . append ( \"/content/bbox\" ) # append to import properly. from src.bbox_utils import clone , xywhn2xyxy , xyxy2xywhn # tolerance to assert_allclose ATOL , RTOL = 1e-4 , 1e-07 # image width and height HEIGHT , WIDTH = 480 , 640 xyxy = [ 98 , 345 , 420 , 462 ] xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] GT_BBOXES = { \"xyxy\" : xyxy , \"xywhn\" : xywhn } Overwriting /content/bbox/tests/test_bbox_utils_before_refactor.py In line 14 , we defined the tolerance level for allclose , where we allow some slight numerical differences. In lines 17 we defined the height and width of the image. In lines 19-20 , we defined our ground truth values. In line 22 , we defined a global variable GT_BBOXES , a dictionary that holds the bounding box format name as key and its ground truth as values. Note that the ground truth values are equivalent in their own format. Parametrize Input Types We now want to test the correctness of the transformation functions we wrote. Recall earlier we used assert np . allclose ( xyxy2xywhn ( xyxy , height = 480 , width = 640 ), xywhn , rtol = 1e-05 , atol = 1e-08 ) to test the correctness. This assumes that our input type is of np.ndarray , but since our functions allow torch.Tensor as well, we need to ensure that our test function can accept two types of input type, np.ndarray and torch.Tensor , and still work. So for one transform function xyxy2xywhn , we need to test it twice , one for which the input data type is a np.ndarray , the other when it's a torch.Tensor . This means we need to write more assertions! Fortunately, as we have seen in the pytest documentation , the decorator pytest.mark.parametrize does just that. %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py def list2numpy ( input_list ): return np . asarray ( input_list ) def list2torch ( input_list ): return torch . tensor ( input_list ) @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) def test_correct_transformation_xyxy2xywhn ( convert_type ): \"\"\"Test correctness of conversion from VOC to YOLO.\"\"\" from_bbox = convert_type ( GT_BBOXES [ \"xyxy\" ]) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = convert_type ( GT_BBOXES [ \"xywhn\" ]) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_close ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py lines 3-7 consists of two utility functions, list2numpy and list2torch , which converts the ground truth bounding box inputs to either numpy or torch (note that the ground truth is created as a list so that the conversion is easy). line 9 defines the pytest.mark.parametrize decorator where the first argument is a comma-delimited string of parameter names, this string will be the argument names in the function that follows. Here I named it \"convert_type\" ; the second argument will define what values the first argument can take on. This argument has type List[Tuple[Any]] or List[Any] or even single values Any . In our example, our first argument convert_type can take on values of either list2numpy or list2torch , so we should populate the second argument as a list of two elements: [list2numpy, list2torch] . line 10 is our function name test_correct_transformation_xyxy2xywhn and as the name suggests, it will test whether our conversion of voc to yolo is correct. Note that the argument is named convert_type , corresponding exactly to our first argument in the decorator. line 12 is where we apply our argument convert_type to the input GT_BBOXES[\"voc\"] = [98, 345, 420, 462] , the parametrize decorator will then apply list2numpy and list2torch this input and convert the list to a np.ndarray and torch.Tensor respectively. We name this input from_bbox . line 13 will then convert the input using our function voc2yolo to its yolo equivalent format. We name this converted input to_bbox . line 15 gets the ground truth for yolo. Note I need to convert them into the same type as the input ground truth using convert_type . We name this variable expected_bbox . line 17-20 will then check if our converted bounding box input to_bbox matches the ground truth for yolo expected_bbox using numpy.testing and torch.testing . The process does not stop here, since we passed in two values for the function convert_type , it will also loop through the list2torch step. We will see it in action now by running the pytest command. Let's run pytest and see what happens. ! pytest - v tests / test_bbox_utils_before_refactor . py - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 2 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2torch] PASSED =========================== 2 passed in 0.62 seconds =========================== As we can see $ tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn [ list2numpy ] PASSED $ tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn [ list2torch ] PASSED means that the test function has tested for both combinations, the case where the input is a np.ndarray and when it is a torch.Tensor , both passed the assertion! As important thing to realize here is that we are only testing the correctness of this transformation. It does not test whether our transformation functions ensure the same return type as the inputs. For that, we need to write a test to ensure the outputs has the same type as the inputs. %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) def test_correct_return_type_xyxy2xywhn ( convert_type ): from_bbox = convert_type ( GT_BBOXES [ \"xyxy\" ]) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) assert isinstance ( to_bbox , type ( from_bbox )) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py ! pytest - v tests / test_bbox_utils_before_refactor . py - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 4 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_return_type_xyxy2xywhn[list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_return_type_xyxy2xywhn[list2torch] PASSED =========================== 4 passed in 0.65 seconds =========================== Now we are sure that the inputs and outputs of our functions are of the same type! Parametrize Consistent Dimensions Our next step is to test that our transform functions can handle different dimensions. Whether the input is a 3d-tensor, or a 10d-array, all of them should work. Of course, our main goal here is still to test the correctness of the transformation, but bear in mind we need to have a separate test to check the consistency of input and output dimensions (i.e. passing in a 2d-array will result in an output of 2d-array). Let's say we want to test if the code works for 3 dimensions, means checking if the code can execute correctly without error for dimensions in [1d, 2d, 3d] . This is not trivial as we need to check for 6 different cases, a result of the cartesian product of [list2numpy, list2torch] x [0, 1, 2] = {(list2numpy, 0), (list2numpy, 1), ...} a total of 6 combinations. We will continue to leverage pytest 's parametrize to test all 6 cases. There will not be much change besides defining an extra utility function expand_dim will expands the input's dimensions according to the num_dims argument. To be able to use the cartesian product, we simply add one more decorator below our convert_type , in which case it now takes in num_dims as first argument, and [0, 1, 2] as the second, indicating that we want the function to test for the aforementioned 3 dimensions. Having two parametrize decorators stacked together means it will execute in combination, exactly as what we wanted. %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py def expand_dim ( bboxes : Union [ np . ndarray , torch . Tensor ], num_dims : int , ) -> Union [ np . ndarray , torch . Tensor ]: \"\"\"Expand the dimension of bboxes to num_dims. Note: np.expand_dims will not work for tuple dim numpy < 1.18.0 which is not the version in our cicd. \"\"\" bboxes = clone ( bboxes ) return bboxes [( None ,) * num_dims ] @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) @pytest . mark . parametrize ( \"num_dims\" , [ 0 , 1 , 2 ]) def test_correct_transformation_xyxy2xywhn_with_dims ( convert_type , num_dims ): \"\"\"Test conversion from VOC to YOLO.\"\"\" from_bbox = convert_type ( GT_BBOXES [ \"xyxy\" ]) from_bbox = expand_dim ( from_bbox , num_dims ) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = expand_dim ( convert_type ( GT_BBOXES [ \"xywhn\" ]), num_dims ) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_close ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py ! pytest - v tests / test_bbox_utils_before_refactor . py :: test_correct_transformation_xyxy2xywhn_with_dims - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 6 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[0-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[0-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[1-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[1-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[2-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[2-list2torch] PASSED =========================== 6 passed in 0.62 seconds =========================== We see that a total of 6 results were tested by test_correct_transformation_xyxy2xywhn_with_dims . Notice that in each line they indicate the combination, for example, the first line says test_voc2yolo[0-list2numpy] PASSED , which means they tested for the case of 0 dimensions and the input type of np.ndarray . We can also test whether the input dimensions must match the output dimensions. But since np.testing.assert_allclose will raise an error if their shape mismatch, we will skip over this. It may still be good practice to write test as we typically want one test to handle one type of error. a = np . asarray ([ 1 , 2 , 3 ]) b = np . asarray ([[ 1 , 2 , 3 ][) np . testing . assert_allclose ( a , b ) -> raises an error Creating Fixtures to Manage State and Dependencies The idea of fixtures is that if you have multiple test functions that take in a \"fixed\" set of inputs (i.e. GT_BBOXES ), then we should consider using fixtures. In our past 2 test functions, test_correct_return_type_xyxy2xywhn and test_correct_transformation_xyxy2xywhn , we both used the same set of global constant GT_BBOXES , we can imagine now that for our testing on xywhn2xyxy , we would use this variable again. For that we can write a function gt_bboxes that has pytest.fixture as decorator. Subsequently, we can pass gt_bboxes to any test functions. For example, @pytest . fixture ( scope = \"module\" ) def gt_bboxes (): return GT_BBOXES @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) @pytest . mark . parametrize ( \"num_dims\" , [ 0 , 1 , 2 ]) def test_voc2yolo ( gt_bboxes , convert_type , num_dims ): from_bbox = convert_type ( gt_bboxes [ \"xyxy\" ]) ... where we simply defined a new fixture function called gt_bboxes , decorated with pytest.fixture(scope=\"module\") . The scopes defines how frequent your fixture is called, expensive operations often require a higher scope. However, the same result can be achieved with our old method, defining a global constant GT_BBOXES work as well. This is true in our case because GT_BBOXES is a relative cheap operation. But imagine if creating our GT_BBOXES is an expensive operation (i.e. GT_BBOXES involves creating a 10,000 by 10,000 array), then having it defined under a fixture with a proper scope is important. For our purpose, imagine a scenario where we have 3 test files test_1.py, test_2.py, test_3.py which all need GT_BBOXES , then we can now put this fixture in a file called conftest.py , with scope to be defined at a modular level, when running your test suites across all 3 files, gt_bboxes will be called once and re-use it across all functions in these 3 files. %% writefile { TEST_DIR } / conftest . py import pytest xyxy = [ 98 , 345 , 420 , 462 ] xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] GT_BBOXES = { \"xyxy\" : xyxy , \"xywhn\" : xywhn } @pytest . fixture ( scope = \"module\" ) def gt_bboxes (): return GT_BBOXES Overwriting /content/bbox/tests/conftest.py %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) @pytest . mark . parametrize ( \"num_dims\" , [ 0 , 1 , 2 ]) def test_correct_transformation_xyxy2xywhn_with_fixture ( gt_bboxes , convert_type , num_dims ): \"\"\"Test conversion from VOC to YOLO.\"\"\" from_bbox = convert_type ( gt_bboxes [ \"xyxy\" ]) from_bbox = expand_dim ( from_bbox , num_dims ) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = expand_dim ( convert_type ( gt_bboxes [ \"xywhn\" ]), num_dims ) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_close ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py We now run the test on this function we created test_correct_transformation_xyxy2xywhn_with_fixture , the only difference with test_correct_transformation_xyxy2xywhn is that it now takes in an argument gt_bboxes corresponding to the fixture name, all instances of the global variable GT_BBOXES are changed to gt_bboxes . ! pytest - v tests / test_bbox_utils_before_refactor . py :: test_correct_transformation_xyxy2xywhn_with_fixture - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 6 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[0-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[0-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[1-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[1-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[2-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[2-list2torch] PASSED =========================== 6 passed in 0.63 seconds =========================== There is no need to import conftest into any of our tests, pytest handles it for us. Step 3. Refactoring our tests Using parametrize for different transforms Now I actually have more than 10 transform functions that converts bounding boxes. This means I have to write 10 cases: test_xyxy2xywhn , test_xywhn2xyxy , ... Most of the code inside the test functions are the same. We can again leverage on parametrize and define the first argument to be conversion_name which takes on values such as [\"xyxy2xywhn\", \"xywhn2xyxy\"] . These values will then be our identifier on which conversion/transform to use. For that, we need to revamp our fixture gt_bboxes . %% writefile { TEST_DIR } / conftest . py import sys import pytest sys . path . append ( \"/content/bbox\" ) # append to import properly. from src.bbox_utils import xywhn2xyxy , xyxy2xywhn xyxy = [ 98 , 345 , 420 , 462 ] xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] @pytest . fixture ( scope = \"module\" ) def gt_bboxes (): return { \"xyxy2xywhn\" : [ xyxy , xywhn , xyxy2xywhn ], \"xywhn2xyxy\" : [ xywhn , xyxy , xywhn2xyxy ]} Overwriting /content/bbox/tests/conftest.py The changes are: The key is now exact name of the function call of the transformation. This means that if our transform function is xyxy2xywhn , then our key is called \"xyxy2xywhn\" . The corresponding value is a list, where the first element is the ground truth of the input (i.e. xyxy ), and the second element is the ground truth of the output after the conversion (i.e. xywhn ), the third element is the function call itself. %% writefile { TEST_DIR } / test_bbox_utils_after_refactor . py from typing import Union import numpy as np import numpy.testing as npt import pytest import torch from src.bbox_utils import clone CONVERT_TYPES = [ np . array , torch . tensor ] NUM_DIMS = [ 0 , 1 , 2 ] TRANSFORMS = [ \"xyxy2xywhn\" , \"xywhn2xyxy\" ] # tolerance to assert_allclose ATOL , RTOL = 1e-4 , 1e-07 # image height and width HEIGHT , WIDTH = 480 , 640 def list2numpy ( input_list ): return np . asarray ( input_list ) def list2torch ( input_list ): return torch . tensor ( input_list ) def expand_dim ( bboxes : Union [ np . ndarray , torch . Tensor ], num_dims : int , ) -> Union [ np . ndarray , torch . Tensor ]: \"\"\"Expand the dimension of bboxes to num_dims. Note: np.expand_dims will not work for tuple dim numpy < 1.18.0 which is not the version in our cicd. \"\"\" bboxes = clone ( bboxes ) return bboxes [( None ,) * num_dims ] @pytest . mark . parametrize ( \"convert_type\" , CONVERT_TYPES ) class TestBboxTransforms : @pytest . mark . parametrize ( \"conversion_name\" , TRANSFORMS ) def test_correct_return_type ( self , gt_bboxes , convert_type , conversion_name ): from_bbox , _ , conversion_fn = gt_bboxes [ conversion_name ] from_bbox = convert_type ( from_bbox ) to_bbox = conversion_fn ( from_bbox , height = HEIGHT , width = WIDTH ) assert isinstance ( to_bbox , type ( from_bbox )) @pytest . mark . parametrize ( \"num_dims\" , NUM_DIMS ) @pytest . mark . parametrize ( \"conversion_name\" , TRANSFORMS ) def test_correct_transformation ( self , gt_bboxes , convert_type , num_dims , conversion_name ): from_bbox , expected_bbox , conversion_fn = gt_bboxes [ conversion_name ] from_bbox = convert_type ( from_bbox ) from_bbox = expand_dim ( from_bbox , num_dims ) to_bbox = conversion_fn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = expand_dim ( convert_type ( expected_bbox ), num_dims ) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Overwriting /content/bbox/tests/test_bbox_utils_after_refactor.py ! pytest - W ignore - v tests / test_bbox_utils_after_refactor . py - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 16 items tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xyxy2xywhn-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xyxy2xywhn-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xywhn2xyxy-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xywhn2xyxy-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-0-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-0-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-1-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-1-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-2-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-2-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-0-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-0-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-1-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-1-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-2-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-2-convert_type1] PASSED ========================== 16 passed in 0.04 seconds =========================== We see that we achieved the same results. References & Citations https://madewithml.com/courses/mlops/testing/ @misc{goku mohandas_2020, title={Testing Machine Learning Systems: Code, Data and Models - Made With ML}, url={https://madewithml.com/courses/mlops/testing/}, journal={Madewithml.com}, author={Goku Mohandas}, year={2020} } \u200c - https://realpython.com/pytest-python-testing/ @misc{real python_2022, title={Effective Python Testing With Pytest}, url={https://realpython.com/pytest-python-testing/}, journal={Realpython.com}, publisher={Real Python}, author={Real Python}, year={2022}, month={Jun} }","title":"Testing"},{"location":"mlops_docs/testing/testing/#intuition","text":"Tests are a way for us to ensure that something works as intended. We're incentivized to implement tests and discover sources of error as early in the development cycle as possible so that we can reduce increasing downstream costs and wasted time. Once we've designed our tests, we can automatically execute them every time we implement a change to our system and continue to build on them over time. In this lesson, we'll learn how to test machine learning code, data and models to construct a system that we can reliably iterate on.","title":"Intuition"},{"location":"mlops_docs/testing/testing/#types-of-tests","text":"There are many four majors types of tests which are utilized at different points in the development cycle: Unit tests: tests on individual components that each have a single responsibility (ex. function that filters a list). Integration tests: tests on the combined functionality of individual components (ex. data processing). System tests: tests on the design of a system for expected outputs given inputs (ex. training, inference, etc.). Acceptance tests: tests to verify that requirements have been met, usually referred to as User Acceptance Testing (UAT). Regression tests: testing errors we've seen before to ensure new changes don't reintroduce them.","title":"Types of tests"},{"location":"mlops_docs/testing/testing/#how-should-we-test","text":"The framework to use when composing tests is the Arrange Act Assert methodology . Arrange: set up the different inputs to test on. Act: apply the inputs on the component we want to test. Assert: confirm that we received the expected output.","title":"How should we test?"},{"location":"mlops_docs/testing/testing/#what-should-we-be-testing-for","text":"An example: When arranging our inputs and asserting our expected outputs, what are some aspects of our inputs and outputs that we should be testing for? inputs: data types, format, length, edge cases (min/max, small/large, etc.) outputs: data types, formats, exceptions, intermediary and final outputs","title":"What should we be testing for?"},{"location":"mlops_docs/testing/testing/#best-practices","text":"Regardless of the framework we use, it's important to strongly tie testing into the development process. atomic: when creating unit components, we need to ensure that they have a single responsibility so that we can easily test them. If not, we'll need to split them into more granular units. compose: when we create new components, we want to compose tests to validate their functionality. It's a great way to ensure reliability and catch errors early on. regression: we want to account for new errors we come across with a regression test so we can ensure we don't reintroduce the same errors in the future. coverage: we want to ensure that 100% of our codebase has been accounter for. This doesn't mean writing a test for every single line of code but rather accounting for every single line (more on this in the coverage section below). automate: in the event we forget to run our tests before committing to a repository, we want to auto run tests for every commit. We'll learn how to do this locally using pre-commit hooks and remotely (ie. main branch) via GitHub actions in subsequent lessons.","title":"Best practices"},{"location":"mlops_docs/testing/testing/#test-driven-development-or-otherwise","text":"Test-driven development (TDD) is the process where you write a test before completely writing the functionality to ensure that tests are always written. This is in contrast to writing functionality first and then composing tests afterwards. Here are my thoughts on this: good to write tests as we progress, but it's not the representation of correctness. initial time should be spent on design before ever getting into the code or tests. using a test as guide doesn't mean that our functionality is error free. Perfect coverage doesn't mean that our application is error free if those tests aren't meaningful and don't encompass the field of possible inputs, intermediates and outputs. Therefore, we should work towards better design and agility when facing errors, quickly resolving them and writing test cases around them to avoid them next time.","title":"Test-driven development or Otherwise?"},{"location":"mlops_docs/testing/testing/#pytest","text":"We're going to be using pytest as our testing framework for it's powerful builtin features such as parametrization, fixtures, markers, etc.","title":"Pytest"},{"location":"mlops_docs/testing/testing/#configuration","text":"Pytest expects tests to be organized under a tests directory by default. However, we can also use our pyproject.toml file to configure any other test path directories as well. Once in the directory, pytest looks for python scripts starting with tests_*.py but we can configure it to read any other file patterns as well. # Pytest [tool.pytest.ini_options] testpaths = [ \"tests\" ] python_files = \"test_*.py\"","title":"Configuration"},{"location":"mlops_docs/testing/testing/#assertions","text":"Simple assertion testing example. from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () SRC_DIR = Path . joinpath ( BASE_DIR , \"src\" ) TEST_DIR = Path . joinpath ( BASE_DIR , \"tests\" ) SRC_DIR . mkdir ( parents = True , exist_ok = True ) TEST_DIR . mkdir ( parents = True , exist_ok = True ) %% writefile { BASE_DIR } / pyproject . toml # Pytest [ tool . pytest . ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\" Writing /content/pyproject.toml %% writefile { SRC_DIR } / __init__ . py \"init file\" Writing /content/src/__init__.py %% writefile { SRC_DIR } / fruits . py def is_crisp ( fruit ): if fruit : fruit = fruit . lower () if fruit in [ \"apple\" , \"watermelon\" , \"cherries\" ]: return True elif fruit in [ \"orange\" , \"mango\" , \"strawberry\" ]: return False else : raise ValueError ( f \" { fruit } not in known list of fruits.\" ) return False Writing /content/src/fruits.py %% writefile { TEST_DIR } / test_fruits . py import pytest import sys sys . path . append ( \"/content\" ) # append to import properly. from src.fruits import is_crisp def test_is_crisp (): assert is_crisp ( fruit = \"apple\" ) # or == True assert is_crisp ( fruit = \"Apple\" ) assert not is_crisp ( fruit = \"orange\" ) with pytest . raises ( ValueError ): is_crisp ( fruit = None ) is_crisp ( fruit = \"pear\" ) Writing /content/tests/test_fruits.py ! pytest # all tests ! pytest tests / # tests under a directory ! pytest tests / test_fruits . py # tests for a single file ! pytest tests / test_fruits . py :: test_is_crisp # tests for a single function ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.02 seconds =========================== ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.01 seconds =========================== ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.01 seconds =========================== ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 1 item tests/test_fruits.py . [100%] =========================== 1 passed in 0.01 seconds ===========================","title":"Assertions"},{"location":"mlops_docs/testing/testing/#classes","text":"See examples from madewithml repo to understand better.","title":"Classes"},{"location":"mlops_docs/testing/testing/#interfaces","text":"See madewithml interface section .","title":"Interfaces"},{"location":"mlops_docs/testing/testing/#parametrize","text":"So far, in our tests, we've had to create individual assert statements to validate different combinations of inputs and expected outputs. However, there's a bit of redundancy here because the inputs always feed into our functions as arguments and the outputs are compared with our expected outputs. To remove this redundancy, pytest has the @pytest.mark.parametrize decorator which allows us to represent our inputs and outputs as parameters. Let us create a new python file test_fruits_parametrize.py to test it out. %% writefile { TEST_DIR } / test_fruits_parametrize . py import pytest import sys sys . path . append ( \"/content\" ) # append to import properly. from src.fruits import is_crisp @pytest . mark . parametrize ( \"fruit, crisp\" , [ ( \"apple\" , True ), ( \"Apple\" , True ), ( \"orange\" , False ), ], ) def test_is_crisp_parametrize ( fruit , crisp ): assert is_crisp ( fruit = fruit ) == crisp @pytest . mark . parametrize ( \"fruit, exception\" , [ ( \"pear\" , ValueError ), ], ) def test_is_crisp_exceptions ( fruit , exception ): with pytest . raises ( exception ): is_crisp ( fruit = fruit ) Overwriting /content/tests/test_fruits_parametrize.py To fix line number, but for now the line number starts from the decorator @pytest.mark.parametrize . [Line 2]: define the names of the parameters under the decorator, ex. \"fruit, crisp\" (note that this is one string). Note that this string names should correspond to the function defined under the decorator. [Lines 3-7]: provide a list of combinations of values for the parameters from Step 1. [Line 9]: pass in parameter names to the test function. [Line 10]: include necessary assert statements which will be executed for each of the combinations in the list from Step 2. [Line 12-20]: this tests exception handling as well if you pass in as such. ! pytest tests / test_fruits_parametrize . py # tests for a single function ============================= test session starts ============================== platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 rootdir: /content, inifile: plugins: typeguard-2.7.1 collected 4 items tests/test_fruits_parametrize.py .... [100%] =========================== 4 passed in 0.01 seconds ===========================","title":"Parametrize"},{"location":"mlops_docs/testing/testing/#fixtures","text":"What's the benefits of using fixtures? One obvious reason that I know of is about reducing the redundancies of re-defining inputs every time. import numpy as np def add ( nums_list ): return np . sum ( nums_list ) def mul ( nums_list ): return np . prod ( nums_list ) def test_add (): nums_list = [ 1 , 2 , 3 , 4 , 5 ] assert add ( nums_list ) == 15 def test_mul (): nums_list = [ 1 , 2 , 3 , 4 , 5 ] assert add ( nums_list ) == 120 Notice that you defined nums_list twice when we want to test different functions with the same inputs . So to reduce this redundancy, we can do: import pytest @pytest . fixture def sample_nums_list (): nums_list = [ 1 , 2 , 3 , 4 , 5 ] return nums_list","title":"Fixtures"},{"location":"mlops_docs/testing/testing/#unit-test","text":"","title":"Unit Test"},{"location":"mlops_docs/testing/testing/#mock","text":"Readings: https://realpython.com/python-mock-library/ https://docs.python.org/3/library/unittest.mock.html (READ THE API) PeekingDuck draw.poses test suites.","title":"Mock"},{"location":"mlops_docs/testing/testing/#example-walkthrough","text":"The example walkthrough assumes you have a basic understanding of pytests.","title":"Example Walkthrough"},{"location":"mlops_docs/testing/testing/#problem-setup","text":"In the field of object detection, given a query image, our task is to localize and classify . Localization needs labels and they are in the form of bounding boxes . As an example, we take the image from albumentations , this image shows us a cat with a bounding box drawn around it. The coordinates are marked accordingly. One thing to note is that the image coordinates' origin starts at the top-left corner, so it is like a cartesian coordinate but rotated clockwise by 90 degrees. The bounding boxes can be represented in various different formats. Most notably, the Pascal-VOC, COCO and the YOLO format. More information can be found here . Our task is two fold, one is to create some utility functions that transform from one format to another, and the other is to write some unit testing to ensure the correctness of our codes. For our purpose, we will deal with two formats, given a bounding box of Pascal-VOC format, we want to convert it to YOLO format, and vice versa. First, we briefly quote from albumentations on the two formats. Pascal-VOC is a format used by the Pascal VOC dataset. Coordinates of a bounding box are encoded with four values in pixels: [x_min, y_min, x_max, y_max] . x_min and y_min are coordinates of the top-left corner of the bounding box. x_max and y_max are coordinates of bottom-right corner of the bounding box. For our purpose, we will call them xyxy for abbreviation. YOLO format's bounding box is represented by four values [x_center, y_center, width, height] . x_center and y_center are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. width and height represent the width and the height of the bounding box. They are normalized as well. For our purpose, we will call them xywhn for abbreviation. More concretely, if an image is of heigth of 480 and width 640 with its bounding box in Pascal-VOC format with coordinates xyxy = [ 98 , 345 , 420 , 462 ] # x1 y1 x2 y2 we want to transform it to the equivalent YOLO coordinates xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] # x y w h and vice versa. As of now, we have verified by hand that [98, 345, 420, 462] indeed convert correctly to [0.4046875, 0.840625, 0.503125, 0.24375] with the given height and width. But as programmers, our task is to reduce manual work, let's write out the functions to convert the bounding boxes in between formats.","title":"Problem Setup"},{"location":"mlops_docs/testing/testing/#step-0-setting-up-foldersscripts","text":"%%bash mkdir -p bbox % cd / content / bbox /content/bbox from pathlib import Path # Creating Directories BASE_DIR = Path ( \"__file__\" ) . parent . absolute () SRC_DIR = Path . joinpath ( BASE_DIR , \"src\" ) TEST_DIR = Path . joinpath ( BASE_DIR , \"tests\" ) SRC_DIR . mkdir ( parents = True , exist_ok = True ) TEST_DIR . mkdir ( parents = True , exist_ok = True ) %% writefile { SRC_DIR } / __init__ . py \"init file\" Overwriting /content/bbox/src/__init__.py %% writefile { BASE_DIR } / pyproject . toml # Pytest [ tool . pytest . ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\" Overwriting /content/bbox/pyproject.toml","title":"Step 0. Setting up Folders/Scripts"},{"location":"mlops_docs/testing/testing/#step-1-writing-our-functions","text":"We will create two functions xyxy2xywhn and xywhn2xyxy , the former takes in inputs of Pascal-VOC style bounding box and transforms it to its equivalent YOLO format, while the latter does the opposite. We will write our functions into src/bbox_utils.py . %% writefile { SRC_DIR } / bbox_utils . py from typing import Union import numpy as np import torch BboxType = Union [ np . ndarray , torch . Tensor ] def cast_to_float ( inputs : BboxType ) -> BboxType : if isinstance ( inputs , torch . Tensor ): return inputs . float () return inputs . astype ( np . float32 ) def clone ( inputs : BboxType ) -> BboxType : if isinstance ( inputs , torch . Tensor ): return inputs . clone () return inputs . copy () def xyxy2xywhn ( inputs : BboxType , height : float , width : float ) -> BboxType : outputs = clone ( inputs ) outputs = cast_to_float ( outputs ) outputs [ ... , [ 0 , 2 ]] /= width outputs [ ... , [ 1 , 3 ]] /= height outputs [ ... , 2 ] -= outputs [ ... , 0 ] outputs [ ... , 3 ] -= outputs [ ... , 1 ] outputs [ ... , 0 ] += outputs [ ... , 2 ] / 2 outputs [ ... , 1 ] += outputs [ ... , 3 ] / 2 return outputs def xywhn2xyxy ( inputs : BboxType , height : float , width : float ) -> BboxType : outputs = clone ( inputs ) outputs = cast_to_float ( outputs ) outputs [ ... , [ 0 , 2 ]] *= width outputs [ ... , [ 1 , 3 ]] *= height outputs [ ... , 0 ] -= outputs [ ... , 2 ] / 2 outputs [ ... , 1 ] -= outputs [ ... , 3 ] / 2 outputs [ ... , 2 ] += outputs [ ... , 0 ] outputs [ ... , 3 ] += outputs [ ... , 1 ] return outputs Overwriting /content/bbox/src/bbox_utils.py After writing the functions, we can simply just test the correctness of the transformation by passing in our pre-defined ground truths defined earlier. For example, when I pass in xyxy = [98, 345, 420, 462] to voc2yolo , I expect it voc2yolo(xyxy, 480, 640) to output xywhn = [0.4046875, 0.840625, 0.503125, 0.24375] . Without using any library, we can simply do something like the following, using an assertion %% writefile -- append { SRC_DIR } / bbox_utils . py xyxy = np . asarray ([ 98 , 345 , 420 , 462 ]) xywhn = np . asarray ([ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ]) assert np . allclose ( xyxy2xywhn ( xyxy , height = 480 , width = 640 ), xywhn , rtol = 1e-05 , atol = 1e-08 ) assert np . allclose ( xywhn2xyxy ( xywhn , height = 480 , width = 640 ), xyxy , rtol = 1e-05 , atol = 1e-08 ) Appending to /content/bbox/src/bbox_utils.py %%bash python src/bbox_utils.py The assertion passed! This may seem fine, but it is very hard to scale up when you add in more transformations, imagine having 10 pairs of transformation functions, you will need to do the assertion 20 times. Furthermore, functions like these often have implicit assumptions that need to be rigourously tested as well. For example, we defined BboxType = Union[np.ndarray, torch.Tensor] and type hinted our functions' inputs and outputs to be both of this type. In particular, when a user pass in an array of type torch.Tensor , I expect the output to be of the same type as in the input. This is important as many operations performed on torch.Tensor does not carry forward to their np.ndarray counterpart. Our assert statement above does not check this, and this will be a problem. See example below. def xywhn2xyxy ( inputs : BboxType , height : float , width : float ) -> BboxType : outputs = clone ( inputs ) outputs = cast_to_float ( outputs ) outputs [ ... , [ 0 , 2 ]] *= width outputs [ ... , [ 1 , 3 ]] *= height outputs [ ... , 0 ] -= outputs [ ... , 2 ] / 2 outputs [ ... , 1 ] -= outputs [ ... , 3 ] / 2 outputs [ ... , 2 ] += outputs [ ... , 0 ] outputs [ ... , 3 ] += outputs [ ... , 1 ] if isinstance ( outputs , torch . Tensor ): outputs = outputs . detach () . cpu () . numpy () return outputs Imagine if we coded our xywhn2xyxy as such, where we hard coded a conversion of outputs to numpy . Then the user will experience errors down stream. xywhn_tensor = torch . tensor ([ 0.1 , 0.2 , 0.3 , 0.4 ]) xyxy_tensor = yolo2voc ( yolo ) unsqueeze_voc = voc . unsqueeze ( 0 ) The user trusted that when he passed in an input of type torch.Tensor , the outputs he get will also be the same type, in which he performed an operation unsqueeze , unique to torch. An error ensues, since now his yolo became a np.ndarray instead. Things get a bit more complicated when I also want to check that the input dimension is the same as the output dimension. For example, if I pass in a 3d-array as input, I expect the same dimension for its outputs. Our assert statement above does not check this. This is where PyTests come in","title":"Step 1. Writing our functions"},{"location":"mlops_docs/testing/testing/#step-2-writing-our-tests","text":"This section will be written into tests/test_bbox_utils_before_refactor.py .","title":"Step 2. Writing our tests"},{"location":"mlops_docs/testing/testing/#defining-global-variables","text":"We start first by importing the libraries and define some global constants. %% writefile { TEST_DIR } / test_bbox_utils_before_refactor . py import sys from typing import Union import numpy as np import numpy.testing as npt import pytest import torch sys . path . append ( \"/content/bbox\" ) # append to import properly. from src.bbox_utils import clone , xywhn2xyxy , xyxy2xywhn # tolerance to assert_allclose ATOL , RTOL = 1e-4 , 1e-07 # image width and height HEIGHT , WIDTH = 480 , 640 xyxy = [ 98 , 345 , 420 , 462 ] xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] GT_BBOXES = { \"xyxy\" : xyxy , \"xywhn\" : xywhn } Overwriting /content/bbox/tests/test_bbox_utils_before_refactor.py In line 14 , we defined the tolerance level for allclose , where we allow some slight numerical differences. In lines 17 we defined the height and width of the image. In lines 19-20 , we defined our ground truth values. In line 22 , we defined a global variable GT_BBOXES , a dictionary that holds the bounding box format name as key and its ground truth as values. Note that the ground truth values are equivalent in their own format.","title":"Defining Global Variables"},{"location":"mlops_docs/testing/testing/#parametrize-input-types","text":"We now want to test the correctness of the transformation functions we wrote. Recall earlier we used assert np . allclose ( xyxy2xywhn ( xyxy , height = 480 , width = 640 ), xywhn , rtol = 1e-05 , atol = 1e-08 ) to test the correctness. This assumes that our input type is of np.ndarray , but since our functions allow torch.Tensor as well, we need to ensure that our test function can accept two types of input type, np.ndarray and torch.Tensor , and still work. So for one transform function xyxy2xywhn , we need to test it twice , one for which the input data type is a np.ndarray , the other when it's a torch.Tensor . This means we need to write more assertions! Fortunately, as we have seen in the pytest documentation , the decorator pytest.mark.parametrize does just that. %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py def list2numpy ( input_list ): return np . asarray ( input_list ) def list2torch ( input_list ): return torch . tensor ( input_list ) @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) def test_correct_transformation_xyxy2xywhn ( convert_type ): \"\"\"Test correctness of conversion from VOC to YOLO.\"\"\" from_bbox = convert_type ( GT_BBOXES [ \"xyxy\" ]) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = convert_type ( GT_BBOXES [ \"xywhn\" ]) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_close ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py lines 3-7 consists of two utility functions, list2numpy and list2torch , which converts the ground truth bounding box inputs to either numpy or torch (note that the ground truth is created as a list so that the conversion is easy). line 9 defines the pytest.mark.parametrize decorator where the first argument is a comma-delimited string of parameter names, this string will be the argument names in the function that follows. Here I named it \"convert_type\" ; the second argument will define what values the first argument can take on. This argument has type List[Tuple[Any]] or List[Any] or even single values Any . In our example, our first argument convert_type can take on values of either list2numpy or list2torch , so we should populate the second argument as a list of two elements: [list2numpy, list2torch] . line 10 is our function name test_correct_transformation_xyxy2xywhn and as the name suggests, it will test whether our conversion of voc to yolo is correct. Note that the argument is named convert_type , corresponding exactly to our first argument in the decorator. line 12 is where we apply our argument convert_type to the input GT_BBOXES[\"voc\"] = [98, 345, 420, 462] , the parametrize decorator will then apply list2numpy and list2torch this input and convert the list to a np.ndarray and torch.Tensor respectively. We name this input from_bbox . line 13 will then convert the input using our function voc2yolo to its yolo equivalent format. We name this converted input to_bbox . line 15 gets the ground truth for yolo. Note I need to convert them into the same type as the input ground truth using convert_type . We name this variable expected_bbox . line 17-20 will then check if our converted bounding box input to_bbox matches the ground truth for yolo expected_bbox using numpy.testing and torch.testing . The process does not stop here, since we passed in two values for the function convert_type , it will also loop through the list2torch step. We will see it in action now by running the pytest command. Let's run pytest and see what happens. ! pytest - v tests / test_bbox_utils_before_refactor . py - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 2 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2torch] PASSED =========================== 2 passed in 0.62 seconds =========================== As we can see $ tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn [ list2numpy ] PASSED $ tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn [ list2torch ] PASSED means that the test function has tested for both combinations, the case where the input is a np.ndarray and when it is a torch.Tensor , both passed the assertion! As important thing to realize here is that we are only testing the correctness of this transformation. It does not test whether our transformation functions ensure the same return type as the inputs. For that, we need to write a test to ensure the outputs has the same type as the inputs. %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) def test_correct_return_type_xyxy2xywhn ( convert_type ): from_bbox = convert_type ( GT_BBOXES [ \"xyxy\" ]) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) assert isinstance ( to_bbox , type ( from_bbox )) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py ! pytest - v tests / test_bbox_utils_before_refactor . py - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 4 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_return_type_xyxy2xywhn[list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_return_type_xyxy2xywhn[list2torch] PASSED =========================== 4 passed in 0.65 seconds =========================== Now we are sure that the inputs and outputs of our functions are of the same type!","title":"Parametrize Input Types"},{"location":"mlops_docs/testing/testing/#parametrize-consistent-dimensions","text":"Our next step is to test that our transform functions can handle different dimensions. Whether the input is a 3d-tensor, or a 10d-array, all of them should work. Of course, our main goal here is still to test the correctness of the transformation, but bear in mind we need to have a separate test to check the consistency of input and output dimensions (i.e. passing in a 2d-array will result in an output of 2d-array). Let's say we want to test if the code works for 3 dimensions, means checking if the code can execute correctly without error for dimensions in [1d, 2d, 3d] . This is not trivial as we need to check for 6 different cases, a result of the cartesian product of [list2numpy, list2torch] x [0, 1, 2] = {(list2numpy, 0), (list2numpy, 1), ...} a total of 6 combinations. We will continue to leverage pytest 's parametrize to test all 6 cases. There will not be much change besides defining an extra utility function expand_dim will expands the input's dimensions according to the num_dims argument. To be able to use the cartesian product, we simply add one more decorator below our convert_type , in which case it now takes in num_dims as first argument, and [0, 1, 2] as the second, indicating that we want the function to test for the aforementioned 3 dimensions. Having two parametrize decorators stacked together means it will execute in combination, exactly as what we wanted. %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py def expand_dim ( bboxes : Union [ np . ndarray , torch . Tensor ], num_dims : int , ) -> Union [ np . ndarray , torch . Tensor ]: \"\"\"Expand the dimension of bboxes to num_dims. Note: np.expand_dims will not work for tuple dim numpy < 1.18.0 which is not the version in our cicd. \"\"\" bboxes = clone ( bboxes ) return bboxes [( None ,) * num_dims ] @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) @pytest . mark . parametrize ( \"num_dims\" , [ 0 , 1 , 2 ]) def test_correct_transformation_xyxy2xywhn_with_dims ( convert_type , num_dims ): \"\"\"Test conversion from VOC to YOLO.\"\"\" from_bbox = convert_type ( GT_BBOXES [ \"xyxy\" ]) from_bbox = expand_dim ( from_bbox , num_dims ) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = expand_dim ( convert_type ( GT_BBOXES [ \"xywhn\" ]), num_dims ) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_close ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py ! pytest - v tests / test_bbox_utils_before_refactor . py :: test_correct_transformation_xyxy2xywhn_with_dims - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 6 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[0-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[0-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[1-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[1-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[2-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[2-list2torch] PASSED =========================== 6 passed in 0.62 seconds =========================== We see that a total of 6 results were tested by test_correct_transformation_xyxy2xywhn_with_dims . Notice that in each line they indicate the combination, for example, the first line says test_voc2yolo[0-list2numpy] PASSED , which means they tested for the case of 0 dimensions and the input type of np.ndarray . We can also test whether the input dimensions must match the output dimensions. But since np.testing.assert_allclose will raise an error if their shape mismatch, we will skip over this. It may still be good practice to write test as we typically want one test to handle one type of error. a = np . asarray ([ 1 , 2 , 3 ]) b = np . asarray ([[ 1 , 2 , 3 ][) np . testing . assert_allclose ( a , b ) -> raises an error","title":"Parametrize Consistent Dimensions"},{"location":"mlops_docs/testing/testing/#creating-fixtures-to-manage-state-and-dependencies","text":"The idea of fixtures is that if you have multiple test functions that take in a \"fixed\" set of inputs (i.e. GT_BBOXES ), then we should consider using fixtures. In our past 2 test functions, test_correct_return_type_xyxy2xywhn and test_correct_transformation_xyxy2xywhn , we both used the same set of global constant GT_BBOXES , we can imagine now that for our testing on xywhn2xyxy , we would use this variable again. For that we can write a function gt_bboxes that has pytest.fixture as decorator. Subsequently, we can pass gt_bboxes to any test functions. For example, @pytest . fixture ( scope = \"module\" ) def gt_bboxes (): return GT_BBOXES @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) @pytest . mark . parametrize ( \"num_dims\" , [ 0 , 1 , 2 ]) def test_voc2yolo ( gt_bboxes , convert_type , num_dims ): from_bbox = convert_type ( gt_bboxes [ \"xyxy\" ]) ... where we simply defined a new fixture function called gt_bboxes , decorated with pytest.fixture(scope=\"module\") . The scopes defines how frequent your fixture is called, expensive operations often require a higher scope. However, the same result can be achieved with our old method, defining a global constant GT_BBOXES work as well. This is true in our case because GT_BBOXES is a relative cheap operation. But imagine if creating our GT_BBOXES is an expensive operation (i.e. GT_BBOXES involves creating a 10,000 by 10,000 array), then having it defined under a fixture with a proper scope is important. For our purpose, imagine a scenario where we have 3 test files test_1.py, test_2.py, test_3.py which all need GT_BBOXES , then we can now put this fixture in a file called conftest.py , with scope to be defined at a modular level, when running your test suites across all 3 files, gt_bboxes will be called once and re-use it across all functions in these 3 files. %% writefile { TEST_DIR } / conftest . py import pytest xyxy = [ 98 , 345 , 420 , 462 ] xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] GT_BBOXES = { \"xyxy\" : xyxy , \"xywhn\" : xywhn } @pytest . fixture ( scope = \"module\" ) def gt_bboxes (): return GT_BBOXES Overwriting /content/bbox/tests/conftest.py %% writefile -- append { TEST_DIR } / test_bbox_utils_before_refactor . py @pytest . mark . parametrize ( \"convert_type\" , [ list2numpy , list2torch ]) @pytest . mark . parametrize ( \"num_dims\" , [ 0 , 1 , 2 ]) def test_correct_transformation_xyxy2xywhn_with_fixture ( gt_bboxes , convert_type , num_dims ): \"\"\"Test conversion from VOC to YOLO.\"\"\" from_bbox = convert_type ( gt_bboxes [ \"xyxy\" ]) from_bbox = expand_dim ( from_bbox , num_dims ) to_bbox = xyxy2xywhn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = expand_dim ( convert_type ( gt_bboxes [ \"xywhn\" ]), num_dims ) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_close ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py We now run the test on this function we created test_correct_transformation_xyxy2xywhn_with_fixture , the only difference with test_correct_transformation_xyxy2xywhn is that it now takes in an argument gt_bboxes corresponding to the fixture name, all instances of the global variable GT_BBOXES are changed to gt_bboxes . ! pytest - v tests / test_bbox_utils_before_refactor . py :: test_correct_transformation_xyxy2xywhn_with_fixture - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 6 items tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[0-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[0-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[1-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[1-list2torch] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[2-list2numpy] PASSED tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[2-list2torch] PASSED =========================== 6 passed in 0.63 seconds =========================== There is no need to import conftest into any of our tests, pytest handles it for us.","title":"Creating Fixtures to Manage State and Dependencies"},{"location":"mlops_docs/testing/testing/#step-3-refactoring-our-tests","text":"","title":"Step 3. Refactoring our tests"},{"location":"mlops_docs/testing/testing/#using-parametrize-for-different-transforms","text":"Now I actually have more than 10 transform functions that converts bounding boxes. This means I have to write 10 cases: test_xyxy2xywhn , test_xywhn2xyxy , ... Most of the code inside the test functions are the same. We can again leverage on parametrize and define the first argument to be conversion_name which takes on values such as [\"xyxy2xywhn\", \"xywhn2xyxy\"] . These values will then be our identifier on which conversion/transform to use. For that, we need to revamp our fixture gt_bboxes . %% writefile { TEST_DIR } / conftest . py import sys import pytest sys . path . append ( \"/content/bbox\" ) # append to import properly. from src.bbox_utils import xywhn2xyxy , xyxy2xywhn xyxy = [ 98 , 345 , 420 , 462 ] xywhn = [ 0.4046875 , 0.840625 , 0.503125 , 0.24375 ] @pytest . fixture ( scope = \"module\" ) def gt_bboxes (): return { \"xyxy2xywhn\" : [ xyxy , xywhn , xyxy2xywhn ], \"xywhn2xyxy\" : [ xywhn , xyxy , xywhn2xyxy ]} Overwriting /content/bbox/tests/conftest.py The changes are: The key is now exact name of the function call of the transformation. This means that if our transform function is xyxy2xywhn , then our key is called \"xyxy2xywhn\" . The corresponding value is a list, where the first element is the ground truth of the input (i.e. xyxy ), and the second element is the ground truth of the output after the conversion (i.e. xywhn ), the third element is the function call itself. %% writefile { TEST_DIR } / test_bbox_utils_after_refactor . py from typing import Union import numpy as np import numpy.testing as npt import pytest import torch from src.bbox_utils import clone CONVERT_TYPES = [ np . array , torch . tensor ] NUM_DIMS = [ 0 , 1 , 2 ] TRANSFORMS = [ \"xyxy2xywhn\" , \"xywhn2xyxy\" ] # tolerance to assert_allclose ATOL , RTOL = 1e-4 , 1e-07 # image height and width HEIGHT , WIDTH = 480 , 640 def list2numpy ( input_list ): return np . asarray ( input_list ) def list2torch ( input_list ): return torch . tensor ( input_list ) def expand_dim ( bboxes : Union [ np . ndarray , torch . Tensor ], num_dims : int , ) -> Union [ np . ndarray , torch . Tensor ]: \"\"\"Expand the dimension of bboxes to num_dims. Note: np.expand_dims will not work for tuple dim numpy < 1.18.0 which is not the version in our cicd. \"\"\" bboxes = clone ( bboxes ) return bboxes [( None ,) * num_dims ] @pytest . mark . parametrize ( \"convert_type\" , CONVERT_TYPES ) class TestBboxTransforms : @pytest . mark . parametrize ( \"conversion_name\" , TRANSFORMS ) def test_correct_return_type ( self , gt_bboxes , convert_type , conversion_name ): from_bbox , _ , conversion_fn = gt_bboxes [ conversion_name ] from_bbox = convert_type ( from_bbox ) to_bbox = conversion_fn ( from_bbox , height = HEIGHT , width = WIDTH ) assert isinstance ( to_bbox , type ( from_bbox )) @pytest . mark . parametrize ( \"num_dims\" , NUM_DIMS ) @pytest . mark . parametrize ( \"conversion_name\" , TRANSFORMS ) def test_correct_transformation ( self , gt_bboxes , convert_type , num_dims , conversion_name ): from_bbox , expected_bbox , conversion_fn = gt_bboxes [ conversion_name ] from_bbox = convert_type ( from_bbox ) from_bbox = expand_dim ( from_bbox , num_dims ) to_bbox = conversion_fn ( from_bbox , height = HEIGHT , width = WIDTH ) expected_bbox = expand_dim ( convert_type ( expected_bbox ), num_dims ) if isinstance ( to_bbox , torch . Tensor ): torch . testing . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) else : npt . assert_allclose ( to_bbox , expected_bbox , atol = ATOL , rtol = RTOL ) Overwriting /content/bbox/tests/test_bbox_utils_after_refactor.py ! pytest - W ignore - v tests / test_bbox_utils_after_refactor . py - s # tests for a single file ============================= test session starts ============================== platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3 cachedir: .pytest_cache rootdir: /content/bbox, inifile: plugins: typeguard-2.7.1 collected 16 items tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xyxy2xywhn-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xyxy2xywhn-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xywhn2xyxy-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xywhn2xyxy-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-0-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-0-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-1-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-1-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-2-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-2-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-0-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-0-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-1-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-1-convert_type1] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-2-convert_type0] PASSED tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-2-convert_type1] PASSED ========================== 16 passed in 0.04 seconds =========================== We see that we achieved the same results.","title":"Using parametrize for different transforms"},{"location":"mlops_docs/testing/testing/#references-citations","text":"https://madewithml.com/courses/mlops/testing/ @misc{goku mohandas_2020, title={Testing Machine Learning Systems: Code, Data and Models - Made With ML}, url={https://madewithml.com/courses/mlops/testing/}, journal={Madewithml.com}, author={Goku Mohandas}, year={2020} } \u200c - https://realpython.com/pytest-python-testing/ @misc{real python_2022, title={Effective Python Testing With Pytest}, url={https://realpython.com/pytest-python-testing/}, journal={Realpython.com}, publisher={Real Python}, author={Real Python}, year={2022}, month={Jun} }","title":"References &amp; Citations"},{"location":"software_principles/design_patterns/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Strategy Pattern Inheritance and Composition Use David's example: A is a B vs A is made up of B. Dependency Inverson Principle Definition In object-oriented design, the dependency inversion principle is a specific methodology for loosely coupling software modules. When following this principle, the conventional dependency relationships established from high-level, policy-setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. The principle states:[1] High-level modules should not import anything from low-level modules. Both should depend on abstractions (e.g., interfaces). Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. By dictating that both high-level and low-level objects must depend on the same abstraction, this design principle inverts the way some people may think about object-oriented programming.[2] References: Abstractions should not depend on implementations. Wikipedia Low Level and High Level Modules Low level modules are \"low level\" because they have no dependencies, or no relevant dependencies. Very often, they can be easily reused in different contexts without introducing any separate, formal interfaces - which means, reusing them is straightforward, simple and does not require any Dependency Inversion. High level modules, however, are \"high level\", because they require other, lower level modules to work. But if they are tied to a specific low-level implementation, this often prevents to reuse them in a different context. High level modules depend on low level modules, but shouldn't depend on their implementation. This can be achieved by using interfaces, thus decoupling the definition of the service from the implementation. References: https://stackoverflow.com/questions/3780388/what-are-high-level-modules-and-low-level-modules-in-the-context-of-depende Example CustomDataset is a high-level module that will apply transformation to the data and return the transformed data depending on the stage . ImageClassificationTransforms is a low-level module that will return the transformation functions. In our code, the high level module depends on the low level module such that the creation of ImageClassificationTransforms is done inside the CustomDataset constructor. This leads to high coupling . The code looks fine but if we want to change the ImageClassificationTransforms to ImageSegmentationTransforms , then we have to change the CustomDataset code in two places: Type hint of ImageClassificationTransforms to ImageSegmentationTransforms ; Change manually the ImageClassificationTransforms to ImageSegmentationTransforms in the constructor. Things soon get out of hand if we have a lot more of such dependencies, such as ObjectDetectionTransforms , ImageCaptioningTransforms , etc. One way to solve this is to use the Dependency Inversion Principle . We can create an interface Transforms that will be implemented by ImageClassificationTransforms , ImageSegmentationTransforms , etc. Then, we can pass the Transforms object to the CustomDataset constructor __init__ method. This way, the CustomDataset will depend on the Transforms interface and not on the ImageClassificationTransforms class. This way, we can change the ImageClassificationTransforms to ImageSegmentationTransforms without changing the CustomDataset code. This is called Dependency Inversion . The abstraction does not depend on details simply mean the abstract class should not hold any implementation. The implementation should be done in the concrete class. Example: In my Transforms(ABC) abstract class/interface below, I have two abstract methods get_train_transforms and get_test_transforms . These methods are not implemented in the abstract class. They are implemented in the concrete class ImageClassificationTransforms . This is the second rule in Dependency Inversion Principle . In the high level module CustomDataset , I have a constructor __init__ that takes in a Transforms abstract class/interface. Now my CustomDataset depends on the Transforms abstraction and not on the ImageClassificationTransforms class. This is the first rule in Dependency Inversion Principle . Furthermore, if you were to switch your task from image classification to image segmentation, you can simply change the ImageClassificationTransforms to ImageSegmentationTransforms without changing the CustomDataset code as you are not creating/coupled to the ImageClassificationTransforms class. class Transforms ( ABC ): \"\"\"Abstract class for transforms.\"\"\" @abstractmethod def get_train_transforms ( self ) -> Callable : \"\"\"Get train transforms.\"\"\" @abstractmethod def get_test_transforms ( self ) -> Callable : \"\"\"Get test transforms.\"\"\" class ImageClassificationTransforms ( Transforms ): \"\"\"Dummy class for image classification transforms.\"\"\" def get_train_transforms ( self ) -> Callable : \"\"\"Get train transforms.\"\"\" print ( \"Getting image classification train transforms.\" ) return lambda x : None def get_test_transforms ( self ) -> Callable : \"\"\"Get test transforms.\"\"\" print ( \"Getting image classification test transforms.\" ) return lambda x : None class ImageSegmentationTransforms ( Transforms ): \"\"\"Dummy class for image segmentation transforms.\"\"\" def get_train_transforms ( self ) -> Callable : \"\"\"Get train transforms.\"\"\" print ( \"Getting image segmentation train transforms.\" ) return lambda x : None def get_test_transforms ( self ) -> Callable : \"\"\"Get test transforms.\"\"\" print ( \"Getting image segmentation test transforms.\" ) return lambda x : None class CustomDataset : def __init__ ( self , transforms : Transforms , stage : str = \"train\" ) -> None : self . stage = stage self . transforms = transforms def apply_transforms ( self , dummy_data : Any = None ) -> Any : \"\"\"Apply transforms to dataset based on stage.\"\"\" if self . stage == \"train\" : transformed = self . transforms . get_train_transforms ()( dummy_data ) else : transformed = self . transforms . get_test_transforms ()( dummy_data ) return transformed Originally, CustomDataset creates its own dependency and it is the one controlling the dependency. Now after applying Dependency Inversion Principle , CustomDataset is no longer creating its own dependency. It is now injected with the dependency. This inverts the control of the dependency from CustomDataset to the caller of CustomDataset . This is the Dependency Inversion Principle . More concretely, in traditional sense, since class A depends on class B, then class A is the one creating the dependency. But after applying Dependency Inversion Principle , class A is no longer creating the dependency. Instead, the dependency is instantiated outside of class A at runtime and is injected into class A. This is the Dependency Inversion Principle , a form of Inversion of Control . # convert C# to Python class AppPoolWatcher : # high level module def __init__ ( self ): self . writer = EventLogWriter () def notify ( self , message ): self . writer . write ( message ) class EventLogWriter : # low level module def write ( self , message ): # Write to event log here pass High level module AppPoolWatcher depends on low level module EventLogWriter where EventLogWriter is a concrete class and not an interface/abstract class. creation of EventLogWriter is tightly coupled with AppPoolWatcher class. in other words, AppPoolWatcher creates EventLogWriter object since we are creating EventLogWriter object inside the constructor of AppPoolWatcher . Now the problem is that if we want to change the implementation of EventLogWriter then we will have to change the AppPoolWatcher class as well. This is not a good design. For example, if we want our AppPoolWatcher to write the error messages to an email instead of event log then we will have to change the self.writer to say, EmailWriter and then create an instance of EmailWriter inside the class. The problem will get even worse when we have more actions to take selectively, like sending SMS. Then we will have to have one more class whose instance will be kept inside the AppPoolWatcher. The dependency inversion principle says that we need to decouple this system in such a way that the higher level modules i.e. the AppPoolWatcher in our case will depend on a simple abstraction and will use it. This abstraction will in turn will be mapped to some concrete class which will perform the actual operation. (Next we will see how this can be done) Dependency Injection https://stackoverflow.com/questions/1314730/which-design-patterns-can-be-applied-to-the-configuration-settings-problem https://www.youtube.com/watch?v=2ejbLVkCndI http://martinfowler.com/articles/injection.html Terminology Polymorphism: Further Readings https://eugeneyan.com/writing/design-patterns/ https://github.com/msaroufim/ml-design-patterns https://refactoring.guru/design-patterns https://en.wikipedia.org/wiki/Design_Patterns","title":"Design Patterns"},{"location":"software_principles/design_patterns/#strategy-pattern","text":"","title":"Strategy Pattern"},{"location":"software_principles/design_patterns/#inheritance-and-composition","text":"Use David's example: A is a B vs A is made up of B.","title":"Inheritance and Composition"},{"location":"software_principles/design_patterns/#dependency-inverson-principle","text":"","title":"Dependency Inverson Principle"},{"location":"software_principles/design_patterns/#definition","text":"In object-oriented design, the dependency inversion principle is a specific methodology for loosely coupling software modules. When following this principle, the conventional dependency relationships established from high-level, policy-setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. The principle states:[1] High-level modules should not import anything from low-level modules. Both should depend on abstractions (e.g., interfaces). Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. By dictating that both high-level and low-level objects must depend on the same abstraction, this design principle inverts the way some people may think about object-oriented programming.[2] References: Abstractions should not depend on implementations. Wikipedia","title":"Definition"},{"location":"software_principles/design_patterns/#low-level-and-high-level-modules","text":"Low level modules are \"low level\" because they have no dependencies, or no relevant dependencies. Very often, they can be easily reused in different contexts without introducing any separate, formal interfaces - which means, reusing them is straightforward, simple and does not require any Dependency Inversion. High level modules, however, are \"high level\", because they require other, lower level modules to work. But if they are tied to a specific low-level implementation, this often prevents to reuse them in a different context. High level modules depend on low level modules, but shouldn't depend on their implementation. This can be achieved by using interfaces, thus decoupling the definition of the service from the implementation. References: https://stackoverflow.com/questions/3780388/what-are-high-level-modules-and-low-level-modules-in-the-context-of-depende","title":"Low Level and High Level Modules"},{"location":"software_principles/design_patterns/#example","text":"CustomDataset is a high-level module that will apply transformation to the data and return the transformed data depending on the stage . ImageClassificationTransforms is a low-level module that will return the transformation functions. In our code, the high level module depends on the low level module such that the creation of ImageClassificationTransforms is done inside the CustomDataset constructor. This leads to high coupling . The code looks fine but if we want to change the ImageClassificationTransforms to ImageSegmentationTransforms , then we have to change the CustomDataset code in two places: Type hint of ImageClassificationTransforms to ImageSegmentationTransforms ; Change manually the ImageClassificationTransforms to ImageSegmentationTransforms in the constructor. Things soon get out of hand if we have a lot more of such dependencies, such as ObjectDetectionTransforms , ImageCaptioningTransforms , etc. One way to solve this is to use the Dependency Inversion Principle . We can create an interface Transforms that will be implemented by ImageClassificationTransforms , ImageSegmentationTransforms , etc. Then, we can pass the Transforms object to the CustomDataset constructor __init__ method. This way, the CustomDataset will depend on the Transforms interface and not on the ImageClassificationTransforms class. This way, we can change the ImageClassificationTransforms to ImageSegmentationTransforms without changing the CustomDataset code. This is called Dependency Inversion . The abstraction does not depend on details simply mean the abstract class should not hold any implementation. The implementation should be done in the concrete class. Example: In my Transforms(ABC) abstract class/interface below, I have two abstract methods get_train_transforms and get_test_transforms . These methods are not implemented in the abstract class. They are implemented in the concrete class ImageClassificationTransforms . This is the second rule in Dependency Inversion Principle . In the high level module CustomDataset , I have a constructor __init__ that takes in a Transforms abstract class/interface. Now my CustomDataset depends on the Transforms abstraction and not on the ImageClassificationTransforms class. This is the first rule in Dependency Inversion Principle . Furthermore, if you were to switch your task from image classification to image segmentation, you can simply change the ImageClassificationTransforms to ImageSegmentationTransforms without changing the CustomDataset code as you are not creating/coupled to the ImageClassificationTransforms class. class Transforms ( ABC ): \"\"\"Abstract class for transforms.\"\"\" @abstractmethod def get_train_transforms ( self ) -> Callable : \"\"\"Get train transforms.\"\"\" @abstractmethod def get_test_transforms ( self ) -> Callable : \"\"\"Get test transforms.\"\"\" class ImageClassificationTransforms ( Transforms ): \"\"\"Dummy class for image classification transforms.\"\"\" def get_train_transforms ( self ) -> Callable : \"\"\"Get train transforms.\"\"\" print ( \"Getting image classification train transforms.\" ) return lambda x : None def get_test_transforms ( self ) -> Callable : \"\"\"Get test transforms.\"\"\" print ( \"Getting image classification test transforms.\" ) return lambda x : None class ImageSegmentationTransforms ( Transforms ): \"\"\"Dummy class for image segmentation transforms.\"\"\" def get_train_transforms ( self ) -> Callable : \"\"\"Get train transforms.\"\"\" print ( \"Getting image segmentation train transforms.\" ) return lambda x : None def get_test_transforms ( self ) -> Callable : \"\"\"Get test transforms.\"\"\" print ( \"Getting image segmentation test transforms.\" ) return lambda x : None class CustomDataset : def __init__ ( self , transforms : Transforms , stage : str = \"train\" ) -> None : self . stage = stage self . transforms = transforms def apply_transforms ( self , dummy_data : Any = None ) -> Any : \"\"\"Apply transforms to dataset based on stage.\"\"\" if self . stage == \"train\" : transformed = self . transforms . get_train_transforms ()( dummy_data ) else : transformed = self . transforms . get_test_transforms ()( dummy_data ) return transformed Originally, CustomDataset creates its own dependency and it is the one controlling the dependency. Now after applying Dependency Inversion Principle , CustomDataset is no longer creating its own dependency. It is now injected with the dependency. This inverts the control of the dependency from CustomDataset to the caller of CustomDataset . This is the Dependency Inversion Principle . More concretely, in traditional sense, since class A depends on class B, then class A is the one creating the dependency. But after applying Dependency Inversion Principle , class A is no longer creating the dependency. Instead, the dependency is instantiated outside of class A at runtime and is injected into class A. This is the Dependency Inversion Principle , a form of Inversion of Control . # convert C# to Python class AppPoolWatcher : # high level module def __init__ ( self ): self . writer = EventLogWriter () def notify ( self , message ): self . writer . write ( message ) class EventLogWriter : # low level module def write ( self , message ): # Write to event log here pass High level module AppPoolWatcher depends on low level module EventLogWriter where EventLogWriter is a concrete class and not an interface/abstract class. creation of EventLogWriter is tightly coupled with AppPoolWatcher class. in other words, AppPoolWatcher creates EventLogWriter object since we are creating EventLogWriter object inside the constructor of AppPoolWatcher . Now the problem is that if we want to change the implementation of EventLogWriter then we will have to change the AppPoolWatcher class as well. This is not a good design. For example, if we want our AppPoolWatcher to write the error messages to an email instead of event log then we will have to change the self.writer to say, EmailWriter and then create an instance of EmailWriter inside the class. The problem will get even worse when we have more actions to take selectively, like sending SMS. Then we will have to have one more class whose instance will be kept inside the AppPoolWatcher. The dependency inversion principle says that we need to decouple this system in such a way that the higher level modules i.e. the AppPoolWatcher in our case will depend on a simple abstraction and will use it. This abstraction will in turn will be mapped to some concrete class which will perform the actual operation. (Next we will see how this can be done)","title":"Example"},{"location":"software_principles/design_patterns/#dependency-injection","text":"https://stackoverflow.com/questions/1314730/which-design-patterns-can-be-applied-to-the-configuration-settings-problem https://www.youtube.com/watch?v=2ejbLVkCndI http://martinfowler.com/articles/injection.html","title":"Dependency Injection"},{"location":"software_principles/design_patterns/#terminology","text":"Polymorphism:","title":"Terminology"},{"location":"software_principles/design_patterns/#further-readings","text":"https://eugeneyan.com/writing/design-patterns/ https://github.com/msaroufim/ml-design-patterns https://refactoring.guru/design-patterns https://en.wikipedia.org/wiki/Design_Patterns","title":"Further Readings"},{"location":"software_principles/python/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Imports from typing import * from dataclasses import dataclass , field from typing import List , Union , Dict import uuid Common Pitfalls References Common Gotchas in Python Is None vs == None Not much of a big difference but better to use is to compare. A small caution is below. p = [ 1 ] q = [ 1 ] False p is q # False because they are not the same actual object p == q # True because they are equivalent True None/0 is False None and \\(0\\) are False by default in python, but the catch is: 0 is False evaluates to True but; None is False evaluates to False but; None is True is also False ; (not None) is True evaluates to True since (not None) is (not False) which is True ; not None is False evaluates to True since one should evaluate not (None is False) and since (None is False) is False so not False is True . not None is False # not (None is False) True Hash hash ( \"mystring\" ), hash (( 'foo' , 'bar' )), hash ( 1 ) (7474225329682574832, -283857846903866188, 1) Dataclasses General Properties Dataclass as boiler plate as a Class For example if you want to hash your class, then you need to write __hash__ and etc which takes time, dataclass can be done easily. Why need hash? For example the dunder method __repr__ are done for you in an easy to read manner rather than the default one which points to memory address. Default Factory Intuition By design, dataclass does not take in mutable default . What does that mean? Let us see a quick example. Passing Mutable Default Directly to Dataclass The following C_dataclass has init=True by default, and setting the attributes in the dataclass simply translates to a normal class with the __init__(a: int, b: int = 0) . @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' class C_class : def __init__ ( self , a : int , b : int = 0 ): self . a = a self . b = b c_dataclass = C_dataclass ( a = 1 ) c_class = C_class ( a = 1 ) Notice that both classes have a default attribute b = 0 . This is totally fine! We can also set default attributes as a list for example in a normal class: class C_class : def __init__ ( self , a : int , b : int = 0 , c : List [ int ] = [ 1 , 2 , 3 ]): self . a = a self . b = b self . c = c However, if one tries to do that for dataclass , an error will pop out: ValueError : mutable default < class ' list '> for field c is not allowed: use default_factory @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' c : List [ int ] = [ 1 , 2 , 3 ] But why? One major reason is that to to enforce avoidance of the mutable default argument problem . The Mutable Default Argument def append_to ( element , to = []): to . append ( element ) return to Let us append some values to 2 different variables: my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) [12] [12, 42] One would expect the output to be: my_list = append_to ( 12 ) print ( my_list ) [ 12 ] my_other_list = append_to ( 42 ) print ( my_other_list ) [ 42 ] but the result is: my_list = append_to ( 12 ) print ( my_list ) [ 12 ] my_other_list = append_to ( 42 ) print ( my_other_list ) [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called (like it is in say, Ruby). This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well. We hence see the same issue when passing in mutable defaults to a class: class C_class : def __init__ ( self , a : int , b : int = 0 , c : List [ int ] = [ 1 , 2 , 3 ]): self . a = a self . b = b self . c = c c1 = C_class ( a = 1 ) c2 = C_class ( a = 1 ) c1 . c . append ( 4 ) print ( c1 . c ) c2 . c . append ( 5 ) print ( c2 . c ) assert c1 . c == [ 1 , 2 , 3 , 4 , 5 ] assert c1 . c is c2 . c [1, 2, 3, 4] [1, 2, 3, 4, 5] Thus this is deeemed dangerous and dataclass does not allow you to do it out of the bat. More info on this design can be read here Mutable Default Values . Examples (Using Default Factory to Pass Mutable Defaults) The parameters to field() are: default_factory : If provided, it must be a zero-argument callable that will be called when a default value is needed for this field. Among other purposes, this can be used to specify fields with mutable default values, as discussed below. It is an error to specify both default and default_factory. @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' c : List [ int ] = field ( default_factory = [ 1 , 2 , 3 ]) C_dataclass ( a = 1 ) Calling this dataclass will result in an error: TypeError : 'list' object is not callable because you are passing in a list of values and not what default_factory expects, a zero-argument callable . What is a zero-argument callable ? The examples below illustrates: Indeed, a list with values is not a function or class method, and hence not callable if you put the \"parenthesis\" () behind. The same error occurs. default_factory = [ 1 , 2 , 3 ] default_factory () --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/3467513020.py in <module> 1 default_factory = [ 1 , 2 , 3 ] ----> 2 default_factory ( ) TypeError : 'list' object is not callable Now if we set default_factory as a function that returns the list [1, 2, 3] , then it is indeed callable and in fact, it's zero-argument callable since this function call takes in zero arguments : default_factory = lambda : [ 1 , 2 , 3 ] default_factory () So remember, if you want pass in a default mutable object with values inside, set a lambda as such: @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' c : List [ int ] = field ( default_factory = lambda : [ 1 , 2 , 3 ]) c1 = C_dataclass ( a = 1 ) c1 . c [1, 2, 3] Generate Functions in Default Factory ( Important for Model Registry! ) This is very very useful especially if you are doing a ML project with multiple experiments, each experiments should be stored in an unique folder as a form of model registry. sample directory 1 2 3 4 5 6 7 8 \ud83d\udcc1 model_registry/ \u251c\u2500\u2500 \ud83d\udcc1 exp1_uuid4/ \u2502 \u251c\u2500\u2500 \ud83d\udcc1 weights/ \u2502 \u251c\u2500\u2500 \ud83d\udcc1 logs/ \u2502 \u2514\u2500\u2500 \ud83d\udcc4 ... \u2514\u2500\u2500 \ud83d\udcc1 exp2_uuid4/ \u251c\u2500\u2500 \ud83d\udcc1 weights/ \u2514\u2500\u2500 \ud83d\udcc1 logs/ We can simply say create a function called generate_uuid4() and pass it to field 's default_factory and it will generate a new unique id for each run of experiment. def generate_uuid4 () -> str : \"\"\"Generate a random UUID4. Returns: str: Random UUID4 \"\"\" return str ( uuid . uuid4 ()) @dataclass ( init = True , frozen = False ) class ModelRegistry : \"\"\"A class to keep track of model artifacts.\"\"\" project_name : str = \"pkd\" unique_id : str = field ( default_factory = generate_uuid4 ) exp1 = ModelRegistry ( project_name = \"pkd\" ) exp1 . unique_id 'd6f3eb61-72f7-4ed4-9637-6df42a179bdf' exp2 = ModelRegistry () exp2 . unique_id '56d5f4f8-adca-4e2c-a5f3-9207da7d1c9a' Although the unique_id should be run by the random generator uuid , users are allowed to modify it and that is something we do not want. We can therefore set init=False inside the field so it \"won't\" initialize let's see example below: # example where user can modify exp3 = ModelRegistry ( project_name = \"pkd\" , unique_id = \"123\" ) exp3 . unique_id '123' @dataclass ( init = True , frozen = False ) class ModelRegistry : \"\"\"A class to keep track of model artifacts.\"\"\" project_name : str = \"PeekingDuck\" unique_id : str = field ( init = False , default_factory = generate_uuid4 ) # example where user CANNOT modify and throws error exp3 = ModelRegistry ( project_name = \"pkd\" , unique_id = \"123\" ) exp3 . unique_id --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_20736/826619969.py in <module> 1 # example where user CANNOT modify and throws error ----> 2 exp3 = ModelRegistry ( project_name = \"pkd\" , unique_id = \"123\" ) 3 exp3 . unique_id TypeError : __init__() got an unexpected keyword argument 'unique_id' # you should not set the unique id yourself. exp3 = ModelRegistry ( project_name = \"pkd\" ) exp3 . unique_id '91679660-a805-423a-bb76-feffede8c731' References Passing default list argument to dataclasses Why can't dataclasses have mutable defaults in their class attributes declaration? Mutable Default Values This is why python dataclasses are awesome Frozen (Hashable/Mutable) Intuition When frozen is True, the dataclass is an immuatable object and immutable means you can't change the attributes or characteristics of an object after it's initialised. Note hash and immutable is a bit similar 1 . Tip Generally, making an object mutable is good since it stays constant. No surprises :) Examples Let us see the below example, both AugParamsFrozenTrue and AugParamsFrozenFalse have the same attributes, the only difference is that one is frozen and the other isn't. @dataclass ( init = True , frozen = True ) class AugParamsFrozenTrue : \"\"\"Class to keep track of the augmentation parameters.\"\"\" mean : List [ float ] = field ( default_factory = lambda : [ 0.485 , 0.456 , 0.406 ]) std : List [ float ] = field ( default_factory = lambda : [ 0.229 , 0.224 , 0.225 ]) image_size : int = 256 mixup : bool = False mixup_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"mixup_alpha\" : 1 , \"use_cuda\" : True } ) When we freeze the dataclass AugParamsFrozenTrue then we can no longer change its attribute instances . For example, we cannot re-assign the mean attribute. aug_frozen_true = AugParamsFrozenTrue () print ( id ( aug_frozen_true . mean )) aug_frozen_true . mean = [ 1 , 2 , 3 ] # same as setattr(aug_frozen_true, \"mean\", [1, 2, 3]) 1693345842112 --------------------------------------------------------------------------- FrozenInstanceError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/656634435.py in <module> 1 aug_frozen_true = AugParamsFrozenTrue ( ) 2 print ( id ( aug_frozen_true . mean ) ) ----> 3 aug_frozen_true . mean = [ 1 , 2 , 3 ] # same as setattr(aug_frozen_true, \"mean\", [1, 2, 3]) <string> in __setattr__ (self, name, value) FrozenInstanceError : cannot assign to field 'mean' However, frozen only applies to the dataclass instance itself \u2013 a frozen dataclass can contain mutable items such as lists, and a regular dataclass can contain frozen/immutable items such as tuples. This means that I can change the state of the attribute by mutating the list itself. Therefore, one must be careful that freezing a dataclass does not guarantee immutability of all its attributes. aug_frozen_true . mean [ 0 ] = 1 aug_frozen_true . mean [ 1 ] = 2 aug_frozen_true . mean [ 2 ] = 3 print ( aug_frozen_true . mean ) print ( id ( aug_frozen_true . mean )) [1, 2, 3] 1693345842112 On the other hand, if one set frozen=False , then it is just like any other class in Python, you can re-assign the attributes freely. @dataclass ( init = True , frozen = False ) class AugParamsFrozenFalse : \"\"\"Class to keep track of the augmentation parameters.\"\"\" mean : List [ float ] = field ( default_factory = lambda : [ 0.485 , 0.456 , 0.406 ]) std : List [ float ] = field ( default_factory = lambda : [ 0.229 , 0.224 , 0.225 ]) image_size : int = 256 mixup : bool = False mixup_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"mixup_alpha\" : 1 , \"use_cuda\" : True } ) aug_frozen_false = AugParamsFrozenFalse () print ( id ( aug_frozen_false . mean )) print ( aug_frozen_false . mean ) print () aug_frozen_false . mean = [ 1 , 2 , 3 ] print ( id ( aug_frozen_false . mean )) print ( aug_frozen_false . mean ) 1693347054912 [0.485, 0.456, 0.406] 1693345843648 [1, 2, 3] References What is immutability and why should I worry about it? What does frozen mean for dataclasses? Post Init 2 Notice the example below that average_marks is an attribute that can only be known after marks is known. So we can set field(init=False) and tabulate using __post_init__ . Examples (Post Init) A dunder method: Motivation: you want an \"attribute\" that is derived from your other instance attributes; Average marks of a student for example can only be known when all his \"marks\" are known; We set average_marks as an attribute BUT init is False so it is not initialized by the class as we won't know it yet + we won't pass it in the dataclass; post_init helps us to calculate and return back the average marks. Tip average_marks should be a private member cause it is not something you want the user to call and change! @dataclass ( init = True , frozen = False ) class Student : name : str student_id : int marks : List [ Union [ int , float ]] _average_marks : float = field ( init = False ) def __post_init__ ( self ) -> Union [ int , float ]: self . _average_marks = sum ( self . marks ) / len ( self . marks ) student = Student ( name = \"hongnan\" , student_id = \"123\" , marks = [ 88 , 92 , 96 ]) print ( student ) print ( student . average_marks ) Student(name='hongnan', student_id='123', marks=[88, 92, 96], average_marks=92.0) 92.0 Using Dataclass as Config File Intuition Usually we store configurations in a .yaml file or the likes and load it as dict in our script and subsequently use the dict as a way to get the config values. We will now introduce a way to store our config in a dataclass : This method has a ton of benefits: We get code completion and type hints in the editor It's easier to maintain, since you only have to change a config property name in one place Can implement version reconciliation in the from_dict method Refactoring is a breeze, since editors can auto-refactor class property names Allows you to define configurations with python code, since you can instantiate the dataclasses directly in a settings.py file, for example It's testable. Parsing from Dict Consider a config file in yaml to be the following: model_params : { model_name : resnet50d , out_features : 2 , in_channels : 3 , pretrained : false , use_meta : false } aug_params : { image_size : 224 , mean : [ 0.485 , 0.456 , 0.406 ], std : [ 0.229 , 0.224 , 0.225 ] } train_params : { epochs : 10 , use_amp : true } We can easily parse it into a python dict by: import yaml from pathlib import Path config_dict = yaml . safe_load ( Path ( \"tmp.yaml\" ) . read_text ()) to get { \"model_params\" : { \"model_name\" : \"resnet50d\" , \"out_features\" : 2 , \"in_channels\" : 3 , \"pretrained\" : False , \"use_meta\" : False , }, \"aug_params\" : { \"image_size\" : 224 , \"mean\" : [ 0.485 , 0.456 , 0.406 ], \"std\" : [ 0.229 , 0.224 , 0.225 ], }, \"train_params\" : { \"epochs\" : 10 , \"use_amp\" : True }, } Whenever we want to use the config, we can call say config_dict[model_params][\"model_name\"] . This is cumbersome if the dict is very nested; This is prone to error as you need to write the correct keys; This is difficult to refactor and hard to read. Most importantly, we can parse the config file to multiple sub-configs that is responsible for each part of the configuration, for example, we can create 3 dataclasses named ModelParams , AugParams and TrainParams to indicate what each config does. @dataclass ( init = True , frozen = False ) class ModelParams : \"\"\"Model Params.\"\"\" model_name : str pretrained : bool input_channels : int output_dimension : int use_meta : bool @classmethod def from_dict ( cls : Type [ \"ModelParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"ModelParams\" ]: return cls ( model_name = params_dict [ \"model_name\" ], pretrained = params_dict [ \"pretrained\" ], input_channels = params_dict [ \"input_channels\" ], output_dimension = params_dict [ \"output_dimension\" ], use_meta = params_dict [ \"use_meta\" ], ) @dataclass ( init = True , frozen = False ) class AugParams : \"\"\"Augmentation Params.\"\"\" mean : List [ float ] = field ( default_factory = lambda : [ 0.485 , 0.456 , 0.406 ]) std : List [ float ] = field ( default_factory = lambda : [ 0.229 , 0.224 , 0.225 ]) image_size : int = 256 @classmethod def from_dict ( cls : Type [ \"AugParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"AugParams\" ]: return cls ( mean = params_dict [ \"mean\" ], std = params_dict [ \"std\" ], image_size = params_dict [ \"image_size\" ], ) @dataclass ( init = True , frozen = False ) class TrainParams : \"\"\"Global Train Params.\"\"\" epochs : int use_amp : bool @classmethod def from_dict ( cls : Type [ \"TrainParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"TrainParams\" ]: return cls ( epochs = params_dict [ \"epochs\" ], use_amp = params_dict [ \"use_amp\" ] ) config_dict = { \"model_params\" : { \"model_name\" : \"resnet50d\" , \"out_features\" : 2 , \"in_channels\" : 3 , \"pretrained\" : False , \"use_meta\" : False , }, \"aug_params\" : { \"image_size\" : 224 , \"mean\" : [ 0.485 , 0.456 , 0.406 ], \"std\" : [ 0.229 , 0.224 , 0.225 ], }, \"train_params\" : { \"epochs\" : 10 , \"use_amp\" : True }, } train_dict = config_dict [ \"train_params\" ] train_config = TrainParams . from_dict ( params_dict = train_dict ) print ( train_config ) print ( train_config . epochs ) TrainParams(epochs=10, use_amp=True) 10 We can do the same for the rest. To Dict or Yaml You can also define method to to_dict to convert dataclass to dict. Can define variable name In yaml file, it is very difficult to define python variable inside! For example, @dataclass class FilePaths : \"\"\"Class to keep track of the files.\"\"\" train_images : Path = Path ( config . DATA_DIR , \"train\" ) I can call Path directly on the config key whereas if you put in yaml it needs a lot of tweaks. Testing We can even test our dataclass config to ensure no mistakes were made when populating the keys. # Testing import unittest class TestTrainConfig ( unittest . TestCase ): def test_example_config ( self ): raw_train_dict = { \"epochs\" : 10 , \"use_amp\" : True } expected_dict_from_dataclass = TrainParams ( epochs = 10 , use_amp = True ) self . assertEqual ( TrainParams . from_dict ( raw_train_dict ), expected_dict_from_dataclass ) TestTrainConfig () . test_example_config () References Using Dataclasses for Configuration in Python Main References https://www.youtube.com/watch?v=CvQ7e6yUtnw and his other dataclass videos.s Object-Oriented Programming (OOP) in Python 3 The main reference details a lot of practices on classes in Python. And also read Python . Creating a Class [ Line 1 ] : This defines a class Dog . [ Line 5 ] : The init method must take in self alongside with other optional arguments. The optional arguments are attributes . class Dog : # Class attribute species = \"Pomeranian\" def __init__ ( self , name , age ) -> None : print ( f \"Class Instance id: { id ( Dog ) } \" ) # instance attributes self . name = name self . age = age print ( f \"Object Instance id: { id ( self ) } \\n \" ) Terminologies A Class Instance Note that if you call Dog , you are creating a class instance . The unique id of this class instance should preserve the whole session. class_instance = Dog print ( class_instance ) print ( id ( class_instance )) <class '__main__.Dog'> 1834272044992 An Object Instance Once you instantiated the class instance with the __init__ method, then you have created an object instance . Note that every time you create a new object instance , that is a brand new object and thus the unique id of these objects are different. Let us see the example below: d1 = Dog ( name = \"ben\" , age = 2 ) d2 = Dog ( name = \"ben\" , age = 2 ) d3 = Dog ( name = \"ken\" , age = 10 ) print ( f \"id(d1)= { id ( d1 ) } , id(d2)= { id ( d2 ) } , id(d3)= { id ( d3 ) } \" ) print ( id ( d1 ) != id ( d2 )) Class Instance id: 1834272031776 Object Instance id: 1834276348784 Class Instance id: 1834272031776 Object Instance id: 1834276141520 Class Instance id: 1834272031776 Object Instance id: 1834276142336 id(d1)=1834276348784, id(d2)=1834276141520, id(d3)=1834276142336 True Notice even though d1 and d2 has exactly the same attributes, they belong to different objects. However, notice that their class instance id is the same throughout. Class Attributes A class attribute can be defined before the __init__ method. We can call them as such: class_instance . species 'Pomeranian' Object Attributes This is the more common attribute that we usually see. They are usually defined by assigning it to self : self . name = name self . age = age d1 . name , d1 . age , d1 . species ('ben', 2, 'Pomeranian') You can also call the class attribute from the object . Class/Object is Mutable by Default In this example, you change the .age attribute of the d1 object to \\(10\\) . Then your d1 's age will no longer be \\(2\\) . The key takeaway here is that custom objects are mutable by default . An object is mutable if it can be altered dynamically. For example, lists and dictionaries are mutable, but strings and tuples are immutable. print ( d1 . age ) d1 . age = 100 print ( d1 . age ) 2 100 A fancier way is to use setattr to do the same thing: print ( d1 . name ) setattr ( d1 , \"name\" , \"mary\" ) print ( d1 . name ) ben mary Object Instance Methods Dunder Methods https://www.tutorialsteacher.com/python/magic-methods-in-python Str vs Repr We define two common dunder methods __str__ and __repr__ . class Dog : # Class attribute species = \"Pomeranian\" def __init__ ( self , name , age ) -> None : print ( f \"Class Instance id: { id ( Dog ) } \" ) # instance attributes self . name = name self . age = age print ( f \"Object Instance id: { id ( self ) } \\n \" ) def __str__ ( self ) -> str : return f \"Species { self . species } is called { self . name } and is { self . age } years old!\" def __repr__ ( self ) -> str : return f \"Dog('name'= { self . name } ', 'age'= { self . age } )\" Basically if you print the str(d4) you get a human readable string talking about the class. repr(d4) also returns a string, but the difference is we usually want to return the \"class object representation\". See example below for intuition. d4 = Dog ( name = \"ken\" , age = 10 ) print ( str ( d4 )) print ( repr ( d4 )) Class Instance id: 1834272031776 Object Instance id: 1834277502304 Species Pomeranian is called ken and is 10 years old! Dog('name'=ken', 'age'=10) Instance Methods class Pizza : def __init__ ( self , size : float ): self . size = size self . class_instance_id = id ( Pizza ) def get_pizza_size ( self ): return self . size , self @classmethod def return_classmethod ( cls ): assert id ( cls ) == id ( Pizza ), \"The id of both must be the same!\" return cls Initialize an instance of Pizza object with size 10 named p1 . Note the id of this p1 is id(p1) . p1 = Pizza ( size = 10 ) print ( id ( p1 )) 1470722576792 You can see the method get_pizza_size() takes one parameter, self , which points to an instance of Pizza when the method is called (but of course instance methods can accept more than just one parameter). I returned self.size and self for this method. pizza_size , instance_of_pizza = p1 . get_pizza_size () Note that id(instance_of_pizza) is equals to id(p1) since self points directly to p1 . assert id ( instance_of_pizza ) == id ( p1 ) Through the self parameter, instance methods can freely access attributes and other methods on the same object. This gives them a lot of power when it comes to modifying an object\u2019s state. In our example, under this get_pizza_size method, we can access the attribute size of the Pizza object by calling self.size , which is equivalent to Pizza(size=10).size . This is powerful cause we can modify the object instance's state! For example: class Pizza : def __init__ ( self , size : float ): self . size = size def get_pizza_size ( self ): self . size = 100 return self . size , self Now if we call: p1 = Pizza ( size = 10 ) pizza_size , _ = p1 . get_pizza_size () print ( p1 . size ) 100 and note that the attribute of p1 is no longer 10 but 100 since we changed it using self . It is like doing: p1 = Pizza ( size = 10 ) p1 . size = 100 print ( p1 . size ) Class Methods Instead of accepting a self parameter, class methods take a cls parameter that points to the class\u2014and not the object instance\u2014when the method is called. Recall earlier the minor difference between a class instance vs an object instance . # 1. recall that class instance is: Pizza , p1 . return_classmethod () (__main__.Pizza, __main__.Pizza) # 2. now compare id! class_instance_id = p1 . class_instance_id class_method_id = id ( p1 . return_classmethod ()) class_instance_id , class_method_id (1470710370600, 1470710370600) So now one should be clear that within the object instance p1 , the Pizza class id must be the same as the id of cls . Example Usage class Pizza : def __init__ ( self , ingredients : List [ str ]): self . ingredients = ingredients def __repr__ ( self ): return f 'Pizza( { self . ingredients !r} )' @classmethod def margherita ( cls ): return cls ([ 'mozzarella' , 'tomatoes' ]) @classmethod def prosciutto ( cls ): return cls ([ 'mozzarella' , 'tomatoes' , 'ham' ]) First, if we want to create two object instances named margherita and prosciutto that are created by: margherita = Pizza ([ 'mozzarella' , 'tomatoes' ]) prosciutto = Pizza ([ 'mozzarella' , 'tomatoes' , 'ham' ]) A neater way is to use classmethod . Pizza . margherita (), Pizza . prosciutto () (Pizza(['mozzarella', 'tomatoes']), Pizza(['mozzarella', 'tomatoes', 'ham'])) Static Method Note static method has no self or cls , so it can neither access to the class instance nor the object instance . Then why is it useful sometimes since it is as good as I were to define the static method outside the class as a function. One reason can be understood as follows, albeit a bit of a forced example: class Pizza : def __init__ ( self , radius , ingredients ): self . radius = radius self . ingredients = ingredients def __repr__ ( self ): return ( f 'Pizza( { self . radius !r} , ' f ' { self . ingredients !r} )' ) def calculate_pizza_area ( self ): return self . calculate_circle_area ( self . radius ) @staticmethod def calculate_circle_area ( r ): return r ** 2 * math . pi Maintain your class design, even though calculate_circle_area is independent of the class/object state, one can still argue that calculating circle area is still relevant to the whole architecture of the Pizza class since we have a method to calculate pizza area. Ease of testing, one can just test the static method without initializing the object instance itself. Abstract Methods This provides us a template or blueprint in a sense. from abc import ABCMeta , abstractmethod class AbstractPizza ( metaclass = ABCMeta ): def __init__ ( self , radius : float ): self . radius = radius @abstractmethod def calculate_pizza_area ( self ): raise NotImplementedError ( \"This method needs to be implemented\" ) AbstractPizza ( radius = 10 ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/1152115237.py in <module> ----> 1 AbstractPizza ( radius = 10 ) TypeError : Can't instantiate abstract class AbstractPizza with abstract method calculate_pizza_area class Pizza ( AbstractPizza ): def __init__ ( self , radius : float ): self . radius = radius Pizza ( radius = 10 ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/4082189809.py in <module> ----> 1 Pizza ( radius = 10 ) TypeError : Can't instantiate abstract class Pizza with abstract method calculate_pizza_area class Pizza ( AbstractPizza ): def __init__ ( self , radius : float ): self . radius = radius def calculate_pizza_area ( self ): return self . radius ** 2 * math . pi Pizza ( radius = 10 ) <__main__.Pizza at 0x18a435d3a00> Inheritance Inheritance is the process by which one class takes on the attributes and methods of another. Newly formed classes are called child classes, and the classes that child classes are derived from are called parent classes. Intuition Child classes can override or extend the attributes and methods of parent classes. In other words, child classes inherit all of the parent\u2019s attributes and methods but can also specify attributes and methods that are unique to themselves. Although the analogy isn\u2019t perfect, you can think of object inheritance sort of like genetic inheritance. You may have inherited your hair color from your mother. It\u2019s an attribute you were born with. Let\u2019s say you decide to color your hair purple. Assuming your mother doesn\u2019t have purple hair, you\u2019ve just overridden the hair color attribute that you inherited from your mom. You also inherit, in a sense, your language from your parents. If your parents speak English, then you\u2019ll also speak English. Now imagine you decide to learn a second language, like German. In this case you\u2019ve extended your attributes because you\u2019ve added an attribute that your parents don\u2019t have. How to use Inheritance A base parent class Employee with 2 attributes: employee_id employee_name A child class that inherits the parent class called SalaryEmployee ; note the __init__ of this child class takes in 3 attributes: employee_id : from parent employee_name : from parent monthly_salary : from child It is also worth noting we used super().__init__ to take in the parent class's attributes which is the same as calling Employee.__init__(employee_id, employee_name) , both of which initializes the parents class in the child class. Intuitively, you can think of that the child class has all the attributes and methods that the parent class has. class Employee : def __init__ ( self , employee_id : int , employee_name : str ) -> None : self . employee_id = employee_id self . employee_name = employee_name class SalaryEmployee ( Employee ): def __init__ ( self , employee_id : int , employee_name : str , monthly_salary : Union [ int , float ], ) -> None : super () . __init__ ( employee_id , employee_name ) self . monthly_salary = monthly_salary def calculate_annual_salary ( self ) -> Union [ int , float ]: \"\"\"Calculate annual salary. Returns: Union[int, float]: Monthly salary * 12 \"\"\" return self . monthly_salary * 12 class CommissionEmployee ( SalaryEmployee ): def __init__ ( self , employee_id : int , employee_name : str , monthly_salary : Union [ int , float ], commission : Union [ int , float ], ) -> None : super () . __init__ ( employee_id , employee_name , monthly_salary ) self . commission = commission def calculate_annual_salary ( self ) -> Union [ int , float ]: \"\"\"Calculate annual salary + commission. Returns: Union[int, float]: Monthly salary * 12 + commission \"\"\" fixed_annual_salary = super () . calculate_annual_salary () return fixed_annual_salary + self . commission Simple example of inheritance. ken_salary = SalaryEmployee ( employee_id = 123 , employee_name = \"ken\" , monthly_salary = 5000 ) ken_salary . calculate_annual_salary () 60000 Slightly more complicated logic where one used super() in line 44. Why don't we just use fixed_annual_salary = self.monthly_salary * 12 to get the fixed year wage like how we did in SalaryEmployee . The problem with accessing the property directly is that if the implementation of SalaryEmployee.calculate_annual_salary() changes, then you\u2019ll have to also change the implementation of CommissionEmployee.calculate_annual_salary() . It\u2019s better to rely on the already implemented method in the base class and extend the functionality as needed. Calling super() in this child class will invoke the method in the parent class. So if calculate_annual_salary() in the parent class becomes something like monthly_salary * 13 , then you don't need to worry about changing the logic again in the child CommissionEmployee when calculating the total annual salary. ken_salary_and_commision = CommissionEmployee ( employee_id = 123 , employee_name = \"ken\" , monthly_salary = 5000 , commission = 10000 ) ken_salary_and_commision . calculate_annual_salary () 70000 Super Init and Inheritance Diamond https://stackoverflow.com/questions/29173299/super-init-vs-parent-init https://thepythonguru.com/python-classes-and-interfaces/ Main References Main Reference for Python Classes Designs Overall well rounder for many concepts. So if one has to choose one, this will be the one to read first or together with other references. Main Reference for Inheritance and Composition Mentions ABC class as well. Basic OOP Guide Object Instance Methods Main Reference for Python Classes Designs Python's Instance, Class, and Static Methods Demystified The definitive guide on how to use static, class or abstract methods in Python : Mostly Python 2 so slightly outdated but did mention about Python 3 inside. What is the advantage of using static methods? Class Methods in Python Args and Kwargs https://stackoverflow.com/questions/9872824/calling-a-python-function-with-args-kwargs-and-optional-default-arguments Property Decorator https://docs.python.org/3/library/functions.html#property https://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work-in-python class C : def __init__ ( self ): self . _x = None def getx ( self ): return self . _x def setx ( self , value ): self . _x = value def delx ( self ): del self . _x x = property ( getx , setx , delx , \"I'm the 'x' property.\" ) c = C () c . x # get attribute x which is None c . x = 10 # set attribute x to 10 c . x # get attribute x which is 10 10 class Parrot : def __init__ ( self ): self . _voltage = 100000 @property def voltage ( self ): \"\"\"Get the current voltage.\"\"\" return self . _voltage p = Parrot () 2 practical use with just using property : To make a class attribute read-only. Here _voltage is a private attribute and you don't want to allow the user to change it. So you can use property to make it read-only. p . voltage # this works! p . _voltage # this works too! 100000 p . voltage = 100 # this does not work because voltage is a property with read only access! No overwriting... # p._voltage = 100 # this works so but should not be used! --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In [16], line 1 ----> 1 p.voltage = 100 AttributeError : can't set attribute second use case is less keystrokes, like the following, instead of calling bbox.coordinates() you can just call bbox.coordinates . import numpy as np import torch from typing import Union class Box : def __init__ ( self , inputs : Union [ torch . Tensor , np . ndarray ]): self . inputs = inputs @property def coordinates ( self ): \"\"\"Get the current voltage.\"\"\" if isinstance ( self . inputs , torch . Tensor ): return self . inputs . clone () return self . inputs . copy () bbox = Box ( inputs = np . asarray ([ 1 , 2 , 3 ])) bbox . coordinates array([1, 2, 3]) Furthermore, now your coordinates cannot be setattr by the user here. bbox . coordinates = 10 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In [7], line 1 ----> 1 bbox.coordinates = 10 AttributeError : can't set attribute One more very good answer here from https://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work-in-python just very recently. In the following, I have given an example to clarify @property Consider a class named Student with two variables: name and class_number and you want class_number to be in the range of 1 to 5 . Now I will explain two wrong solutions and finally the correct one: The code below is wrong because it doesn't validate the class_number (to be in the range 1 to 5) class Student: def __init__(self, name, class_number): self.name = name self.class_number = class_number Despite validation, this solution is also wrong : def validate_class_number(number): if 1 <= number <= 5: return number else: raise Exception(\"class number should be in the range of 1 to 5\") class Student: def __init__(self, name, class_number): self.name = name self.class_number = validate_class_number(class_number) Because class_number validation is checked only at the time of making a class instance and it is not checked after that (it is possible to change class_number with a number outside of the range 1 to 5): student1 = Student(\"masoud\",5) student1.class_number = 7 The correct solution is: class Student: def __init__(self, name, class_number): self.name = name self.class_number = class_number @property def class_number(self): return self._class_number @class_number.setter def class_number(self, class_number): if not (1 <= class_number <= 5): raise Exception(\"class number should be in the range of 1 to 5\") self._class_number = class_number Note that with setter then you are able to assign attributes again (not read only), but it will check the validation first. Type Hinting Why using typevar is better than Any in the below case. from typing import TypeVar , Generic , List T = TypeVar ( 'T' ) class Stack ( Generic [ T ]): def __init__ ( self ) -> None : # Create an empty list with items of type T self . items : List [ T ] = [] def push ( self , item : T ) -> None : self . items . append ( item ) def pop ( self ) -> T : return self . items . pop () def empty ( self ) -> bool : return not self . items my_str = \"abcdefg\" def reverse_string_using_stack ( stack : Stack [ str ], string : str ) -> str : reversed_string = \"\" for s in string : stack . push ( s ) while not stack . empty (): reversed_string += stack . pop () return reversed_string stack = Stack () reverse_string_using_stack ( stack , my_str ) 'gfedcba' Notice in argument I defined stack: Stack[str] indicating that the stack is a stack of strings. But in the function body, I used stack.push(1) which is not a string. So the type checker will complain if I use mypy for example. from typing import Any , List class Stack : def __init__ ( self ) -> None : # Create an empty list with items of type T self . items : List [ Any ] = [] def push ( self , item : T ) -> None : self . items . append ( item ) def pop ( self ) -> T : return self . items . pop () def empty ( self ) -> bool : return not self . items my_str = \"abcdefg\" def reverse_string_using_stack ( stack : Stack [ str ], string : str ) -> str : reversed_string = \"\" for s in string : stack . push ( s ) while not stack . empty (): reversed_string += stack . pop () return reversed_string stack = Stack () reverse_string_using_stack ( stack , my_str ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In [9], line 3 1 my_str = \"abcdefg\" ----> 3 def reverse_string_using_stack(stack: Stack[str], string: str) -> str: 5 reversed_string = \"\" 7 for s in string: TypeError : 'type' object is not subscriptable If we just use List[Any] above, then we cannot define a type for Stack object just how we do for Tuple[int, int, int] . So in this case it is better to do the first method. See https://mypy.readthedocs.io/en/stable/generics.html for more info.and https://codereview.stackexchange.com/questions/256319/singly-linked-list-python-code/256351?noredirect=1#comment556249_256351 Generators and Iterators https://www.programiz.com/python-programming/iterator https://anandology.com/python-practice-book/iterators.html Real python? Immutable vs Hash . \u21a9 Post Init Example \u21a9","title":"Python"},{"location":"software_principles/python/#imports","text":"from typing import * from dataclasses import dataclass , field from typing import List , Union , Dict import uuid","title":"Imports"},{"location":"software_principles/python/#common-pitfalls","text":"","title":"Common Pitfalls"},{"location":"software_principles/python/#references","text":"Common Gotchas in Python","title":"References"},{"location":"software_principles/python/#is-none-vs-none","text":"Not much of a big difference but better to use is to compare. A small caution is below. p = [ 1 ] q = [ 1 ] False p is q # False because they are not the same actual object p == q # True because they are equivalent True","title":"Is None vs == None"},{"location":"software_principles/python/#none0-is-false","text":"None and \\(0\\) are False by default in python, but the catch is: 0 is False evaluates to True but; None is False evaluates to False but; None is True is also False ; (not None) is True evaluates to True since (not None) is (not False) which is True ; not None is False evaluates to True since one should evaluate not (None is False) and since (None is False) is False so not False is True . not None is False # not (None is False) True","title":"None/0 is False"},{"location":"software_principles/python/#hash","text":"hash ( \"mystring\" ), hash (( 'foo' , 'bar' )), hash ( 1 ) (7474225329682574832, -283857846903866188, 1)","title":"Hash"},{"location":"software_principles/python/#dataclasses","text":"","title":"Dataclasses"},{"location":"software_principles/python/#general-properties","text":"Dataclass as boiler plate as a Class For example if you want to hash your class, then you need to write __hash__ and etc which takes time, dataclass can be done easily. Why need hash? For example the dunder method __repr__ are done for you in an easy to read manner rather than the default one which points to memory address.","title":"General Properties"},{"location":"software_principles/python/#default-factory","text":"","title":"Default Factory"},{"location":"software_principles/python/#intuition","text":"By design, dataclass does not take in mutable default . What does that mean? Let us see a quick example.","title":"Intuition"},{"location":"software_principles/python/#passing-mutable-default-directly-to-dataclass","text":"The following C_dataclass has init=True by default, and setting the attributes in the dataclass simply translates to a normal class with the __init__(a: int, b: int = 0) . @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' class C_class : def __init__ ( self , a : int , b : int = 0 ): self . a = a self . b = b c_dataclass = C_dataclass ( a = 1 ) c_class = C_class ( a = 1 ) Notice that both classes have a default attribute b = 0 . This is totally fine! We can also set default attributes as a list for example in a normal class: class C_class : def __init__ ( self , a : int , b : int = 0 , c : List [ int ] = [ 1 , 2 , 3 ]): self . a = a self . b = b self . c = c However, if one tries to do that for dataclass , an error will pop out: ValueError : mutable default < class ' list '> for field c is not allowed: use default_factory @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' c : List [ int ] = [ 1 , 2 , 3 ] But why? One major reason is that to to enforce avoidance of the mutable default argument problem .","title":"Passing Mutable Default Directly to Dataclass"},{"location":"software_principles/python/#the-mutable-default-argument","text":"def append_to ( element , to = []): to . append ( element ) return to Let us append some values to 2 different variables: my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) [12] [12, 42] One would expect the output to be: my_list = append_to ( 12 ) print ( my_list ) [ 12 ] my_other_list = append_to ( 42 ) print ( my_other_list ) [ 42 ] but the result is: my_list = append_to ( 12 ) print ( my_list ) [ 12 ] my_other_list = append_to ( 42 ) print ( my_other_list ) [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called (like it is in say, Ruby). This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well. We hence see the same issue when passing in mutable defaults to a class: class C_class : def __init__ ( self , a : int , b : int = 0 , c : List [ int ] = [ 1 , 2 , 3 ]): self . a = a self . b = b self . c = c c1 = C_class ( a = 1 ) c2 = C_class ( a = 1 ) c1 . c . append ( 4 ) print ( c1 . c ) c2 . c . append ( 5 ) print ( c2 . c ) assert c1 . c == [ 1 , 2 , 3 , 4 , 5 ] assert c1 . c is c2 . c [1, 2, 3, 4] [1, 2, 3, 4, 5] Thus this is deeemed dangerous and dataclass does not allow you to do it out of the bat. More info on this design can be read here Mutable Default Values .","title":"The Mutable Default Argument"},{"location":"software_principles/python/#examples-using-default-factory-to-pass-mutable-defaults","text":"The parameters to field() are: default_factory : If provided, it must be a zero-argument callable that will be called when a default value is needed for this field. Among other purposes, this can be used to specify fields with mutable default values, as discussed below. It is an error to specify both default and default_factory. @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' c : List [ int ] = field ( default_factory = [ 1 , 2 , 3 ]) C_dataclass ( a = 1 ) Calling this dataclass will result in an error: TypeError : 'list' object is not callable because you are passing in a list of values and not what default_factory expects, a zero-argument callable . What is a zero-argument callable ? The examples below illustrates: Indeed, a list with values is not a function or class method, and hence not callable if you put the \"parenthesis\" () behind. The same error occurs. default_factory = [ 1 , 2 , 3 ] default_factory () --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/3467513020.py in <module> 1 default_factory = [ 1 , 2 , 3 ] ----> 2 default_factory ( ) TypeError : 'list' object is not callable Now if we set default_factory as a function that returns the list [1, 2, 3] , then it is indeed callable and in fact, it's zero-argument callable since this function call takes in zero arguments : default_factory = lambda : [ 1 , 2 , 3 ] default_factory () So remember, if you want pass in a default mutable object with values inside, set a lambda as such: @dataclass ( init = True ) class C_dataclass : a : int # 'a' has no default value b : int = 0 # assign a default value for 'b' c : List [ int ] = field ( default_factory = lambda : [ 1 , 2 , 3 ]) c1 = C_dataclass ( a = 1 ) c1 . c [1, 2, 3]","title":"Examples (Using Default Factory to Pass Mutable Defaults)"},{"location":"software_principles/python/#generate-functions-in-default-factory-important-for-model-registry","text":"This is very very useful especially if you are doing a ML project with multiple experiments, each experiments should be stored in an unique folder as a form of model registry. sample directory 1 2 3 4 5 6 7 8 \ud83d\udcc1 model_registry/ \u251c\u2500\u2500 \ud83d\udcc1 exp1_uuid4/ \u2502 \u251c\u2500\u2500 \ud83d\udcc1 weights/ \u2502 \u251c\u2500\u2500 \ud83d\udcc1 logs/ \u2502 \u2514\u2500\u2500 \ud83d\udcc4 ... \u2514\u2500\u2500 \ud83d\udcc1 exp2_uuid4/ \u251c\u2500\u2500 \ud83d\udcc1 weights/ \u2514\u2500\u2500 \ud83d\udcc1 logs/ We can simply say create a function called generate_uuid4() and pass it to field 's default_factory and it will generate a new unique id for each run of experiment. def generate_uuid4 () -> str : \"\"\"Generate a random UUID4. Returns: str: Random UUID4 \"\"\" return str ( uuid . uuid4 ()) @dataclass ( init = True , frozen = False ) class ModelRegistry : \"\"\"A class to keep track of model artifacts.\"\"\" project_name : str = \"pkd\" unique_id : str = field ( default_factory = generate_uuid4 ) exp1 = ModelRegistry ( project_name = \"pkd\" ) exp1 . unique_id 'd6f3eb61-72f7-4ed4-9637-6df42a179bdf' exp2 = ModelRegistry () exp2 . unique_id '56d5f4f8-adca-4e2c-a5f3-9207da7d1c9a' Although the unique_id should be run by the random generator uuid , users are allowed to modify it and that is something we do not want. We can therefore set init=False inside the field so it \"won't\" initialize let's see example below: # example where user can modify exp3 = ModelRegistry ( project_name = \"pkd\" , unique_id = \"123\" ) exp3 . unique_id '123' @dataclass ( init = True , frozen = False ) class ModelRegistry : \"\"\"A class to keep track of model artifacts.\"\"\" project_name : str = \"PeekingDuck\" unique_id : str = field ( init = False , default_factory = generate_uuid4 ) # example where user CANNOT modify and throws error exp3 = ModelRegistry ( project_name = \"pkd\" , unique_id = \"123\" ) exp3 . unique_id --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_20736/826619969.py in <module> 1 # example where user CANNOT modify and throws error ----> 2 exp3 = ModelRegistry ( project_name = \"pkd\" , unique_id = \"123\" ) 3 exp3 . unique_id TypeError : __init__() got an unexpected keyword argument 'unique_id' # you should not set the unique id yourself. exp3 = ModelRegistry ( project_name = \"pkd\" ) exp3 . unique_id '91679660-a805-423a-bb76-feffede8c731'","title":"Generate Functions in Default Factory (Important for Model Registry!)"},{"location":"software_principles/python/#references_1","text":"Passing default list argument to dataclasses Why can't dataclasses have mutable defaults in their class attributes declaration? Mutable Default Values This is why python dataclasses are awesome","title":"References"},{"location":"software_principles/python/#frozen-hashablemutable","text":"","title":"Frozen (Hashable/Mutable)"},{"location":"software_principles/python/#intuition_1","text":"When frozen is True, the dataclass is an immuatable object and immutable means you can't change the attributes or characteristics of an object after it's initialised. Note hash and immutable is a bit similar 1 . Tip Generally, making an object mutable is good since it stays constant. No surprises :)","title":"Intuition"},{"location":"software_principles/python/#examples","text":"Let us see the below example, both AugParamsFrozenTrue and AugParamsFrozenFalse have the same attributes, the only difference is that one is frozen and the other isn't. @dataclass ( init = True , frozen = True ) class AugParamsFrozenTrue : \"\"\"Class to keep track of the augmentation parameters.\"\"\" mean : List [ float ] = field ( default_factory = lambda : [ 0.485 , 0.456 , 0.406 ]) std : List [ float ] = field ( default_factory = lambda : [ 0.229 , 0.224 , 0.225 ]) image_size : int = 256 mixup : bool = False mixup_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"mixup_alpha\" : 1 , \"use_cuda\" : True } ) When we freeze the dataclass AugParamsFrozenTrue then we can no longer change its attribute instances . For example, we cannot re-assign the mean attribute. aug_frozen_true = AugParamsFrozenTrue () print ( id ( aug_frozen_true . mean )) aug_frozen_true . mean = [ 1 , 2 , 3 ] # same as setattr(aug_frozen_true, \"mean\", [1, 2, 3]) 1693345842112 --------------------------------------------------------------------------- FrozenInstanceError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/656634435.py in <module> 1 aug_frozen_true = AugParamsFrozenTrue ( ) 2 print ( id ( aug_frozen_true . mean ) ) ----> 3 aug_frozen_true . mean = [ 1 , 2 , 3 ] # same as setattr(aug_frozen_true, \"mean\", [1, 2, 3]) <string> in __setattr__ (self, name, value) FrozenInstanceError : cannot assign to field 'mean' However, frozen only applies to the dataclass instance itself \u2013 a frozen dataclass can contain mutable items such as lists, and a regular dataclass can contain frozen/immutable items such as tuples. This means that I can change the state of the attribute by mutating the list itself. Therefore, one must be careful that freezing a dataclass does not guarantee immutability of all its attributes. aug_frozen_true . mean [ 0 ] = 1 aug_frozen_true . mean [ 1 ] = 2 aug_frozen_true . mean [ 2 ] = 3 print ( aug_frozen_true . mean ) print ( id ( aug_frozen_true . mean )) [1, 2, 3] 1693345842112 On the other hand, if one set frozen=False , then it is just like any other class in Python, you can re-assign the attributes freely. @dataclass ( init = True , frozen = False ) class AugParamsFrozenFalse : \"\"\"Class to keep track of the augmentation parameters.\"\"\" mean : List [ float ] = field ( default_factory = lambda : [ 0.485 , 0.456 , 0.406 ]) std : List [ float ] = field ( default_factory = lambda : [ 0.229 , 0.224 , 0.225 ]) image_size : int = 256 mixup : bool = False mixup_params : Dict [ str , Any ] = field ( default_factory = lambda : { \"mixup_alpha\" : 1 , \"use_cuda\" : True } ) aug_frozen_false = AugParamsFrozenFalse () print ( id ( aug_frozen_false . mean )) print ( aug_frozen_false . mean ) print () aug_frozen_false . mean = [ 1 , 2 , 3 ] print ( id ( aug_frozen_false . mean )) print ( aug_frozen_false . mean ) 1693347054912 [0.485, 0.456, 0.406] 1693345843648 [1, 2, 3]","title":"Examples"},{"location":"software_principles/python/#references_2","text":"What is immutability and why should I worry about it? What does frozen mean for dataclasses?","title":"References"},{"location":"software_principles/python/#post-init-2","text":"Notice the example below that average_marks is an attribute that can only be known after marks is known. So we can set field(init=False) and tabulate using __post_init__ .","title":"Post Init 2"},{"location":"software_principles/python/#examples-post-init","text":"A dunder method: Motivation: you want an \"attribute\" that is derived from your other instance attributes; Average marks of a student for example can only be known when all his \"marks\" are known; We set average_marks as an attribute BUT init is False so it is not initialized by the class as we won't know it yet + we won't pass it in the dataclass; post_init helps us to calculate and return back the average marks. Tip average_marks should be a private member cause it is not something you want the user to call and change! @dataclass ( init = True , frozen = False ) class Student : name : str student_id : int marks : List [ Union [ int , float ]] _average_marks : float = field ( init = False ) def __post_init__ ( self ) -> Union [ int , float ]: self . _average_marks = sum ( self . marks ) / len ( self . marks ) student = Student ( name = \"hongnan\" , student_id = \"123\" , marks = [ 88 , 92 , 96 ]) print ( student ) print ( student . average_marks ) Student(name='hongnan', student_id='123', marks=[88, 92, 96], average_marks=92.0) 92.0","title":"Examples (Post Init)"},{"location":"software_principles/python/#using-dataclass-as-config-file","text":"","title":"Using Dataclass as Config File"},{"location":"software_principles/python/#intuition_2","text":"Usually we store configurations in a .yaml file or the likes and load it as dict in our script and subsequently use the dict as a way to get the config values. We will now introduce a way to store our config in a dataclass : This method has a ton of benefits: We get code completion and type hints in the editor It's easier to maintain, since you only have to change a config property name in one place Can implement version reconciliation in the from_dict method Refactoring is a breeze, since editors can auto-refactor class property names Allows you to define configurations with python code, since you can instantiate the dataclasses directly in a settings.py file, for example It's testable.","title":"Intuition"},{"location":"software_principles/python/#parsing-from-dict","text":"Consider a config file in yaml to be the following: model_params : { model_name : resnet50d , out_features : 2 , in_channels : 3 , pretrained : false , use_meta : false } aug_params : { image_size : 224 , mean : [ 0.485 , 0.456 , 0.406 ], std : [ 0.229 , 0.224 , 0.225 ] } train_params : { epochs : 10 , use_amp : true } We can easily parse it into a python dict by: import yaml from pathlib import Path config_dict = yaml . safe_load ( Path ( \"tmp.yaml\" ) . read_text ()) to get { \"model_params\" : { \"model_name\" : \"resnet50d\" , \"out_features\" : 2 , \"in_channels\" : 3 , \"pretrained\" : False , \"use_meta\" : False , }, \"aug_params\" : { \"image_size\" : 224 , \"mean\" : [ 0.485 , 0.456 , 0.406 ], \"std\" : [ 0.229 , 0.224 , 0.225 ], }, \"train_params\" : { \"epochs\" : 10 , \"use_amp\" : True }, } Whenever we want to use the config, we can call say config_dict[model_params][\"model_name\"] . This is cumbersome if the dict is very nested; This is prone to error as you need to write the correct keys; This is difficult to refactor and hard to read. Most importantly, we can parse the config file to multiple sub-configs that is responsible for each part of the configuration, for example, we can create 3 dataclasses named ModelParams , AugParams and TrainParams to indicate what each config does. @dataclass ( init = True , frozen = False ) class ModelParams : \"\"\"Model Params.\"\"\" model_name : str pretrained : bool input_channels : int output_dimension : int use_meta : bool @classmethod def from_dict ( cls : Type [ \"ModelParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"ModelParams\" ]: return cls ( model_name = params_dict [ \"model_name\" ], pretrained = params_dict [ \"pretrained\" ], input_channels = params_dict [ \"input_channels\" ], output_dimension = params_dict [ \"output_dimension\" ], use_meta = params_dict [ \"use_meta\" ], ) @dataclass ( init = True , frozen = False ) class AugParams : \"\"\"Augmentation Params.\"\"\" mean : List [ float ] = field ( default_factory = lambda : [ 0.485 , 0.456 , 0.406 ]) std : List [ float ] = field ( default_factory = lambda : [ 0.229 , 0.224 , 0.225 ]) image_size : int = 256 @classmethod def from_dict ( cls : Type [ \"AugParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"AugParams\" ]: return cls ( mean = params_dict [ \"mean\" ], std = params_dict [ \"std\" ], image_size = params_dict [ \"image_size\" ], ) @dataclass ( init = True , frozen = False ) class TrainParams : \"\"\"Global Train Params.\"\"\" epochs : int use_amp : bool @classmethod def from_dict ( cls : Type [ \"TrainParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"TrainParams\" ]: return cls ( epochs = params_dict [ \"epochs\" ], use_amp = params_dict [ \"use_amp\" ] ) config_dict = { \"model_params\" : { \"model_name\" : \"resnet50d\" , \"out_features\" : 2 , \"in_channels\" : 3 , \"pretrained\" : False , \"use_meta\" : False , }, \"aug_params\" : { \"image_size\" : 224 , \"mean\" : [ 0.485 , 0.456 , 0.406 ], \"std\" : [ 0.229 , 0.224 , 0.225 ], }, \"train_params\" : { \"epochs\" : 10 , \"use_amp\" : True }, } train_dict = config_dict [ \"train_params\" ] train_config = TrainParams . from_dict ( params_dict = train_dict ) print ( train_config ) print ( train_config . epochs ) TrainParams(epochs=10, use_amp=True) 10 We can do the same for the rest.","title":"Parsing from Dict"},{"location":"software_principles/python/#to-dict-or-yaml","text":"You can also define method to to_dict to convert dataclass to dict.","title":"To Dict or Yaml"},{"location":"software_principles/python/#can-define-variable-name","text":"In yaml file, it is very difficult to define python variable inside! For example, @dataclass class FilePaths : \"\"\"Class to keep track of the files.\"\"\" train_images : Path = Path ( config . DATA_DIR , \"train\" ) I can call Path directly on the config key whereas if you put in yaml it needs a lot of tweaks.","title":"Can define variable name"},{"location":"software_principles/python/#testing","text":"We can even test our dataclass config to ensure no mistakes were made when populating the keys. # Testing import unittest class TestTrainConfig ( unittest . TestCase ): def test_example_config ( self ): raw_train_dict = { \"epochs\" : 10 , \"use_amp\" : True } expected_dict_from_dataclass = TrainParams ( epochs = 10 , use_amp = True ) self . assertEqual ( TrainParams . from_dict ( raw_train_dict ), expected_dict_from_dataclass ) TestTrainConfig () . test_example_config ()","title":"Testing"},{"location":"software_principles/python/#references_3","text":"Using Dataclasses for Configuration in Python","title":"References"},{"location":"software_principles/python/#main-references","text":"https://www.youtube.com/watch?v=CvQ7e6yUtnw and his other dataclass videos.s","title":"Main References"},{"location":"software_principles/python/#object-oriented-programming-oop-in-python-3","text":"The main reference details a lot of practices on classes in Python. And also read Python .","title":"Object-Oriented Programming (OOP) in Python 3"},{"location":"software_principles/python/#creating-a-class","text":"[ Line 1 ] : This defines a class Dog . [ Line 5 ] : The init method must take in self alongside with other optional arguments. The optional arguments are attributes . class Dog : # Class attribute species = \"Pomeranian\" def __init__ ( self , name , age ) -> None : print ( f \"Class Instance id: { id ( Dog ) } \" ) # instance attributes self . name = name self . age = age print ( f \"Object Instance id: { id ( self ) } \\n \" )","title":"Creating a Class"},{"location":"software_principles/python/#terminologies","text":"","title":"Terminologies"},{"location":"software_principles/python/#a-class-instance","text":"Note that if you call Dog , you are creating a class instance . The unique id of this class instance should preserve the whole session. class_instance = Dog print ( class_instance ) print ( id ( class_instance )) <class '__main__.Dog'> 1834272044992","title":"A Class Instance"},{"location":"software_principles/python/#an-object-instance","text":"Once you instantiated the class instance with the __init__ method, then you have created an object instance . Note that every time you create a new object instance , that is a brand new object and thus the unique id of these objects are different. Let us see the example below: d1 = Dog ( name = \"ben\" , age = 2 ) d2 = Dog ( name = \"ben\" , age = 2 ) d3 = Dog ( name = \"ken\" , age = 10 ) print ( f \"id(d1)= { id ( d1 ) } , id(d2)= { id ( d2 ) } , id(d3)= { id ( d3 ) } \" ) print ( id ( d1 ) != id ( d2 )) Class Instance id: 1834272031776 Object Instance id: 1834276348784 Class Instance id: 1834272031776 Object Instance id: 1834276141520 Class Instance id: 1834272031776 Object Instance id: 1834276142336 id(d1)=1834276348784, id(d2)=1834276141520, id(d3)=1834276142336 True Notice even though d1 and d2 has exactly the same attributes, they belong to different objects. However, notice that their class instance id is the same throughout.","title":"An Object Instance"},{"location":"software_principles/python/#class-attributes","text":"A class attribute can be defined before the __init__ method. We can call them as such: class_instance . species 'Pomeranian'","title":"Class Attributes"},{"location":"software_principles/python/#object-attributes","text":"This is the more common attribute that we usually see. They are usually defined by assigning it to self : self . name = name self . age = age d1 . name , d1 . age , d1 . species ('ben', 2, 'Pomeranian') You can also call the class attribute from the object .","title":"Object Attributes"},{"location":"software_principles/python/#classobject-is-mutable-by-default","text":"In this example, you change the .age attribute of the d1 object to \\(10\\) . Then your d1 's age will no longer be \\(2\\) . The key takeaway here is that custom objects are mutable by default . An object is mutable if it can be altered dynamically. For example, lists and dictionaries are mutable, but strings and tuples are immutable. print ( d1 . age ) d1 . age = 100 print ( d1 . age ) 2 100 A fancier way is to use setattr to do the same thing: print ( d1 . name ) setattr ( d1 , \"name\" , \"mary\" ) print ( d1 . name ) ben mary","title":"Class/Object is Mutable by Default"},{"location":"software_principles/python/#object-instance-methods","text":"","title":"Object Instance Methods"},{"location":"software_principles/python/#dunder-methods","text":"https://www.tutorialsteacher.com/python/magic-methods-in-python","title":"Dunder Methods"},{"location":"software_principles/python/#str-vs-repr","text":"We define two common dunder methods __str__ and __repr__ . class Dog : # Class attribute species = \"Pomeranian\" def __init__ ( self , name , age ) -> None : print ( f \"Class Instance id: { id ( Dog ) } \" ) # instance attributes self . name = name self . age = age print ( f \"Object Instance id: { id ( self ) } \\n \" ) def __str__ ( self ) -> str : return f \"Species { self . species } is called { self . name } and is { self . age } years old!\" def __repr__ ( self ) -> str : return f \"Dog('name'= { self . name } ', 'age'= { self . age } )\" Basically if you print the str(d4) you get a human readable string talking about the class. repr(d4) also returns a string, but the difference is we usually want to return the \"class object representation\". See example below for intuition. d4 = Dog ( name = \"ken\" , age = 10 ) print ( str ( d4 )) print ( repr ( d4 )) Class Instance id: 1834272031776 Object Instance id: 1834277502304 Species Pomeranian is called ken and is 10 years old! Dog('name'=ken', 'age'=10)","title":"Str vs Repr"},{"location":"software_principles/python/#instance-methods","text":"class Pizza : def __init__ ( self , size : float ): self . size = size self . class_instance_id = id ( Pizza ) def get_pizza_size ( self ): return self . size , self @classmethod def return_classmethod ( cls ): assert id ( cls ) == id ( Pizza ), \"The id of both must be the same!\" return cls Initialize an instance of Pizza object with size 10 named p1 . Note the id of this p1 is id(p1) . p1 = Pizza ( size = 10 ) print ( id ( p1 )) 1470722576792 You can see the method get_pizza_size() takes one parameter, self , which points to an instance of Pizza when the method is called (but of course instance methods can accept more than just one parameter). I returned self.size and self for this method. pizza_size , instance_of_pizza = p1 . get_pizza_size () Note that id(instance_of_pizza) is equals to id(p1) since self points directly to p1 . assert id ( instance_of_pizza ) == id ( p1 ) Through the self parameter, instance methods can freely access attributes and other methods on the same object. This gives them a lot of power when it comes to modifying an object\u2019s state. In our example, under this get_pizza_size method, we can access the attribute size of the Pizza object by calling self.size , which is equivalent to Pizza(size=10).size . This is powerful cause we can modify the object instance's state! For example: class Pizza : def __init__ ( self , size : float ): self . size = size def get_pizza_size ( self ): self . size = 100 return self . size , self Now if we call: p1 = Pizza ( size = 10 ) pizza_size , _ = p1 . get_pizza_size () print ( p1 . size ) 100 and note that the attribute of p1 is no longer 10 but 100 since we changed it using self . It is like doing: p1 = Pizza ( size = 10 ) p1 . size = 100 print ( p1 . size )","title":"Instance Methods"},{"location":"software_principles/python/#class-methods","text":"Instead of accepting a self parameter, class methods take a cls parameter that points to the class\u2014and not the object instance\u2014when the method is called. Recall earlier the minor difference between a class instance vs an object instance . # 1. recall that class instance is: Pizza , p1 . return_classmethod () (__main__.Pizza, __main__.Pizza) # 2. now compare id! class_instance_id = p1 . class_instance_id class_method_id = id ( p1 . return_classmethod ()) class_instance_id , class_method_id (1470710370600, 1470710370600) So now one should be clear that within the object instance p1 , the Pizza class id must be the same as the id of cls .","title":"Class Methods"},{"location":"software_principles/python/#example-usage","text":"class Pizza : def __init__ ( self , ingredients : List [ str ]): self . ingredients = ingredients def __repr__ ( self ): return f 'Pizza( { self . ingredients !r} )' @classmethod def margherita ( cls ): return cls ([ 'mozzarella' , 'tomatoes' ]) @classmethod def prosciutto ( cls ): return cls ([ 'mozzarella' , 'tomatoes' , 'ham' ]) First, if we want to create two object instances named margherita and prosciutto that are created by: margherita = Pizza ([ 'mozzarella' , 'tomatoes' ]) prosciutto = Pizza ([ 'mozzarella' , 'tomatoes' , 'ham' ]) A neater way is to use classmethod . Pizza . margherita (), Pizza . prosciutto () (Pizza(['mozzarella', 'tomatoes']), Pizza(['mozzarella', 'tomatoes', 'ham']))","title":"Example Usage"},{"location":"software_principles/python/#static-method","text":"Note static method has no self or cls , so it can neither access to the class instance nor the object instance . Then why is it useful sometimes since it is as good as I were to define the static method outside the class as a function. One reason can be understood as follows, albeit a bit of a forced example: class Pizza : def __init__ ( self , radius , ingredients ): self . radius = radius self . ingredients = ingredients def __repr__ ( self ): return ( f 'Pizza( { self . radius !r} , ' f ' { self . ingredients !r} )' ) def calculate_pizza_area ( self ): return self . calculate_circle_area ( self . radius ) @staticmethod def calculate_circle_area ( r ): return r ** 2 * math . pi Maintain your class design, even though calculate_circle_area is independent of the class/object state, one can still argue that calculating circle area is still relevant to the whole architecture of the Pizza class since we have a method to calculate pizza area. Ease of testing, one can just test the static method without initializing the object instance itself.","title":"Static Method"},{"location":"software_principles/python/#abstract-methods","text":"This provides us a template or blueprint in a sense. from abc import ABCMeta , abstractmethod class AbstractPizza ( metaclass = ABCMeta ): def __init__ ( self , radius : float ): self . radius = radius @abstractmethod def calculate_pizza_area ( self ): raise NotImplementedError ( \"This method needs to be implemented\" ) AbstractPizza ( radius = 10 ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/1152115237.py in <module> ----> 1 AbstractPizza ( radius = 10 ) TypeError : Can't instantiate abstract class AbstractPizza with abstract method calculate_pizza_area class Pizza ( AbstractPizza ): def __init__ ( self , radius : float ): self . radius = radius Pizza ( radius = 10 ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_4064/4082189809.py in <module> ----> 1 Pizza ( radius = 10 ) TypeError : Can't instantiate abstract class Pizza with abstract method calculate_pizza_area class Pizza ( AbstractPizza ): def __init__ ( self , radius : float ): self . radius = radius def calculate_pizza_area ( self ): return self . radius ** 2 * math . pi Pizza ( radius = 10 ) <__main__.Pizza at 0x18a435d3a00>","title":"Abstract Methods"},{"location":"software_principles/python/#inheritance","text":"Inheritance is the process by which one class takes on the attributes and methods of another. Newly formed classes are called child classes, and the classes that child classes are derived from are called parent classes.","title":"Inheritance"},{"location":"software_principles/python/#intuition_3","text":"Child classes can override or extend the attributes and methods of parent classes. In other words, child classes inherit all of the parent\u2019s attributes and methods but can also specify attributes and methods that are unique to themselves. Although the analogy isn\u2019t perfect, you can think of object inheritance sort of like genetic inheritance. You may have inherited your hair color from your mother. It\u2019s an attribute you were born with. Let\u2019s say you decide to color your hair purple. Assuming your mother doesn\u2019t have purple hair, you\u2019ve just overridden the hair color attribute that you inherited from your mom. You also inherit, in a sense, your language from your parents. If your parents speak English, then you\u2019ll also speak English. Now imagine you decide to learn a second language, like German. In this case you\u2019ve extended your attributes because you\u2019ve added an attribute that your parents don\u2019t have.","title":"Intuition"},{"location":"software_principles/python/#how-to-use-inheritance","text":"A base parent class Employee with 2 attributes: employee_id employee_name A child class that inherits the parent class called SalaryEmployee ; note the __init__ of this child class takes in 3 attributes: employee_id : from parent employee_name : from parent monthly_salary : from child It is also worth noting we used super().__init__ to take in the parent class's attributes which is the same as calling Employee.__init__(employee_id, employee_name) , both of which initializes the parents class in the child class. Intuitively, you can think of that the child class has all the attributes and methods that the parent class has. class Employee : def __init__ ( self , employee_id : int , employee_name : str ) -> None : self . employee_id = employee_id self . employee_name = employee_name class SalaryEmployee ( Employee ): def __init__ ( self , employee_id : int , employee_name : str , monthly_salary : Union [ int , float ], ) -> None : super () . __init__ ( employee_id , employee_name ) self . monthly_salary = monthly_salary def calculate_annual_salary ( self ) -> Union [ int , float ]: \"\"\"Calculate annual salary. Returns: Union[int, float]: Monthly salary * 12 \"\"\" return self . monthly_salary * 12 class CommissionEmployee ( SalaryEmployee ): def __init__ ( self , employee_id : int , employee_name : str , monthly_salary : Union [ int , float ], commission : Union [ int , float ], ) -> None : super () . __init__ ( employee_id , employee_name , monthly_salary ) self . commission = commission def calculate_annual_salary ( self ) -> Union [ int , float ]: \"\"\"Calculate annual salary + commission. Returns: Union[int, float]: Monthly salary * 12 + commission \"\"\" fixed_annual_salary = super () . calculate_annual_salary () return fixed_annual_salary + self . commission Simple example of inheritance. ken_salary = SalaryEmployee ( employee_id = 123 , employee_name = \"ken\" , monthly_salary = 5000 ) ken_salary . calculate_annual_salary () 60000 Slightly more complicated logic where one used super() in line 44. Why don't we just use fixed_annual_salary = self.monthly_salary * 12 to get the fixed year wage like how we did in SalaryEmployee . The problem with accessing the property directly is that if the implementation of SalaryEmployee.calculate_annual_salary() changes, then you\u2019ll have to also change the implementation of CommissionEmployee.calculate_annual_salary() . It\u2019s better to rely on the already implemented method in the base class and extend the functionality as needed. Calling super() in this child class will invoke the method in the parent class. So if calculate_annual_salary() in the parent class becomes something like monthly_salary * 13 , then you don't need to worry about changing the logic again in the child CommissionEmployee when calculating the total annual salary. ken_salary_and_commision = CommissionEmployee ( employee_id = 123 , employee_name = \"ken\" , monthly_salary = 5000 , commission = 10000 ) ken_salary_and_commision . calculate_annual_salary () 70000","title":"How to use Inheritance"},{"location":"software_principles/python/#super-init-and-inheritance-diamond","text":"https://stackoverflow.com/questions/29173299/super-init-vs-parent-init https://thepythonguru.com/python-classes-and-interfaces/","title":"Super Init and Inheritance Diamond"},{"location":"software_principles/python/#main-references_1","text":"Main Reference for Python Classes Designs Overall well rounder for many concepts. So if one has to choose one, this will be the one to read first or together with other references. Main Reference for Inheritance and Composition Mentions ABC class as well. Basic OOP Guide Object Instance Methods Main Reference for Python Classes Designs Python's Instance, Class, and Static Methods Demystified The definitive guide on how to use static, class or abstract methods in Python : Mostly Python 2 so slightly outdated but did mention about Python 3 inside. What is the advantage of using static methods?","title":"Main References"},{"location":"software_principles/python/#class-methods-in-python","text":"","title":"Class Methods in Python"},{"location":"software_principles/python/#args-and-kwargs","text":"https://stackoverflow.com/questions/9872824/calling-a-python-function-with-args-kwargs-and-optional-default-arguments","title":"Args and Kwargs"},{"location":"software_principles/python/#property-decorator","text":"https://docs.python.org/3/library/functions.html#property https://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work-in-python class C : def __init__ ( self ): self . _x = None def getx ( self ): return self . _x def setx ( self , value ): self . _x = value def delx ( self ): del self . _x x = property ( getx , setx , delx , \"I'm the 'x' property.\" ) c = C () c . x # get attribute x which is None c . x = 10 # set attribute x to 10 c . x # get attribute x which is 10 10 class Parrot : def __init__ ( self ): self . _voltage = 100000 @property def voltage ( self ): \"\"\"Get the current voltage.\"\"\" return self . _voltage p = Parrot () 2 practical use with just using property : To make a class attribute read-only. Here _voltage is a private attribute and you don't want to allow the user to change it. So you can use property to make it read-only. p . voltage # this works! p . _voltage # this works too! 100000 p . voltage = 100 # this does not work because voltage is a property with read only access! No overwriting... # p._voltage = 100 # this works so but should not be used! --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In [16], line 1 ----> 1 p.voltage = 100 AttributeError : can't set attribute second use case is less keystrokes, like the following, instead of calling bbox.coordinates() you can just call bbox.coordinates . import numpy as np import torch from typing import Union class Box : def __init__ ( self , inputs : Union [ torch . Tensor , np . ndarray ]): self . inputs = inputs @property def coordinates ( self ): \"\"\"Get the current voltage.\"\"\" if isinstance ( self . inputs , torch . Tensor ): return self . inputs . clone () return self . inputs . copy () bbox = Box ( inputs = np . asarray ([ 1 , 2 , 3 ])) bbox . coordinates array([1, 2, 3]) Furthermore, now your coordinates cannot be setattr by the user here. bbox . coordinates = 10 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In [7], line 1 ----> 1 bbox.coordinates = 10 AttributeError : can't set attribute One more very good answer here from https://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work-in-python just very recently. In the following, I have given an example to clarify @property Consider a class named Student with two variables: name and class_number and you want class_number to be in the range of 1 to 5 . Now I will explain two wrong solutions and finally the correct one: The code below is wrong because it doesn't validate the class_number (to be in the range 1 to 5) class Student: def __init__(self, name, class_number): self.name = name self.class_number = class_number Despite validation, this solution is also wrong : def validate_class_number(number): if 1 <= number <= 5: return number else: raise Exception(\"class number should be in the range of 1 to 5\") class Student: def __init__(self, name, class_number): self.name = name self.class_number = validate_class_number(class_number) Because class_number validation is checked only at the time of making a class instance and it is not checked after that (it is possible to change class_number with a number outside of the range 1 to 5): student1 = Student(\"masoud\",5) student1.class_number = 7 The correct solution is: class Student: def __init__(self, name, class_number): self.name = name self.class_number = class_number @property def class_number(self): return self._class_number @class_number.setter def class_number(self, class_number): if not (1 <= class_number <= 5): raise Exception(\"class number should be in the range of 1 to 5\") self._class_number = class_number Note that with setter then you are able to assign attributes again (not read only), but it will check the validation first.","title":"Property Decorator"},{"location":"software_principles/python/#type-hinting","text":"Why using typevar is better than Any in the below case. from typing import TypeVar , Generic , List T = TypeVar ( 'T' ) class Stack ( Generic [ T ]): def __init__ ( self ) -> None : # Create an empty list with items of type T self . items : List [ T ] = [] def push ( self , item : T ) -> None : self . items . append ( item ) def pop ( self ) -> T : return self . items . pop () def empty ( self ) -> bool : return not self . items my_str = \"abcdefg\" def reverse_string_using_stack ( stack : Stack [ str ], string : str ) -> str : reversed_string = \"\" for s in string : stack . push ( s ) while not stack . empty (): reversed_string += stack . pop () return reversed_string stack = Stack () reverse_string_using_stack ( stack , my_str ) 'gfedcba' Notice in argument I defined stack: Stack[str] indicating that the stack is a stack of strings. But in the function body, I used stack.push(1) which is not a string. So the type checker will complain if I use mypy for example. from typing import Any , List class Stack : def __init__ ( self ) -> None : # Create an empty list with items of type T self . items : List [ Any ] = [] def push ( self , item : T ) -> None : self . items . append ( item ) def pop ( self ) -> T : return self . items . pop () def empty ( self ) -> bool : return not self . items my_str = \"abcdefg\" def reverse_string_using_stack ( stack : Stack [ str ], string : str ) -> str : reversed_string = \"\" for s in string : stack . push ( s ) while not stack . empty (): reversed_string += stack . pop () return reversed_string stack = Stack () reverse_string_using_stack ( stack , my_str ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In [9], line 3 1 my_str = \"abcdefg\" ----> 3 def reverse_string_using_stack(stack: Stack[str], string: str) -> str: 5 reversed_string = \"\" 7 for s in string: TypeError : 'type' object is not subscriptable If we just use List[Any] above, then we cannot define a type for Stack object just how we do for Tuple[int, int, int] . So in this case it is better to do the first method. See https://mypy.readthedocs.io/en/stable/generics.html for more info.and https://codereview.stackexchange.com/questions/256319/singly-linked-list-python-code/256351?noredirect=1#comment556249_256351","title":"Type Hinting"},{"location":"software_principles/python/#generators-and-iterators","text":"https://www.programiz.com/python-programming/iterator https://anandology.com/python-practice-book/iterators.html Real python? Immutable vs Hash . \u21a9 Post Init Example \u21a9","title":"Generators and Iterators"}]}
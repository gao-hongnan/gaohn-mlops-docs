{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "zzbCCcRIn-eP",
      "metadata": {
        "id": "zzbCCcRIn-eP"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gao-hongnan/gaohn-mlops-docs/blob/main/docs/mlops_docs/testing/07_testing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL4eLW3gpLMH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pytest\n",
        "import torch"
      ],
      "id": "NL4eLW3gpLMH"
    },
    {
      "cell_type": "markdown",
      "id": "lsrQ8EeNvwQ7",
      "metadata": {
        "id": "lsrQ8EeNvwQ7"
      },
      "source": [
        "## Intuition\n",
        "\n",
        "Tests are a way for us to ensure that something works as intended. We're incentivized to implement tests and discover sources of error as early in the development cycle as possible so that we can reduce [increasing downstream costs](https://assets.deepsource.io/39ed384/images/blog/cost-of-fixing-bugs/chart.jpg) and wasted time. Once we've designed our tests, we can automatically execute them every time we implement a change to our system and continue to build on them over time. In this lesson, we'll learn how to test machine learning code, data and models to construct a system that we can reliably iterate on."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tt2JiOX4vxDb",
      "metadata": {
        "id": "tt2JiOX4vxDb"
      },
      "source": [
        "## Types of tests\n",
        "\n",
        "There are many four majors types of tests which are utilized at different points in the development cycle:\n",
        "\n",
        "- Unit tests: tests on individual components that each have a single responsibility (ex. function that filters a list).\n",
        "- Integration tests: tests on the combined functionality of individual components (ex. data processing).\n",
        "- System tests: tests on the design of a system for expected outputs given inputs (ex. training, inference, etc.).\n",
        "- Acceptance tests: tests to verify that requirements have been met, usually referred to as User Acceptance Testing (UAT).\n",
        "- Regression tests: testing errors we've seen before to ensure new changes don't reintroduce them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OE4Nl89ewblO",
      "metadata": {
        "id": "OE4Nl89ewblO"
      },
      "source": [
        "## How should we test?\n",
        "\n",
        "The framework to use when composing tests is the [Arrange Act Assert methodology](http://wiki.c2.com/?ArrangeActAssert).\n",
        "\n",
        "- Arrange: set up the different inputs to test on.\n",
        "- Act: apply the inputs on the component we want to test.\n",
        "- Assert: confirm that we received the expected output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nbuDcZ86wurM",
      "metadata": {
        "id": "nbuDcZ86wurM"
      },
      "source": [
        "## What should we be testing for?\n",
        "\n",
        "An example:\n",
        "\n",
        "> When arranging our inputs and asserting our expected outputs, what are some aspects of our inputs and outputs that we should be testing for?\n",
        "\n",
        "- inputs: data types, format, length, edge cases (min/max, small/large, etc.)\n",
        "- outputs: data types, formats, exceptions, intermediary and final outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ooFZczxxxQ6v",
      "metadata": {
        "id": "ooFZczxxxQ6v"
      },
      "source": [
        "## Best practices\n",
        "\n",
        "Regardless of the framework we use, it's important to strongly tie testing into the development process.\n",
        "\n",
        "- atomic: when creating unit components, we need to ensure that they have a [single responsibility](https://en.wikipedia.org/wiki/Single-responsibility_principle) so that we can easily test them. If not, we'll need to split them into more granular units.\n",
        "\n",
        "- compose: when we create new components, we want to compose tests to validate their functionality. It's a great way to ensure reliability and catch errors early on.\n",
        "\n",
        "- regression: we want to account for new errors we come across with a regression test so we can ensure we don't reintroduce the same errors in the future.\n",
        "\n",
        "- coverage: we want to ensure that 100% of our codebase has been accounter for. This doesn't mean writing a test for every single line of code but rather accounting for every single line (more on this in the coverage section below).\n",
        "\n",
        "- automate: in the event we forget to run our tests before committing to a repository, we want to auto run tests for every commit. We'll learn how to do this locally using pre-commit hooks and remotely (ie. main branch) via GitHub actions in subsequent lessons."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aXXTyZ9zxdg0",
      "metadata": {
        "id": "aXXTyZ9zxdg0"
      },
      "source": [
        "## Test-driven development or Otherwise?\n",
        "\n",
        "[Test-driven development (TDD)](https://en.wikipedia.org/wiki/Test-driven_development) is the process where you write a test before completely writing the functionality to ensure that tests are always written. This is in contrast to writing functionality first and then composing tests afterwards. Here are my thoughts on this:\n",
        "\n",
        "- good to write tests as we progress, but it's not the representation of correctness.\n",
        "\n",
        "- initial time should be spent on design before ever getting into the code or tests.\n",
        "\n",
        "- using a test as guide doesn't mean that our functionality is error free.\n",
        "\n",
        "Perfect coverage doesn't mean that our application is error free if those tests aren't meaningful and don't encompass the field of possible inputs, intermediates and outputs. Therefore, we should work towards better design and agility when facing errors, quickly resolving them and writing test cases around them to avoid them next time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2A3cbTZEyMyp",
      "metadata": {
        "id": "2A3cbTZEyMyp"
      },
      "source": [
        "## Pytest\n",
        "\n",
        "We're going to be using [pytest](https://docs.pytest.org/en/stable/) as our testing framework for it's powerful builtin features such as parametrization, fixtures, markers, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cCkQDIn2yYc5",
      "metadata": {
        "id": "cCkQDIn2yYc5"
      },
      "source": [
        "### Configuration\n",
        "\n",
        "Pytest expects tests to be organized under a `tests` directory by default. However, we can also use our `pyproject.toml` file to configure any other test path directories as well. Once in the directory, pytest looks for python scripts starting with `tests_*.py` but we can configure it to read any other file patterns as well.\n",
        "\n",
        "```toml\n",
        "# Pytest\n",
        "[tool.pytest.ini_options]\n",
        "testpaths = [\"tests\"]\n",
        "python_files = \"test_*.py\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eYibqMWyyYfU",
      "metadata": {
        "id": "eYibqMWyyYfU"
      },
      "source": [
        "### Assertions\n",
        "\n",
        "Simple assertion testing example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JZRGRMjz0ksw",
      "metadata": {
        "id": "JZRGRMjz0ksw"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Creating Directories\n",
        "BASE_DIR = Path(\"__file__\").parent.absolute()\n",
        "\n",
        "SRC_DIR = Path.joinpath(BASE_DIR, \"src\")\n",
        "TEST_DIR = Path.joinpath(BASE_DIR, \"tests\")\n",
        "SRC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEST_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tBT2pW0U1El9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBT2pW0U1El9",
        "outputId": "03ccfc02-7542-4187-ff76-3146b0f3182b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/pyproject.toml\n"
          ]
        }
      ],
      "source": [
        "%%writefile {BASE_DIR}/pyproject.toml\n",
        "# Pytest\n",
        "[tool.pytest.ini_options]\n",
        "testpaths = [\"tests\"]\n",
        "python_files = \"test_*.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pj94RxXL2R5R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj94RxXL2R5R",
        "outputId": "87730208-691b-46f9-edb6-6e8d84af91ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/src/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {SRC_DIR}/__init__.py\n",
        "\"init file\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z6QVf7Vt1Hej",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6QVf7Vt1Hej",
        "outputId": "c4b1f4ff-2eef-4065-eaed-a77dbe9f280c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/src/fruits.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {SRC_DIR}/fruits.py\n",
        "def is_crisp(fruit):\n",
        "    if fruit:\n",
        "        fruit = fruit.lower()\n",
        "    if fruit in [\"apple\", \"watermelon\", \"cherries\"]:\n",
        "        return True\n",
        "    elif fruit in [\"orange\", \"mango\", \"strawberry\"]:\n",
        "        return False\n",
        "    else:\n",
        "        raise ValueError(f\"{fruit} not in known list of fruits.\")\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GI0huQ400d-C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI0huQ400d-C",
        "outputId": "292c5a1b-200b-4274-87a7-7b66c8246e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/tests/test_fruits.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {TEST_DIR}/test_fruits.py\n",
        "import pytest\n",
        "import sys \n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.fruits import is_crisp\n",
        "def test_is_crisp():\n",
        "    assert is_crisp(fruit=\"apple\") #  or == True\n",
        "    assert is_crisp(fruit=\"Apple\")\n",
        "    assert not is_crisp(fruit=\"orange\")\n",
        "    with pytest.raises(ValueError):\n",
        "        is_crisp(fruit=None)\n",
        "        is_crisp(fruit=\"pear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K9phDnsIz2BT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9phDnsIz2BT",
        "outputId": "e03980b0-5c28-41ca-cdba-94726da8922e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest                                      # all tests\n",
        "!pytest tests/                               # tests under a directory\n",
        "!pytest tests/test_fruits.py                 # tests for a single file\n",
        "!pytest tests/test_fruits.py::test_is_crisp  # tests for a single function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IRAyAp-W3TuM",
      "metadata": {
        "id": "IRAyAp-W3TuM"
      },
      "source": [
        "### Classes\n",
        "\n",
        "See [examples from madewithml repo](https://github.com/GokuMohandas/follow/blob/testing/tests/tagifai/test_data.py) to understand better."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "golDlmT04pj8",
      "metadata": {
        "id": "golDlmT04pj8"
      },
      "source": [
        "### Interfaces\n",
        "\n",
        "See [madewithml interface section](https://madewithml.com/courses/mlops/testing/#interfaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sa6WwNTC4rOx",
      "metadata": {
        "id": "sa6WwNTC4rOx"
      },
      "source": [
        "### Parametrize\n",
        "\n",
        "So far, in our tests, we've had to create individual assert statements to validate different combinations of inputs and expected outputs. However, there's a bit of redundancy here because the inputs always feed into our functions as arguments and the outputs are compared with our expected outputs. To remove this redundancy, pytest has the [`@pytest.mark.parametrize`](https://docs.pytest.org/en/stable/parametrize.html) decorator which allows us to represent our inputs and outputs as parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RRC-cuyQ6jcf",
      "metadata": {
        "id": "RRC-cuyQ6jcf"
      },
      "source": [
        "Let us create a new python file `test_fruits_parametrize.py` to test it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nZTBT3dWzyjI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZTBT3dWzyjI",
        "outputId": "7f851b5e-0cd7-46fc-bee4-874fc67b993a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/tests/test_fruits_parametrize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {TEST_DIR}/test_fruits_parametrize.py\n",
        "import pytest\n",
        "import sys \n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.fruits import is_crisp\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"fruit, crisp\",\n",
        "    [\n",
        "        (\"apple\", True),\n",
        "        (\"Apple\", True),\n",
        "        (\"orange\", False),\n",
        "    ],\n",
        ")\n",
        "def test_is_crisp_parametrize(fruit, crisp):\n",
        "    assert is_crisp(fruit=fruit) == crisp\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"fruit, exception\",\n",
        "    [\n",
        "        (\"pear\", ValueError),\n",
        "    ],\n",
        ")\n",
        "def test_is_crisp_exceptions(fruit, exception):\n",
        "    with pytest.raises(exception):\n",
        "        is_crisp(fruit=fruit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5x1_GBJ26u2L",
      "metadata": {
        "id": "5x1_GBJ26u2L"
      },
      "source": [
        "To fix line number, but for now the line number starts from the decorator `@pytest.mark.parametrize`.\n",
        "\n",
        "- [Line 2]: define the names of the parameters under the decorator, ex. \"fruit, crisp\" (note that this is one string). Note that this string names should correspond to the function defined under the decorator.\n",
        "\n",
        "- [Lines 3-7]: provide a list of combinations of values for the parameters from Step 1.\n",
        "\n",
        "- [Line 9]: pass in parameter names to the test function.\n",
        "\n",
        "- [Line 10]: include necessary assert statements which will be executed for each of the combinations in the list from Step 2.\n",
        "\n",
        "- [Line 12-20]: this tests exception handling as well if you pass in as such."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4HBrPbTqz77u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HBrPbTqz77u",
        "outputId": "fcdb75e0-99a7-4bed-ff0e-2d64b6f97f16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_fruits_parametrize.py ....\u001b[36m                                    [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 4 passed in 0.01 seconds ===========================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest tests/test_fruits_parametrize.py  # tests for a single function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8sDwlFcpyYhy",
      "metadata": {
        "id": "8sDwlFcpyYhy"
      },
      "source": [
        "### Fixtures\n",
        "\n",
        "[What's the benefits of using fixtures?](https://realpython.com/pytest-python-testing/#fixtures-managing-state-and-dependencies)\n",
        "\n",
        "One obvious reason that I know of is about reducing the redundancies of re-defining inputs every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5VG1gbzONw8",
      "metadata": {
        "id": "c5VG1gbzONw8"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "def add(nums_list):\n",
        "    return np.sum(nums_list)\n",
        "\n",
        "\n",
        "def mul(nums_list):\n",
        "    return np.prod(nums_list)\n",
        "\n",
        "def test_add():\n",
        "    nums_list = [1, 2, 3, 4, 5]\n",
        "    assert add(nums_list) == 15\n",
        "\n",
        "def test_mul():\n",
        "    nums_list = [1, 2, 3, 4, 5]\n",
        "    assert add(nums_list) == 120"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rp_MjOd3PQ5N",
      "metadata": {
        "id": "Rp_MjOd3PQ5N"
      },
      "source": [
        "Notice that you defined `nums_list` twice when we want to test different functions with the **same inputs**. So to reduce this redundancy, we can do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hiyvxI2kPMgJ",
      "metadata": {
        "id": "hiyvxI2kPMgJ"
      },
      "outputs": [],
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_nums_list():\n",
        "    nums_list = [1, 2, 3, 4, 5]\n",
        "    return nums_list "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Test"
      ],
      "metadata": {
        "id": "oMegSNYBCJjY"
      },
      "id": "oMegSNYBCJjY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mock"
      ],
      "metadata": {
        "id": "VbGeFJNCCNE2"
      },
      "id": "VbGeFJNCCNE2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Readings:\n",
        "\n",
        "- https://realpython.com/python-mock-library/\n",
        "- https://docs.python.org/3/library/unittest.mock.html (READ THE API)\n",
        "- PeekingDuck `draw.poses` test suites."
      ],
      "metadata": {
        "id": "U2O6XzV6CPOZ"
      },
      "id": "U2O6XzV6CPOZ"
    },
    {
      "cell_type": "markdown",
      "id": "4p6BRfFVyYj-",
      "metadata": {
        "id": "4p6BRfFVyYj-"
      },
      "source": [
        "## Example Walkthrough\n",
        "\n",
        "> **The example walkthrough assumes you have a basic understanding of pytests.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Setup"
      ],
      "metadata": {
        "id": "J1HIokKMvFCh"
      },
      "id": "J1HIokKMvFCh"
    },
    {
      "cell_type": "markdown",
      "id": "jp66_v-WyYmE",
      "metadata": {
        "id": "jp66_v-WyYmE"
      },
      "source": [
        "In the field of object detection, given a query image, our task is to **localize** and **classify**. **Localization** needs labels and they are in the form of **bounding boxes**.\n",
        "\n",
        "As an example, we take the image from [albumentations](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/), this image shows us a cat with a bounding box drawn around it. The coordinates are marked accordingly. One thing to note is that the image coordinates' origin starts at the top-left corner, so it is like a cartesian coordinate but rotated clockwise by 90 degrees.\n",
        "\n",
        "![cat_bbox_example](https://albumentations.ai/docs/images/getting_started/augmenting_bboxes/bbox_example.jpg)\n",
        "\n",
        "The bounding boxes can be represented in various different formats. Most notably, the Pascal-VOC, COCO and the YOLO format. More information can be found [here](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/).\n",
        "\n",
        "Our task is two fold, one is to create some utility functions that transform from one format to another, and the other is to write some unit testing to ensure the correctness of our codes.\n",
        "\n",
        "For our purpose, we will deal with two formats, given a bounding box of Pascal-VOC format, we want to convert it to YOLO format, and vice versa. \n",
        "\n",
        "First, we briefly quote from [albumentations](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/) on the two formats.\n",
        "\n",
        "Pascal-VOC is a format used by the Pascal VOC dataset. Coordinates of a bounding box are encoded with four values in pixels: `[x_min, y_min, x_max, y_max]`. `x_min` and `y_min` are coordinates of the top-left corner of the bounding box. `x_max` and `y_max` are coordinates of bottom-right corner of the bounding box. For our purpose, we will call them `xyxy` for abbreviation.\n",
        "\n",
        "YOLO format's bounding box is represented by four values `[x_center, y_center, width, height]`. `x_center` and `y_center` are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. width and height represent the width and the height of the bounding box. They are normalized as well. For our purpose, we will call them `xywhn` for abbreviation.\n",
        "\n",
        "More concretely, if an image is of heigth of 480 and width 640 with its bounding box in Pascal-VOC format with coordinates\n",
        "\n",
        "```python\n",
        "xyxy = [98, 345, 420, 462] # x1 y1 x2 y2\n",
        "```\n",
        "\n",
        "we want to transform it to the equivalent YOLO coordinates\n",
        "\n",
        "```python\n",
        "xywhn = [0.4046875, 0.840625, 0.503125, 0.24375] # x y w h\n",
        "```\n",
        "\n",
        "and vice versa. \n",
        "\n",
        "As of now, we have verified by hand that `[98, 345, 420, 462]` indeed convert correctly to `[0.4046875, 0.840625, 0.503125, 0.24375]` with the given height and width. But as programmers, our task is to reduce manual work, let's write out the functions to convert the bounding boxes in between formats."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0. Setting up Folders/Scripts"
      ],
      "metadata": {
        "id": "URx7Yyr8AsGT"
      },
      "id": "URx7Yyr8AsGT"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p bbox"
      ],
      "metadata": {
        "id": "ww8Qp_fEUl9o"
      },
      "id": "ww8Qp_fEUl9o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/bbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukIMj45GVhS_",
        "outputId": "43f34dea-1d21-474c-eaa3-c8db09302763"
      },
      "id": "ukIMj45GVhS_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bbox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Creating Directories\n",
        "BASE_DIR = Path(\"__file__\").parent.absolute()\n",
        "\n",
        "SRC_DIR = Path.joinpath(BASE_DIR, \"src\")\n",
        "TEST_DIR = Path.joinpath(BASE_DIR, \"tests\")\n",
        "SRC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEST_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "6sS8CFeZAvAf"
      },
      "id": "6sS8CFeZAvAf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {SRC_DIR}/__init__.py\n",
        "\"init file\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxRUyIoIA220",
        "outputId": "eaf65c27-3162-4fdc-f69a-f525af2007ee"
      },
      "id": "vxRUyIoIA220",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bbox/src/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {BASE_DIR}/pyproject.toml\n",
        "# Pytest\n",
        "[tool.pytest.ini_options]\n",
        "testpaths = [\"tests\"]\n",
        "python_files = \"test_*.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMKPrcwZA0qX",
        "outputId": "81b8295e-7890-4cb5-fc73-f32e9c7259d9"
      },
      "id": "xMKPrcwZA0qX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bbox/pyproject.toml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. Writing our functions"
      ],
      "metadata": {
        "id": "3A9dmUmv57Ud"
      },
      "id": "3A9dmUmv57Ud"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create two functions `xyxy2xywhn` and `xywhn2xyxy`, the former takes in `inputs` of Pascal-VOC style bounding box and **transforms** it to its equivalent YOLO format, while the latter does the opposite.\n",
        "\n",
        "We will write our functions into `src/bbox_utils.py`."
      ],
      "metadata": {
        "id": "mdiQg9_Y3Fcj"
      },
      "id": "mdiQg9_Y3Fcj"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {SRC_DIR}/bbox_utils.py\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "BboxType = Union[np.ndarray, torch.Tensor]\n",
        "\n",
        "\n",
        "def cast_to_float(inputs: BboxType) -> BboxType:\n",
        "    if isinstance(inputs, torch.Tensor):\n",
        "        return inputs.float()\n",
        "    return inputs.astype(np.float32)\n",
        "\n",
        "\n",
        "def clone(inputs: BboxType) -> BboxType:\n",
        "    if isinstance(inputs, torch.Tensor):\n",
        "        return inputs.clone()\n",
        "    return inputs.copy()\n",
        "\n",
        "def xyxy2xywhn(inputs: BboxType, height: float, width: float) -> BboxType:\n",
        "    outputs = clone(inputs)\n",
        "    outputs = cast_to_float(outputs)\n",
        "\n",
        "    outputs[..., [0, 2]] /= width\n",
        "    outputs[..., [1, 3]] /= height\n",
        "\n",
        "    outputs[..., 2] -= outputs[..., 0]\n",
        "    outputs[..., 3] -= outputs[..., 1]\n",
        "\n",
        "    outputs[..., 0] += outputs[..., 2] / 2\n",
        "    outputs[..., 1] += outputs[..., 3] / 2\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def xywhn2xyxy(inputs: BboxType, height: float, width: float) -> BboxType:\n",
        "    outputs = clone(inputs)\n",
        "    outputs = cast_to_float(outputs)\n",
        "\n",
        "    outputs[..., [0, 2]] *= width\n",
        "    outputs[..., [1, 3]] *= height\n",
        "\n",
        "    outputs[..., 0] -= outputs[..., 2] / 2\n",
        "    outputs[..., 1] -= outputs[..., 3] / 2\n",
        "    outputs[..., 2] += outputs[..., 0]\n",
        "    outputs[..., 3] += outputs[..., 1]\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFRVdo_G6YRd",
        "outputId": "462cd020-6d98-46fd-fd43-2e2b0637a99d"
      },
      "id": "jFRVdo_G6YRd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bbox/src/bbox_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tb9mEKOq7QOq",
      "metadata": {
        "id": "Tb9mEKOq7QOq"
      },
      "source": [
        "After writing the functions, we can simply just test the **correctness** of the transformation by passing in our pre-defined ground truths defined earlier.\n",
        "\n",
        "For example, when I pass in `xyxy = [98, 345, 420, 462]` to `voc2yolo`, I expect it `voc2yolo(xyxy, 480, 640)` to output `xywhn = [0.4046875, 0.840625, 0.503125, 0.24375]`.\n",
        "\n",
        "Without using any library, we can simply do something like the following, using an `assertion` "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile --append {SRC_DIR}/bbox_utils.py\n",
        "\n",
        "xyxy = np.asarray([98, 345, 420, 462])\n",
        "xywhn = np.asarray([0.4046875, 0.840625, 0.503125, 0.24375])\n",
        "\n",
        "assert np.allclose(xyxy2xywhn(xyxy, height=480, width=640), xywhn, rtol=1e-05, atol=1e-08)\n",
        "assert np.allclose(xywhn2xyxy(xywhn, height=480, width=640), xyxy, rtol=1e-05, atol=1e-08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpRA90ky60Vz",
        "outputId": "7f8326e9-e9c3-4027-b798-15cc53927b23"
      },
      "id": "SpRA90ky60Vz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/bbox/src/bbox_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python src/bbox_utils.py"
      ],
      "metadata": {
        "id": "sVpL8saL7Aaf"
      },
      "id": "sVpL8saL7Aaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "PhRno6SY9_VA",
      "metadata": {
        "id": "PhRno6SY9_VA"
      },
      "source": [
        "The assertion passed!\n",
        "\n",
        "This may seem fine, but it is very hard to scale up when you add in more transformations, imagine having 10 pairs of transformation functions, you will need to do the assertion 20 times.\n",
        "\n",
        "Furthermore, functions like these often have implicit assumptions that need to be rigourously tested as well.\n",
        "\n",
        "For example, we defined `BboxType = Union[np.ndarray, torch.Tensor]` and type hinted our functions' `inputs` and `outputs` to be both of this type. In particular, when a user pass in an array of type `torch.Tensor`, I expect the output to be of the same type as in the input. This is important as many operations performed on `torch.Tensor` does not carry forward to their `np.ndarray` counterpart. ***Our assert statement above does not check this, and this will be a problem. See example below.***\n",
        "\n",
        "```python\n",
        "def xywhn2xyxy(inputs: BboxType, height: float, width: float) -> BboxType:\n",
        "    outputs = clone(inputs)\n",
        "    outputs = cast_to_float(outputs)\n",
        "\n",
        "    outputs[..., [0, 2]] *= width\n",
        "    outputs[..., [1, 3]] *= height\n",
        "\n",
        "    outputs[..., 0] -= outputs[..., 2] / 2\n",
        "    outputs[..., 1] -= outputs[..., 3] / 2\n",
        "    outputs[..., 2] += outputs[..., 0]\n",
        "    outputs[..., 3] += outputs[..., 1]\n",
        "\n",
        "    if isinstance(outputs, torch.Tensor):\n",
        "        outputs = outputs.detach().cpu().numpy()\n",
        "\n",
        "    return outputs\n",
        "```\n",
        "\n",
        "Imagine if we coded our `xywhn2xyxy` as such, where we hard coded a conversion of `outputs` to `numpy`. Then the user will experience errors down stream.\n",
        "\n",
        "```python\n",
        "xywhn_tensor = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
        "xyxy_tensor = yolo2voc(yolo)\n",
        "\n",
        "unsqueeze_voc = voc.unsqueeze(0)\n",
        "```\n",
        "\n",
        "The user trusted that when he passed in an input of type `torch.Tensor`, the `outputs` he get will also be the same type, in which he performed an operation `unsqueeze`, unique to torch. An error ensues, since now his `yolo` became a `np.ndarray` instead.\n",
        "\n",
        "Things get a bit more complicated when I also want to check that the input dimension is the same as the output dimension. For example, if I pass in a 3d-array as input, I expect the same dimension for its outputs. ***Our assert statement above does not check this.***\n",
        "\n",
        "This is where PyTests come in "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Writing our tests\n",
        "\n",
        "This section will be written into `tests/test_bbox_utils_before_refactor.py`. "
      ],
      "metadata": {
        "id": "x-3hbAy6vHzn"
      },
      "id": "x-3hbAy6vHzn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Global Variables"
      ],
      "metadata": {
        "id": "H7xAkfAeDhDY"
      },
      "id": "H7xAkfAeDhDY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start first by importing the libraries and define some global constants."
      ],
      "metadata": {
        "id": "ACPg5l4_wCZd"
      },
      "id": "ACPg5l4_wCZd"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/test_bbox_utils_before_refactor.py\n",
        "import sys\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import numpy.testing as npt\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "sys.path.append(\"/content/bbox\")  # append to import properly.\n",
        "from src.bbox_utils import clone, xywhn2xyxy, xyxy2xywhn\n",
        "\n",
        "# tolerance to assert_allclose\n",
        "ATOL, RTOL = 1e-4, 1e-07\n",
        "\n",
        "# image width and height\n",
        "HEIGHT, WIDTH = 480, 640\n",
        "\n",
        "xyxy = [98, 345, 420, 462]\n",
        "xywhn = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "GT_BBOXES = {\"xyxy\": xyxy, \"xywhn\": xywhn}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h42NjuOaBNtJ",
        "outputId": "5b8f3782-ce80-4357-a04d-2a2b3ff1a62f"
      },
      "id": "h42NjuOaBNtJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bbox/tests/test_bbox_utils_before_refactor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In `line 14`, we defined the tolerance level for `allclose`, where we allow some slight numerical differences.\n",
        "- In `lines 17` we defined the height and width of the image.\n",
        "- In `lines 19-20`, we defined our ground truth values.\n",
        "- In `line 22`, we defined a global variable `GT_BBOXES`, a dictionary that holds the **bounding box format name** as key and its **ground truth** as values. Note that the ground truth values are equivalent in their own format."
      ],
      "metadata": {
        "id": "TK_eOnB1B2--"
      },
      "id": "TK_eOnB1B2--"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parametrize Input Types"
      ],
      "metadata": {
        "id": "dfD_ZHfqDn5Y"
      },
      "id": "dfD_ZHfqDn5Y"
    },
    {
      "cell_type": "markdown",
      "id": "0lk6LZ8StJDM",
      "metadata": {
        "id": "0lk6LZ8StJDM"
      },
      "source": [
        "We now want to test the correctness of the transformation functions we wrote.\n",
        "\n",
        "Recall earlier we used \n",
        "\n",
        "```python\n",
        "assert np.allclose(xyxy2xywhn(xyxy, height=480, width=640), xywhn, rtol=1e-05, atol=1e-08)\n",
        "```\n",
        "\n",
        "to test the correctness. This assumes that our input type is of `np.ndarray`, but since our functions allow `torch.Tensor` as well, we need to ensure that our test function can accept two types of input type, `np.ndarray` and `torch.Tensor`, and still work.\n",
        "\n",
        "So for one transform function `xyxy2xywhn`, we need to test it ***twice***, one for which the input data type is a `np.ndarray`, the other when it's a `torch.Tensor`. This means we need to write more assertions!\n",
        "\n",
        "Fortunately, as we have seen in the [pytest documentation](https://docs.pytest.org/en/6.2.x/parametrize.html), the decorator `pytest.mark.parametrize` does just that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile --append {TEST_DIR}/test_bbox_utils_before_refactor.py\n",
        "\n",
        "def list2numpy(input_list):\n",
        "    return np.asarray(input_list)\n",
        "\n",
        "def list2torch(input_list):\n",
        "    return torch.tensor(input_list)\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "def test_correct_transformation_xyxy2xywhn(convert_type):\n",
        "    \"\"\"Test correctness of conversion from VOC to YOLO.\"\"\"\n",
        "    from_bbox = convert_type(GT_BBOXES[\"xyxy\"])\n",
        "    to_bbox = xyxy2xywhn(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = convert_type(GT_BBOXES[\"xywhn\"])\n",
        "    \n",
        "    if isinstance(to_bbox, torch.Tensor):\n",
        "        torch.testing.assert_close(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)\n",
        "    else:\n",
        "        npt.assert_allclose(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyB-wZ27wBtV",
        "outputId": "5b6447b3-59f0-421f-c869-9ad0beddc495"
      },
      "id": "VyB-wZ27wBtV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_iG4Q0HFneKV",
      "metadata": {
        "id": "_iG4Q0HFneKV"
      },
      "source": [
        "- `lines 3-7` consists of two utility functions, `list2numpy` and `list2torch`, which converts the ground truth bounding box inputs to either `numpy` or `torch` (note that the ground truth is created as a `list` so that the conversion is easy). \n",
        "\n",
        "- `line 9` defines the `pytest.mark.parametrize` decorator where\n",
        "    - the *first argument* is a comma-delimited string of parameter names, this string will be the argument names in the function that follows. Here I named it `\"convert_type\"`;\n",
        "    - the *second argument* will define what *values* the *first argument* can take on. This argument has type `List[Tuple[Any]]` or `List[Any]` or even single values `Any`. In our example, our first argument `convert_type` can take on values of either `list2numpy` or `list2torch`, so we should populate the second argument as a list of two elements: `[list2numpy, list2torch]`. \n",
        "\n",
        "- `line 10` is our function name `test_correct_transformation_xyxy2xywhn` and as the name suggests, it will test whether our conversion of `voc` to `yolo` is correct. Note that the argument is named `convert_type`, corresponding exactly to our *first argument* in the decorator.\n",
        "\n",
        "- `line 12` is where we apply our argument `convert_type` to the input `GT_BBOXES[\"voc\"] = [98, 345, 420, 462]`, the `parametrize` decorator will then apply `list2numpy` and `list2torch` this input and convert the `list` to a `np.ndarray` and `torch.Tensor` respectively. We name this input `from_bbox`.\n",
        "\n",
        "- `line 13` will then convert the input using our function `voc2yolo` to its yolo equivalent format. We name this converted input `to_bbox`.\n",
        "\n",
        "- `line 15` gets the ground truth for yolo. Note I need to convert them into the same type as the input ground truth using `convert_type`. We name this variable `expected_bbox`.\n",
        "\n",
        "- `line 17-20` will then check if our converted bounding box input `to_bbox` matches the ground truth for yolo `expected_bbox` using `numpy.testing` and `torch.testing`.\n",
        "\n",
        "The process does not stop here, since we passed in two values for the function `convert_type`, it will also loop through the `list2torch` step. We will see it in action now by running the `pytest` command.\n",
        "\n",
        "Let's run `pytest` and see what happens.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v tests/test_bbox_utils_before_refactor.py -s       # tests for a single file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZqFHSdBDP1f",
        "outputId": "0d5acd96-ebdb-4062-8661-50e1a17d5d3a"
      },
      "id": "1ZqFHSdBDP1f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/bbox, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 2 passed in 0.62 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see\n",
        "\n",
        "```bash\n",
        "$ tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2numpy] PASSED\n",
        "$ tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2torch] PASSED\n",
        "```\n",
        "\n",
        "means that the test function has tested for both combinations, the case where the input is a `np.ndarray` and when it is a `torch.Tensor`, both passed the assertion!"
      ],
      "metadata": {
        "id": "tjSLj2wWHkJf"
      },
      "id": "tjSLj2wWHkJf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As important thing to realize here is that we are only testing the ***correctness*** of this transformation. It does not test whether our transformation functions ensure the same return type as the inputs. \n",
        "For that, we need to write a test to ensure the outputs has the same type as the inputs."
      ],
      "metadata": {
        "id": "Z1_92OQUaCGb"
      },
      "id": "Z1_92OQUaCGb"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile --append {TEST_DIR}/test_bbox_utils_before_refactor.py\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "def test_correct_return_type_xyxy2xywhn(convert_type):\n",
        "    from_bbox = convert_type(GT_BBOXES[\"xyxy\"])\n",
        "    to_bbox = xyxy2xywhn(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    assert isinstance(to_bbox, type(from_bbox))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuL2Oy1hKK_c",
        "outputId": "8c5ba677-93e7-428a-86c7-a736e3893f50"
      },
      "id": "BuL2Oy1hKK_c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v tests/test_bbox_utils_before_refactor.py -s       # tests for a single file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp3WTJCXKo5l",
        "outputId": "072f8650-12ee-40a0-ba03-de9bfcb97b5a"
      },
      "id": "wp3WTJCXKo5l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/bbox, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 4 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn[list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_return_type_xyxy2xywhn[list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_return_type_xyxy2xywhn[list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 4 passed in 0.65 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are sure that the `inputs` and `outputs` of our functions are of the same type!"
      ],
      "metadata": {
        "id": "Cr58Yl9YIGYx"
      },
      "id": "Cr58Yl9YIGYx"
    },
    {
      "cell_type": "markdown",
      "id": "pL74NDQ4whwu",
      "metadata": {
        "id": "pL74NDQ4whwu"
      },
      "source": [
        "#### Parametrize Consistent Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_0yfucxpwxit",
      "metadata": {
        "id": "_0yfucxpwxit"
      },
      "source": [
        "Our next step is to test that our transform functions can handle different dimensions. Whether the input is a 3d-tensor, or a 10d-array, all of them should work. Of course, our main goal here is still to test the correctness of the transformation, but bear in mind we need to have a separate test to check the consistency of input and output dimensions (i.e. passing in a 2d-array will result in an output of 2d-array).\n",
        "\n",
        "Let's say we want to test if the code works for 3 dimensions, means checking if the code can execute correctly without error for dimensions in `[1d, 2d, 3d]`.\n",
        "\n",
        "This is not trivial as we need to check for 6 different cases, a result of the cartesian product of\n",
        "\n",
        "```\n",
        "[list2numpy, list2torch] x [0, 1, 2] = {(list2numpy, 0), (list2numpy, 1), ...}\n",
        "```\n",
        "\n",
        "a total of 6 combinations.\n",
        "\n",
        "We will continue to leverage `pytest`'s parametrize to test all 6 cases.\n",
        "\n",
        "There will not be much change besides defining an extra utility function `expand_dim` will expands the input's dimensions according to the `num_dims` argument.\n",
        "\n",
        "To be able to use the cartesian product, we simply add one more decorator below our `convert_type`, in which case it now takes in `num_dims` as first argument, and `[0, 1, 2]` as the second, indicating that we want the function to test for the aforementioned 3 dimensions. Having two parametrize decorators stacked together means it will execute in combination, exactly as what we wanted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TnFz8zv95eJK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnFz8zv95eJK",
        "outputId": "07d181ee-bdd0-45d6-9ee0-4a1eb4c163cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile --append {TEST_DIR}/test_bbox_utils_before_refactor.py\n",
        "\n",
        "def expand_dim(\n",
        "    bboxes: Union[np.ndarray, torch.Tensor],\n",
        "    num_dims: int,\n",
        ") -> Union[np.ndarray, torch.Tensor]:\n",
        "    \"\"\"Expand the dimension of bboxes to num_dims.\n",
        "\n",
        "    Note:\n",
        "        np.expand_dims will not work for tuple dim numpy < 1.18.0 which\n",
        "        is not the version in our cicd.\n",
        "    \"\"\"\n",
        "    bboxes = clone(bboxes)\n",
        "    return bboxes[(None,) * num_dims]\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "@pytest.mark.parametrize(\"num_dims\", [0, 1, 2])\n",
        "def test_correct_transformation_xyxy2xywhn_with_dims(convert_type, num_dims):\n",
        "    \"\"\"Test conversion from VOC to YOLO.\"\"\"\n",
        "    from_bbox = convert_type(GT_BBOXES[\"xyxy\"])\n",
        "    from_bbox = expand_dim(from_bbox, num_dims)\n",
        "\n",
        "    to_bbox = xyxy2xywhn(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = expand_dim(convert_type(GT_BBOXES[\"xywhn\"]), num_dims)\n",
        "    \n",
        "    if isinstance(to_bbox, torch.Tensor):\n",
        "        torch.testing.assert_close(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)\n",
        "    else:\n",
        "        npt.assert_allclose(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IYQpDbf55eLk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYQpDbf55eLk",
        "outputId": "72365477-b3e4-4333-a16b-fe39ab8b2bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/bbox, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 6 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[0-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[0-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[1-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[1-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[2-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims[2-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 6 passed in 0.62 seconds ===========================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -v tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_dims -s       # tests for a single file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mk2SQRujy_1p",
      "metadata": {
        "id": "Mk2SQRujy_1p"
      },
      "source": [
        "We see that a total of 6 results were tested by `test_correct_transformation_xyxy2xywhn_with_dims`. \n",
        "\n",
        "Notice that in each line they indicate the combination, for example, the first line says `test_voc2yolo[0-list2numpy] PASSED`, which means they tested for the case of `0` dimensions and the input type of `np.ndarray`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also test whether the input dimensions must match the output dimensions. But since `np.testing.assert_allclose` will raise an error if their shape mismatch, we will skip over this. It may still be good practice to write test as we typically want one test to handle one type of error.\n",
        "\n",
        "```python\n",
        "a = np.asarray([1,2,3])\n",
        "b = np.asarray([[1,2,3][)\n",
        "\n",
        "np.testing.assert_allclose(a, b) -> raises an error\n",
        "```"
      ],
      "metadata": {
        "id": "mImzoYP3NkHF"
      },
      "id": "mImzoYP3NkHF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Fixtures to Manage State and Dependencies"
      ],
      "metadata": {
        "id": "DWVzqv_6ZdK4"
      },
      "id": "DWVzqv_6ZdK4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea of fixtures is that if you have multiple test functions that take in a \"fixed\" set of inputs (i.e. `GT_BBOXES`), then we should consider using fixtures.\n",
        "\n",
        "In our past 2 test functions, `test_correct_return_type_xyxy2xywhn` and `test_correct_transformation_xyxy2xywhn`, we both used the same set of global constant `GT_BBOXES`, we can imagine now that for our testing on `xywhn2xyxy`, we would use this variable again.\n",
        "\n",
        "For that we can write a function `gt_bboxes` that has `pytest.fixture` as decorator. Subsequently, we can pass `gt_bboxes` to any test functions.\n",
        "\n",
        "For example,\n",
        "\n",
        "```python\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def gt_bboxes():\n",
        "    return GT_BBOXES\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "@pytest.mark.parametrize(\"num_dims\", [0, 1, 2])\n",
        "def test_voc2yolo(gt_bboxes, convert_type, num_dims):\n",
        "    from_bbox = convert_type(gt_bboxes[\"xyxy\"])\n",
        "    ...\n",
        "```\n",
        "\n",
        "where we simply defined a new fixture function called `gt_bboxes`, decorated with `pytest.fixture(scope=\"module\")`. The [scopes](https://betterprogramming.pub/understand-5-scopes-of-pytest-fixtures-1b607b5c19ed) defines how frequent your fixture is called, expensive operations often require a higher scope.\n",
        "\n",
        "However, the same result can be achieved with our old method, defining a global constant `GT_BBOXES` work as well. This is true in our case because `GT_BBOXES` is a relative cheap operation. But imagine if creating our `GT_BBOXES` is an expensive operation (i.e. `GT_BBOXES` involves creating a 10,000 by 10,000 array), then having it defined under a fixture with a proper scope is important.\n",
        "\n",
        "For our purpose, imagine a scenario where we have 3 test files `test_1.py, test_2.py, test_3.py` which all need `GT_BBOXES`, then we can now put this fixture in a file called `conftest.py`, with `scope` to be defined at a modular level, when running your test suites across all 3 files, `gt_bboxes` will be called once and re-use it across all functions in these 3 files."
      ],
      "metadata": {
        "id": "Mb-p9gtD0b2x"
      },
      "id": "Mb-p9gtD0b2x"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/conftest.py\n",
        "import pytest \n",
        "\n",
        "xyxy = [98, 345, 420, 462]\n",
        "xywhn = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "GT_BBOXES = {\"xyxy\": xyxy, \"xywhn\": xywhn}\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def gt_bboxes():\n",
        "    return GT_BBOXES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x98_dBoeS_Jo",
        "outputId": "0a9bd7ad-a7d4-4734-a300-c17167e9ebd4"
      },
      "id": "x98_dBoeS_Jo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bbox/tests/conftest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile --append {TEST_DIR}/test_bbox_utils_before_refactor.py\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "@pytest.mark.parametrize(\"num_dims\", [0, 1, 2])\n",
        "def test_correct_transformation_xyxy2xywhn_with_fixture(gt_bboxes, convert_type, num_dims):\n",
        "    \"\"\"Test conversion from VOC to YOLO.\"\"\"\n",
        "    from_bbox = convert_type(gt_bboxes[\"xyxy\"])\n",
        "    from_bbox = expand_dim(from_bbox, num_dims)\n",
        "\n",
        "    to_bbox = xyxy2xywhn(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = expand_dim(convert_type(gt_bboxes[\"xywhn\"]), num_dims)\n",
        "    \n",
        "    if isinstance(to_bbox, torch.Tensor):\n",
        "        torch.testing.assert_close(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)\n",
        "    else:\n",
        "        npt.assert_allclose(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c353eExgTQvf",
        "outputId": "f62f4562-d1dd-4343-981a-5b5abc047f61"
      },
      "id": "c353eExgTQvf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/bbox/tests/test_bbox_utils_before_refactor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now run the test on this function we created `test_correct_transformation_xyxy2xywhn_with_fixture`, the only difference with `test_correct_transformation_xyxy2xywhn` is that it now takes in an argument `gt_bboxes` corresponding to the fixture name, all instances of the global variable `GT_BBOXES` are changed to `gt_bboxes`."
      ],
      "metadata": {
        "id": "SqbzmnmhV-qd"
      },
      "id": "SqbzmnmhV-qd"
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture -s       # tests for a single file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdYhAFrLTVTR",
        "outputId": "970edfe6-dc95-4726-fd5d-b56a91d1f15d"
      },
      "id": "YdYhAFrLTVTR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/bbox, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 6 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[0-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[0-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[1-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[1-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[2-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_before_refactor.py::test_correct_transformation_xyxy2xywhn_with_fixture[2-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 6 passed in 0.63 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no need to import `conftest` into any of our tests, `pytest` handles it for us."
      ],
      "metadata": {
        "id": "BNEElez1JH5W"
      },
      "id": "BNEElez1JH5W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Refactoring our tests"
      ],
      "metadata": {
        "id": "e1M4D_fnOXv4"
      },
      "id": "e1M4D_fnOXv4"
    },
    {
      "cell_type": "markdown",
      "id": "HHRv-T1N08R5",
      "metadata": {
        "id": "HHRv-T1N08R5"
      },
      "source": [
        "#### Using parametrize for different transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13tdUAx71A9Y",
      "metadata": {
        "id": "13tdUAx71A9Y"
      },
      "source": [
        "Now I actually have more than 10 transform functions that converts bounding boxes. This means I have to write 10 cases: `test_xyxy2xywhn`, `test_xywhn2xyxy`, ...\n",
        "\n",
        "Most of the code inside the test functions are the same.\n",
        "\n",
        "We can again leverage on parametrize and define the first argument to be `conversion_name` which takes on values such as `[\"xyxy2xywhn\", \"xywhn2xyxy\"]`. These values will then be our identifier on which conversion/transform to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For that, we need to revamp our fixture `gt_bboxes`."
      ],
      "metadata": {
        "id": "u9h9y9AnbOPm"
      },
      "id": "u9h9y9AnbOPm"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/conftest.py\n",
        "import sys\n",
        "import pytest \n",
        "\n",
        "sys.path.append(\"/content/bbox\")  # append to import properly.\n",
        "from src.bbox_utils import xywhn2xyxy, xyxy2xywhn\n",
        "\n",
        "xyxy = [98, 345, 420, 462]\n",
        "xywhn = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def gt_bboxes():\n",
        "    return {\"xyxy2xywhn\": [xyxy, xywhn, xyxy2xywhn], \"xywhn2xyxy\": [xywhn, xyxy, xywhn2xyxy]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPSCBPRpbU5W",
        "outputId": "14222719-a0ad-402b-eaf2-430edf4f7976"
      },
      "id": "EPSCBPRpbU5W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bbox/tests/conftest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The changes are:\n",
        "\n",
        "- The key is now exact name of the function call of the transformation. This means that if our transform function is `xyxy2xywhn`, then our key is called `\"xyxy2xywhn\"`. \n",
        "- The corresponding value is a list, where \n",
        "    - the first element is the ground truth of the input (i.e. `xyxy`), \n",
        "    - and the second element is the ground truth of the output after the conversion (i.e. `xywhn`),\n",
        "    - the third element is the function call itself."
      ],
      "metadata": {
        "id": "qVfvAofdbWSz"
      },
      "id": "qVfvAofdbWSz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AzkrCgjtwf6M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzkrCgjtwf6M",
        "outputId": "69301f33-4d16-4aba-a8b3-631669e52d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/bbox/tests/test_bbox_utils_after_refactor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {TEST_DIR}/test_bbox_utils_after_refactor.py\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import numpy.testing as npt\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "from src.bbox_utils import clone\n",
        "\n",
        "CONVERT_TYPES = [np.array, torch.tensor]\n",
        "NUM_DIMS = [0, 1, 2]\n",
        "TRANSFORMS = [\"xyxy2xywhn\", \"xywhn2xyxy\"]\n",
        "\n",
        "# tolerance to assert_allclose\n",
        "ATOL, RTOL = 1e-4, 1e-07\n",
        "\n",
        "# image height and width\n",
        "HEIGHT, WIDTH = 480, 640\n",
        "\n",
        "def list2numpy(input_list):\n",
        "    return np.asarray(input_list)\n",
        "\n",
        "def list2torch(input_list):\n",
        "    return torch.tensor(input_list)\n",
        "\n",
        "def expand_dim(\n",
        "    bboxes: Union[np.ndarray, torch.Tensor],\n",
        "    num_dims: int,\n",
        ") -> Union[np.ndarray, torch.Tensor]:\n",
        "    \"\"\"Expand the dimension of bboxes to num_dims.\n",
        "\n",
        "    Note:\n",
        "        np.expand_dims will not work for tuple dim numpy < 1.18.0 which\n",
        "        is not the version in our cicd.\n",
        "    \"\"\"\n",
        "    bboxes = clone(bboxes)\n",
        "    return bboxes[(None,) * num_dims]\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", CONVERT_TYPES)\n",
        "class TestBboxTransforms:\n",
        "    @pytest.mark.parametrize(\"conversion_name\", TRANSFORMS)\n",
        "    def test_correct_return_type(self, gt_bboxes, convert_type, conversion_name):\n",
        "        from_bbox, _, conversion_fn = gt_bboxes[conversion_name]\n",
        "        from_bbox = convert_type(from_bbox)\n",
        "\n",
        "        to_bbox = conversion_fn(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "        assert isinstance(to_bbox, type(from_bbox))\n",
        "\n",
        "    @pytest.mark.parametrize(\"num_dims\", NUM_DIMS)\n",
        "    @pytest.mark.parametrize(\"conversion_name\", TRANSFORMS)\n",
        "    def test_correct_transformation(\n",
        "        self, gt_bboxes, convert_type, num_dims, conversion_name\n",
        "    ):\n",
        "        from_bbox, expected_bbox, conversion_fn = gt_bboxes[conversion_name]\n",
        "        from_bbox = convert_type(from_bbox)\n",
        "        from_bbox = expand_dim(from_bbox, num_dims)\n",
        "\n",
        "        to_bbox = conversion_fn(from_bbox, height=HEIGHT, width=WIDTH)  \n",
        "\n",
        "        expected_bbox = expand_dim(convert_type(expected_bbox), num_dims)\n",
        "\n",
        "        if isinstance(to_bbox, torch.Tensor):\n",
        "            torch.testing.assert_allclose(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)\n",
        "        else:\n",
        "            npt.assert_allclose(to_bbox, expected_bbox, atol=ATOL, rtol=RTOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rJAsVPin45yY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJAsVPin45yY",
        "outputId": "3d236869-1ebe-481e-fd74-ee2d38855499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/bbox, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollecting 16 items                                                            \u001b[0m\u001b[1m\rcollected 16 items                                                             \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xyxy2xywhn-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xyxy2xywhn-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xywhn2xyxy-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_return_type[xywhn2xyxy-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-0-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-0-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-1-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-1-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-2-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xyxy2xywhn-2-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-0-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-0-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-1-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-1-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-2-convert_type0] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils_after_refactor.py::TestBboxTransforms::test_correct_transformation[xywhn2xyxy-2-convert_type1] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m========================== 16 passed in 0.04 seconds ===========================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest -W ignore -v tests/test_bbox_utils_after_refactor.py -s       # tests for a single file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iOmKxGJt5Cj9",
      "metadata": {
        "id": "iOmKxGJt5Cj9"
      },
      "source": [
        "We see that we achieved the same results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References & Citations"
      ],
      "metadata": {
        "id": "iY5LO0HuWlvG"
      },
      "id": "iY5LO0HuWlvG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://madewithml.com/courses/mlops/testing/\n",
        "\n",
        "    ```\n",
        "    @misc{goku mohandas_2020, title={Testing Machine Learning Systems: Code, Data and Models - Made With ML}, url={https://madewithml.com/courses/mlops/testing/}, journal={Madewithml.com}, author={Goku Mohandas}, year={2020} }\n",
        "    ```\n",
        "‌\n",
        "- https://realpython.com/pytest-python-testing/\n",
        "\n",
        "    ```\n",
        "    @misc{real python_2022, title={Effective Python Testing With Pytest}, url={https://realpython.com/pytest-python-testing/}, journal={Realpython.com}, publisher={Real Python}, author={Real Python}, year={2022}, month={Jun} }\n",
        "    ```\n"
      ],
      "metadata": {
        "id": "pQfDNcACWnXT"
      },
      "id": "pQfDNcACWnXT"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv_mlops': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "fee21ade325d7bec1bc496fe52256a16c1a1d317ab64df50f935c606415019bc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
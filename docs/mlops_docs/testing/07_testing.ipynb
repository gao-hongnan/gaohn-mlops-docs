{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pytest\n",
        "import torch"
      ],
      "metadata": {
        "id": "zzbCCcRIn-eP"
      },
      "id": "zzbCCcRIn-eP",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intuition\n",
        "\n",
        "Tests are a way for us to ensure that something works as intended. We're incentivized to implement tests and discover sources of error as early in the development cycle as possible so that we can reduce [increasing downstream costs](https://assets.deepsource.io/39ed384/images/blog/cost-of-fixing-bugs/chart.jpg) and wasted time. Once we've designed our tests, we can automatically execute them every time we implement a change to our system and continue to build on them over time. In this lesson, we'll learn how to test machine learning code, data and models to construct a system that we can reliably iterate on."
      ],
      "metadata": {
        "id": "lsrQ8EeNvwQ7"
      },
      "id": "lsrQ8EeNvwQ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of tests\n",
        "\n",
        "There are many four majors types of tests which are utilized at different points in the development cycle:\n",
        "\n",
        "- Unit tests: tests on individual components that each have a single responsibility (ex. function that filters a list).\n",
        "- Integration tests: tests on the combined functionality of individual components (ex. data processing).\n",
        "- System tests: tests on the design of a system for expected outputs given inputs (ex. training, inference, etc.).\n",
        "- Acceptance tests: tests to verify that requirements have been met, usually referred to as User Acceptance Testing (UAT).\n",
        "- Regression tests: testing errors we've seen before to ensure new changes don't reintroduce them."
      ],
      "metadata": {
        "id": "tt2JiOX4vxDb"
      },
      "id": "tt2JiOX4vxDb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How should we test?\n",
        "\n",
        "The framework to use when composing tests is the [Arrange Act Assert methodology](http://wiki.c2.com/?ArrangeActAssert).\n",
        "\n",
        "- Arrange: set up the different inputs to test on.\n",
        "- Act: apply the inputs on the component we want to test.\n",
        "- Assert: confirm that we received the expected output."
      ],
      "metadata": {
        "id": "OE4Nl89ewblO"
      },
      "id": "OE4Nl89ewblO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What should we be testing for?\n",
        "\n",
        "An example:\n",
        "\n",
        "> When arranging our inputs and asserting our expected outputs, what are some aspects of our inputs and outputs that we should be testing for?\n",
        "\n",
        "- inputs: data types, format, length, edge cases (min/max, small/large, etc.)\n",
        "- outputs: data types, formats, exceptions, intermediary and final outputs"
      ],
      "metadata": {
        "id": "nbuDcZ86wurM"
      },
      "id": "nbuDcZ86wurM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best practices\n",
        "\n",
        "Regardless of the framework we use, it's important to strongly tie testing into the development process.\n",
        "\n",
        "- atomic: when creating unit components, we need to ensure that they have a [single responsibility](https://en.wikipedia.org/wiki/Single-responsibility_principle) so that we can easily test them. If not, we'll need to split them into more granular units.\n",
        "\n",
        "- compose: when we create new components, we want to compose tests to validate their functionality. It's a great way to ensure reliability and catch errors early on.\n",
        "\n",
        "- regression: we want to account for new errors we come across with a regression test so we can ensure we don't reintroduce the same errors in the future.\n",
        "\n",
        "- coverage: we want to ensure that 100% of our codebase has been accounter for. This doesn't mean writing a test for every single line of code but rather accounting for every single line (more on this in the coverage section below).\n",
        "\n",
        "- automate: in the event we forget to run our tests before committing to a repository, we want to auto run tests for every commit. We'll learn how to do this locally using pre-commit hooks and remotely (ie. main branch) via GitHub actions in subsequent lessons."
      ],
      "metadata": {
        "id": "ooFZczxxxQ6v"
      },
      "id": "ooFZczxxxQ6v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test-driven development or Otherwise?\n",
        "\n",
        "[Test-driven development (TDD)](https://en.wikipedia.org/wiki/Test-driven_development) is the process where you write a test before completely writing the functionality to ensure that tests are always written. This is in contrast to writing functionality first and then composing tests afterwards. Here are my thoughts on this:\n",
        "\n",
        "- good to write tests as we progress, but it's not the representation of correctness.\n",
        "\n",
        "- initial time should be spent on design before ever getting into the code or tests.\n",
        "\n",
        "- using a test as guide doesn't mean that our functionality is error free.\n",
        "\n",
        "Perfect coverage doesn't mean that our application is error free if those tests aren't meaningful and don't encompass the field of possible inputs, intermediates and outputs. Therefore, we should work towards better design and agility when facing errors, quickly resolving them and writing test cases around them to avoid them next time.\n"
      ],
      "metadata": {
        "id": "aXXTyZ9zxdg0"
      },
      "id": "aXXTyZ9zxdg0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytest\n",
        "\n",
        "We're going to be using [pytest](https://docs.pytest.org/en/stable/) as our testing framework for it's powerful builtin features such as parametrization, fixtures, markers, etc."
      ],
      "metadata": {
        "id": "2A3cbTZEyMyp"
      },
      "id": "2A3cbTZEyMyp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration\n",
        "\n",
        "Pytest expects tests to be organized under a `tests` directory by default. However, we can also use our `pyproject.toml` file to configure any other test path directories as well. Once in the directory, pytest looks for python scripts starting with `tests_*.py` but we can configure it to read any other file patterns as well.\n",
        "\n",
        "```toml\n",
        "# Pytest\n",
        "[tool.pytest.ini_options]\n",
        "testpaths = [\"tests\"]\n",
        "python_files = \"test_*.py\"\n",
        "```"
      ],
      "metadata": {
        "id": "cCkQDIn2yYc5"
      },
      "id": "cCkQDIn2yYc5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assertions\n",
        "\n",
        "Simple assertion testing example."
      ],
      "metadata": {
        "id": "eYibqMWyyYfU"
      },
      "id": "eYibqMWyyYfU"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Creating Directories\n",
        "BASE_DIR = Path(\"__file__\").parent.absolute()\n",
        "\n",
        "SRC_DIR = Path.joinpath(BASE_DIR, \"src\")\n",
        "TEST_DIR = Path.joinpath(BASE_DIR, \"tests\")\n",
        "SRC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEST_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "JZRGRMjz0ksw"
      },
      "id": "JZRGRMjz0ksw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {BASE_DIR}/pyproject.toml\n",
        "# Pytest\n",
        "[tool.pytest.ini_options]\n",
        "testpaths = [\"tests\"]\n",
        "python_files = \"test_*.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBT2pW0U1El9",
        "outputId": "03ccfc02-7542-4187-ff76-3146b0f3182b"
      },
      "id": "tBT2pW0U1El9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/pyproject.toml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {SRC_DIR}/__init__.py\n",
        "\"init file\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj94RxXL2R5R",
        "outputId": "87730208-691b-46f9-edb6-6e8d84af91ff"
      },
      "id": "pj94RxXL2R5R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/src/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {SRC_DIR}/fruits.py\n",
        "def is_crisp(fruit):\n",
        "    if fruit:\n",
        "        fruit = fruit.lower()\n",
        "    if fruit in [\"apple\", \"watermelon\", \"cherries\"]:\n",
        "        return True\n",
        "    elif fruit in [\"orange\", \"mango\", \"strawberry\"]:\n",
        "        return False\n",
        "    else:\n",
        "        raise ValueError(f\"{fruit} not in known list of fruits.\")\n",
        "    return False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6QVf7Vt1Hej",
        "outputId": "c4b1f4ff-2eef-4065-eaed-a77dbe9f280c"
      },
      "id": "z6QVf7Vt1Hej",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/src/fruits.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/test_fruits.py\n",
        "import pytest\n",
        "import sys \n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.fruits import is_crisp\n",
        "def test_is_crisp():\n",
        "    assert is_crisp(fruit=\"apple\") #  or == True\n",
        "    assert is_crisp(fruit=\"Apple\")\n",
        "    assert not is_crisp(fruit=\"orange\")\n",
        "    with pytest.raises(ValueError):\n",
        "        is_crisp(fruit=None)\n",
        "        is_crisp(fruit=\"pear\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI0huQ400d-C",
        "outputId": "292c5a1b-200b-4274-87a7-7b66c8246e35"
      },
      "id": "GI0huQ400d-C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tests/test_fruits.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest                                      # all tests\n",
        "!pytest tests/                               # tests under a directory\n",
        "!pytest tests/test_fruits.py                 # tests for a single file\n",
        "!pytest tests/test_fruits.py::test_is_crisp  # tests for a single function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9phDnsIz2BT",
        "outputId": "e03980b0-5c28-41ca-cdba-94726da8922e"
      },
      "id": "K9phDnsIz2BT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_fruits.py .\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classes\n",
        "\n",
        "See [examples from madewithml repo](https://github.com/GokuMohandas/follow/blob/testing/tests/tagifai/test_data.py) to understand better."
      ],
      "metadata": {
        "id": "IRAyAp-W3TuM"
      },
      "id": "IRAyAp-W3TuM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interfaces\n",
        "\n",
        "See [madewithml interface section](https://madewithml.com/courses/mlops/testing/#interfaces)."
      ],
      "metadata": {
        "id": "golDlmT04pj8"
      },
      "id": "golDlmT04pj8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametrize\n",
        "\n",
        "So far, in our tests, we've had to create individual assert statements to validate different combinations of inputs and expected outputs. However, there's a bit of redundancy here because the inputs always feed into our functions as arguments and the outputs are compared with our expected outputs. To remove this redundancy, pytest has the [`@pytest.mark.parametrize`](https://docs.pytest.org/en/stable/parametrize.html) decorator which allows us to represent our inputs and outputs as parameters."
      ],
      "metadata": {
        "id": "sa6WwNTC4rOx"
      },
      "id": "sa6WwNTC4rOx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create a new python file `test_fruits_parametrize.py` to test it out."
      ],
      "metadata": {
        "id": "RRC-cuyQ6jcf"
      },
      "id": "RRC-cuyQ6jcf"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/test_fruits_parametrize.py\n",
        "import pytest\n",
        "import sys \n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.fruits import is_crisp\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"fruit, crisp\",\n",
        "    [\n",
        "        (\"apple\", True),\n",
        "        (\"Apple\", True),\n",
        "        (\"orange\", False),\n",
        "    ],\n",
        ")\n",
        "def test_is_crisp_parametrize(fruit, crisp):\n",
        "    assert is_crisp(fruit=fruit) == crisp\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"fruit, exception\",\n",
        "    [\n",
        "        (\"pear\", ValueError),\n",
        "    ],\n",
        ")\n",
        "def test_is_crisp_exceptions(fruit, exception):\n",
        "    with pytest.raises(exception):\n",
        "        is_crisp(fruit=fruit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZTBT3dWzyjI",
        "outputId": "7f851b5e-0cd7-46fc-bee4-874fc67b993a"
      },
      "id": "nZTBT3dWzyjI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tests/test_fruits_parametrize.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix line number, but for now the line number starts from the decorator `@pytest.mark.parametrize`.\n",
        "\n",
        "- [Line 2]: define the names of the parameters under the decorator, ex. \"fruit, crisp\" (note that this is one string). Note that this string names should correspond to the function defined under the decorator.\n",
        "\n",
        "- [Lines 3-7]: provide a list of combinations of values for the parameters from Step 1.\n",
        "\n",
        "- [Line 9]: pass in parameter names to the test function.\n",
        "\n",
        "- [Line 10]: include necessary assert statements which will be executed for each of the combinations in the list from Step 2.\n",
        "\n",
        "- [Line 12-20]: this tests exception handling as well if you pass in as such."
      ],
      "metadata": {
        "id": "5x1_GBJ26u2L"
      },
      "id": "5x1_GBJ26u2L"
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest tests/test_fruits_parametrize.py  # tests for a single function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HBrPbTqz77u",
        "outputId": "fcdb75e0-99a7-4bed-ff0e-2d64b6f97f16"
      },
      "id": "4HBrPbTqz77u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_fruits_parametrize.py ....\u001b[36m                                    [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 4 passed in 0.01 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixtures\n",
        "\n",
        "[What's the benefits of using fixtures?](https://realpython.com/pytest-python-testing/#fixtures-managing-state-and-dependencies)\n",
        "\n",
        "One obvious reason that I know of is about reducing the redundancies of re-defining inputs every time."
      ],
      "metadata": {
        "id": "8sDwlFcpyYhy"
      },
      "id": "8sDwlFcpyYhy"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "def add(nums_list):\n",
        "    return np.sum(nums_list)\n",
        "\n",
        "\n",
        "def mul(nums_list):\n",
        "    return np.prod(nums_list)\n",
        "\n",
        "def test_add():\n",
        "    nums_list = [1, 2, 3, 4, 5]\n",
        "    assert add(nums_list) == 15\n",
        "\n",
        "def test_mul():\n",
        "    nums_list = [1, 2, 3, 4, 5]\n",
        "    assert add(nums_list) == 120"
      ],
      "metadata": {
        "id": "c5VG1gbzONw8"
      },
      "id": "c5VG1gbzONw8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that you defined `nums_list` twice when we want to test different functions with the **same inputs**. So to reduce this redundancy, we can do:"
      ],
      "metadata": {
        "id": "Rp_MjOd3PQ5N"
      },
      "id": "Rp_MjOd3PQ5N"
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_nums_list():\n",
        "    nums_list = [1, 2, 3, 4, 5]\n",
        "    return nums_list "
      ],
      "metadata": {
        "id": "hiyvxI2kPMgJ"
      },
      "id": "hiyvxI2kPMgJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Walkthrough\n",
        "\n",
        "> **The example walkthrough assumes you have a basic understanding of pytests, which can be read from madewithml.com.**"
      ],
      "metadata": {
        "id": "4p6BRfFVyYj-"
      },
      "id": "4p6BRfFVyYj-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have two functions `voc2yolo` and its inverse `yolo2voc`, the former takes in a Pascal-VOC style bounding box and **transforms** it to its equivalent YOLO format, while the latter does the opposite.\n",
        "\n",
        "More concretely, given the ground truth coordinates of \n",
        "\n",
        "```python\n",
        "voc = [98, 345, 420, 462]\n",
        "```\n",
        "\n",
        "we want to transform it to the equivalent YOLO coordinates\n",
        "\n",
        "```python\n",
        "yolo = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "```\n",
        "\n",
        "and vice versa. We also assume the height and width of the image to be `480` and `640` respectively."
      ],
      "metadata": {
        "id": "jp66_v-WyYmE"
      },
      "id": "jp66_v-WyYmE"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "voc = [98, 345, 420, 462]\n",
        "yolo = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "voc = np.asarray(voc)\n",
        "yolo = np.asarray(yolo)"
      ],
      "metadata": {
        "id": "WbYVMLfJ95Iz"
      },
      "id": "WbYVMLfJ95Iz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our transformation functions mainly consists of `voc2yolo` and `yolo2voc` with 2 utility function."
      ],
      "metadata": {
        "id": "53vrA1rc960z"
      },
      "id": "53vrA1rc960z"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "BboxType = Union[np.ndarray, torch.Tensor]\n",
        "\n",
        "def cast_int_to_float(inputs: BboxType) -> BboxType:\n",
        "    if isinstance(inputs, torch.Tensor):\n",
        "        return inputs.float()\n",
        "    return inputs.astype(np.float32)\n",
        "\n",
        "\n",
        "def clone(inputs: BboxType) -> BboxType:\n",
        "    if isinstance(inputs, torch.Tensor):\n",
        "        return inputs.clone()\n",
        "    return inputs.copy()\n",
        "\n",
        "def voc2yolo(inputs: BboxType, height: float, width: float) -> BboxType:\n",
        "    outputs = clone(inputs)\n",
        "    outputs = cast_int_to_float(outputs)\n",
        "\n",
        "    outputs[..., [0, 2]] /= width\n",
        "    outputs[..., [1, 3]] /= height\n",
        "\n",
        "    outputs[..., 2] -= outputs[..., 0]\n",
        "    outputs[..., 3] -= outputs[..., 1]\n",
        "\n",
        "    outputs[..., 0] += outputs[..., 2] / 2\n",
        "    outputs[..., 1] += outputs[..., 3] / 2\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def yolo2voc(inputs: BboxType, height: float, width: float) -> BboxType:\n",
        "    outputs = clone(inputs)\n",
        "    outputs = cast_int_to_float(outputs)\n",
        "\n",
        "    outputs[..., [0, 2]] *= width\n",
        "    outputs[..., [1, 3]] *= height\n",
        "\n",
        "    outputs[..., 0] -= outputs[..., 2] / 2\n",
        "    outputs[..., 1] -= outputs[..., 3] / 2\n",
        "    outputs[..., 2] += outputs[..., 0]\n",
        "    outputs[..., 3] += outputs[..., 1]\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "RwSKGo8x7Ld6"
      },
      "id": "RwSKGo8x7Ld6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can just test the correctness of the transformation (i.e. when I pass in `voc = [98, 345, 420, 462]` to `voc2yolo`, I expect it to output `yolo = [0.4046875, 0.840625, 0.503125, 0.24375]`). \n",
        "\n",
        "Without using any library, we can simply do something like:"
      ],
      "metadata": {
        "id": "Tb9mEKOq7QOq"
      },
      "id": "Tb9mEKOq7QOq"
    },
    {
      "cell_type": "code",
      "source": [
        "assert voc2yolo(voc, height=480, width=640).all() == yolo.all()"
      ],
      "metadata": {
        "id": "FPy6F0CX9b5o"
      },
      "id": "FPy6F0CX9b5o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and for `yolo2voc`, we can repeat:"
      ],
      "metadata": {
        "id": "dGyPnO6l9unZ"
      },
      "id": "dGyPnO6l9unZ"
    },
    {
      "cell_type": "code",
      "source": [
        "assert yolo2voc(yolo, height=480, width=640).all() == voc.all()"
      ],
      "metadata": {
        "id": "B169M0Ph9whR"
      },
      "id": "B169M0Ph9whR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may seem fine, but it is very hard to scale up when you add in more transformations. Furthermore, functions like these often have implicit assumptions that need to be rigourously tested as well.\n",
        "\n",
        "For example, I defined `BboxType = Union[np.ndarray, torch.Tensor]` and type hinted our functions' inputs and outputs to be both of this type. In particular, when a user pass in an array of type `torch.Tensor`, I expect the output to be of the same type as in the input. This is important as many operations performed on `torch.Tensor` does not carry forward to their `np.ndarray` counterpart. ***Our assert statement above does not check this, for all we know, I could maliciously convert our tensor type to numpy array and our code still works.***\n",
        "\n",
        "Things get a bit more complicated when I also want to check that the input dimension is the same as dimension. For example, if I pass in a 3d-array as input, I expect the same dimension for its outputs. ***Our assert statement above does not check this, for all we know, I could maliciously add a statement to squeeze the first dimension of our input during the function body and our code still works.***"
      ],
      "metadata": {
        "id": "PhRno6SY9_VA"
      },
      "id": "PhRno6SY9_VA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we write the tests, let's first create the necessary folders and scripts."
      ],
      "metadata": {
        "id": "jMSLvYi-s8h3"
      },
      "id": "jMSLvYi-s8h3"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Creating Directories\n",
        "BASE_DIR = Path(\"__file__\").parent.absolute()\n",
        "\n",
        "SRC_DIR = Path.joinpath(BASE_DIR, \"src\")\n",
        "TEST_DIR = Path.joinpath(BASE_DIR, \"tests\")\n",
        "SRC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEST_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "sLxAGjlv47Uw"
      },
      "id": "sLxAGjlv47Uw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {BASE_DIR}/pyproject.toml\n",
        "# Pytest\n",
        "[tool.pytest.ini_options]\n",
        "testpaths = [\"tests\"]\n",
        "python_files = \"test_*.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02QgMUz64_Ng",
        "outputId": "8fa08023-6a97-4537-b834-b9f55b9900d2"
      },
      "id": "02QgMUz64_Ng",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/pyproject.toml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {SRC_DIR}/__init__.py\n",
        "\"init file\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqm7ZwQ65DFY",
        "outputId": "a51e7030-6ece-4008-dce4-732dc55ce979"
      },
      "id": "zqm7ZwQ65DFY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/src/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are writing the transformation functions into `src/bbox_utils.py`."
      ],
      "metadata": {
        "id": "AiLgrRh7tCbX"
      },
      "id": "AiLgrRh7tCbX"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {SRC_DIR}/bbox_utils.py\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "BboxType = Union[np.ndarray, torch.Tensor]\n",
        "\n",
        "\n",
        "def cast_int_to_float(inputs: BboxType) -> BboxType:\n",
        "    if isinstance(inputs, torch.Tensor):\n",
        "        return inputs.float()\n",
        "    return inputs.astype(np.float32)\n",
        "\n",
        "\n",
        "def clone(inputs: BboxType) -> BboxType:\n",
        "    if isinstance(inputs, torch.Tensor):\n",
        "        return inputs.clone()\n",
        "    return inputs.copy()\n",
        "\n",
        "def voc2yolo(inputs: BboxType, height: float, width: float) -> BboxType:\n",
        "    outputs = clone(inputs)\n",
        "    outputs = cast_int_to_float(outputs)\n",
        "\n",
        "    outputs[..., [0, 2]] /= width\n",
        "    outputs[..., [1, 3]] /= height\n",
        "\n",
        "    outputs[..., 2] -= outputs[..., 0]\n",
        "    outputs[..., 3] -= outputs[..., 1]\n",
        "\n",
        "    outputs[..., 0] += outputs[..., 2] / 2\n",
        "    outputs[..., 1] += outputs[..., 3] / 2\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def yolo2voc(inputs: BboxType, height: float, width: float) -> BboxType:\n",
        "    outputs = clone(inputs)\n",
        "    outputs = cast_int_to_float(outputs)\n",
        "\n",
        "    outputs[..., [0, 2]] *= width\n",
        "    outputs[..., [1, 3]] *= height\n",
        "\n",
        "    outputs[..., 0] -= outputs[..., 2] / 2\n",
        "    outputs[..., 1] -= outputs[..., 3] / 2\n",
        "    outputs[..., 2] += outputs[..., 0]\n",
        "    outputs[..., 3] += outputs[..., 1]\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D1Nb4eX5S91",
        "outputId": "d143fed2-b087-46bd-a170-ca957a1c0f89"
      },
      "id": "-D1Nb4eX5S91",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/src/bbox_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create some global constants."
      ],
      "metadata": {
        "id": "fEonnRPsuWXW"
      },
      "id": "fEonnRPsuWXW"
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT, WIDTH = 480, 640\n",
        "\n",
        "voc = [98, 345, 420, 462]\n",
        "yolo = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "# GT_BBOXES are the ground truth bboxes, they are equivalent and all stem from [98, 345, 420, 462]\n",
        "GT_BBOXES = {\n",
        "    \"voc\": voc,\n",
        "    \"yolo\": yolo,\n",
        "}"
      ],
      "metadata": {
        "id": "XB-Ch3RKnkuR"
      },
      "id": "XB-Ch3RKnkuR",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `GT_BBOXES` is a dictionary that holds the bounding box format name as key and its ground truth as values. Note that the ground truth are equivalent in the sense that they all stem from the ground truth `[98, 345, 420, 462]` which is in the `xmin, ymin, xmax, ymax` format (also the voc format)."
      ],
      "metadata": {
        "id": "pacSCWtputny"
      },
      "id": "pacSCWtputny"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using parametrize to allow varying input data types"
      ],
      "metadata": {
        "id": "4BHZBui4wjot"
      },
      "id": "4BHZBui4wjot"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, if we want to test the correctness of the transformation, we first need to ensure that our test function can accept two types of input data type, `np.ndarray` and `torch.Tensor`.\n",
        "\n",
        "So for one transform function `voc2yolo`, we need to test it twice, one for which the input data type is a `np.ndarray`, the other when it's a `torch.Tensor`.\n",
        "\n",
        "Fortunately, as we have seen in the pytest documentation, the decorator `pytest.mark.parametrize` does just that."
      ],
      "metadata": {
        "id": "0lk6LZ8StJDM"
      },
      "id": "0lk6LZ8StJDM"
    },
    {
      "cell_type": "code",
      "source": [
        "def list2numpy(list_):\n",
        "    return np.asarray(list_)\n",
        "\n",
        "def list2torch(list_):\n",
        "    return torch.tensor(list_)\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "def test_voc2yolo(convert_type):\n",
        "    \"\"\"Test conversion from VOC to YOLO.\"\"\"\n",
        "    from_bbox = convert_type(GT_BBOXES[\"voc\"])\n",
        "    to_bbox = voc2yolo(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = convert_type(GT_BBOXES[\"yolo\"])\n",
        "\n",
        "    assert expected_bbox == pytest.approx(to_bbox, abs=1e-4)"
      ],
      "metadata": {
        "id": "YFWYGN_jnhsC"
      },
      "id": "YFWYGN_jnhsC",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `lines 1-5` consists of two functions, `list2numpy` and `list2torch`, which converts the ground truth bounding box inputs to either `numpy` or `torch` (note that the ground truth is created as a `list` so that the conversion is easy).\n",
        "\n",
        "- `line 7` defines the `pytest.mark.parametrize` decorator where\n",
        "    - the *first argument* is a comma-delimited string of parameter names, this string will be the argument names in the function that follows. Here I named it `\"convert_type\"`;\n",
        "    - the *second argument* will define what *values* the *first argument* can take on. This argument has type `List[Tuple[Any]]` or `List[Any]`. In our example, our first argument is actually a function `convert_type` which can be either `list2numpy` or `list2torch`.\n",
        "\n",
        "- `line 8` is our function name `test_voc2yolo` and as the name suggests, it will test whether our conversion of voc to yolo is correct. Note that the argument is named `convert_type`, corresponding exactly to our *first argument* in the decorator.\n",
        "\n",
        "- `line 10` is where we apply our argument `convert_type`, a function to the input `GT_BBOXES[\"voc\"] = [98, 345, 420, 462]`. Then the decorator will then perform `list2numpy` first on this input and convert the `list` to a `np.ndarray`.\n",
        "\n",
        "- `line 11` will then convert the input using our `voc2yolo` to its yolo equivalent format.\n",
        "\n",
        "- `line 13` gets the ground truth for yolo. Note I need to convert them into the same type as the input ground truth.\n",
        "\n",
        "- `line 15` will then check if our converted value `to_bbox` matches the ground truth for yolo `expected_bbox`.\n",
        "\n",
        "The process does not stop here, since we passed in two values for the function `convert_type`, it will also loop through the `list2torch` step."
      ],
      "metadata": {
        "id": "_iG4Q0HFneKV"
      },
      "id": "_iG4Q0HFneKV"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/test_bbox_utils.py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import torch\n",
        "from typing import Union\n",
        "import sys\n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.bbox_utils import voc2yolo, yolo2voc, clone\n",
        "\n",
        "HEIGHT, WIDTH = 480, 640\n",
        "\n",
        "voc = [98, 345, 420, 462]\n",
        "yolo = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "# GT_BBOXES are the ground truth bboxes, they are equivalent and all stem from [98, 345, 420, 462]\n",
        "GT_BBOXES = {\n",
        "    \"voc\": voc,\n",
        "    \"yolo\": yolo,\n",
        "}\n",
        "\n",
        "def list2numpy(list_):\n",
        "    return np.asarray(list_)\n",
        "\n",
        "def list2torch(list_):\n",
        "    return torch.tensor(list_)\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "def test_voc2yolo(convert_type):\n",
        "    \"\"\"Test conversion from VOC to YOLO.\"\"\"\n",
        "    from_bbox = convert_type(GT_BBOXES[\"voc\"])\n",
        "    to_bbox = voc2yolo(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = convert_type(GT_BBOXES[\"yolo\"])\n",
        "    \n",
        "    assert expected_bbox.all() == pytest.approx(to_bbox.all(), abs=1e-4)"
      ],
      "metadata": {
        "id": "L_yi59JstNm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3315f9f9-ce24-48b8-981b-4d4bc15709fe"
      },
      "id": "L_yi59JstNm5",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tests/test_bbox_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v tests/test_bbox_utils.py -s       # tests for a single file"
      ],
      "metadata": {
        "id": "4fT3alyX5eGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6093f190-956a-424e-84b6-65865a4b6626"
      },
      "id": "4fT3alyX5eGv",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils.py::test_voc2yolo[list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 2 passed in 0.82 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see\n",
        "\n",
        "```bash\n",
        "$ tests/test_bbox_utils.py::test_voc2yolo[list2numpy] PASSED\n",
        "$ tests/test_bbox_utils.py::test_voc2yolo[list2torch] PASSED\n",
        "```\n",
        "\n",
        "means that the test function has tested for both combinations of `numpy` and `tensor`, and both give us the expected output!"
      ],
      "metadata": {
        "id": "SFj1fuByv6lG"
      },
      "id": "SFj1fuByv6lG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using parametrize to test for dimensions"
      ],
      "metadata": {
        "id": "pL74NDQ4whwu"
      },
      "id": "pL74NDQ4whwu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step is to test that our transform functions can handle different dimensions. Whether the input is a 3d-tensor, or a 10d-array, all of them should work. Of course, our main goal here is still to test the correctness of the transformation, but bear in mind we need to have a separate test to check the consistency of input and output dimensions (i.e. passing in a 2d-array will result in an output of 2d-array).\n",
        "\n",
        "Let's say we want to test if the code works for 3 dimensions, means checking if the code can execute correctly without error for dimensions in `[1d, 2d, 3d]`.\n",
        "\n",
        "This is not trivial as we need to check for 6 different cases, a result of the cartesian product of\n",
        "\n",
        "```\n",
        "[list2numpy, list2torch] x [0, 1, 2] = {(list2numpy, 0), (list2numpy, 1), ...}\n",
        "```\n",
        "\n",
        "a total of 6 combinations.\n",
        "\n",
        "We will continue to leverage `pytest`'s parametrize to test all 6 cases.\n",
        "\n",
        "There will not be much change besides defining an extra utility function `expand_dim` will expands the input's dimensions according to the `num_dims` argument.\n",
        "\n",
        "To be able to use the cartesian product, we simply add one more decorator below our `convert_type`, in which case it now takes in `num_dims` as first argument, and `[0, 1, 2]` as the second, indicating that we want the function to test for the aforementioned 3 dimensions. Having two parametrize decorators stacked together means it will execute in combination, exactly as what we wanted."
      ],
      "metadata": {
        "id": "_0yfucxpwxit"
      },
      "id": "_0yfucxpwxit"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/test_bbox_utils.py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import torch\n",
        "from typing import Union\n",
        "import sys\n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.bbox_utils import voc2yolo, yolo2voc, clone\n",
        "\n",
        "HEIGHT, WIDTH = 480, 640\n",
        "\n",
        "voc = [98, 345, 420, 462]\n",
        "yolo = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "# GT_BBOXES are the ground truth bboxes, they are equivalent and all stem from [98, 345, 420, 462]\n",
        "GT_BBOXES = {\n",
        "    \"voc\": voc,\n",
        "    \"yolo\": yolo,\n",
        "}\n",
        "\n",
        "def list2numpy(list_):\n",
        "    return np.asarray(list_)\n",
        "\n",
        "def list2torch(list_):\n",
        "    return torch.tensor(list_)\n",
        "\n",
        "def expand_dim(\n",
        "    bboxes: Union[np.ndarray, torch.Tensor],\n",
        "    num_dims: int,\n",
        ") -> Union[np.ndarray, torch.Tensor]:\n",
        "    \"\"\"Expand the dimension of bboxes to num_dims.\n",
        "\n",
        "    Note:\n",
        "        np.expand_dims will not work for tuple dim numpy < 1.18.0 which\n",
        "        is not the version in our cicd.\n",
        "    \"\"\"\n",
        "    bboxes = clone(bboxes)\n",
        "    return bboxes[(None,) * num_dims]\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "@pytest.mark.parametrize(\"num_dims\", [0, 1, 2])\n",
        "def test_voc2yolo(convert_type, num_dims):\n",
        "    \"\"\"Test conversion from VOC to YOLO.\"\"\"\n",
        "    from_bbox = convert_type(GT_BBOXES[\"voc\"])\n",
        "    from_bbox = expand_dim(from_bbox, num_dims)\n",
        "\n",
        "    to_bbox = voc2yolo(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = convert_type(GT_BBOXES[\"yolo\"])\n",
        "    \n",
        "    assert expected_bbox.all() == pytest.approx(to_bbox.all(), abs=1e-4)"
      ],
      "metadata": {
        "id": "TnFz8zv95eJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbb9787-b17c-4b3c-9634-6f75e5a838f5"
      },
      "id": "TnFz8zv95eJK",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tests/test_bbox_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v tests/test_bbox_utils.py -s       # tests for a single file"
      ],
      "metadata": {
        "id": "IYQpDbf55eLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a1cc81-57fa-477c-ca13-0b8e96864d7a"
      },
      "id": "IYQpDbf55eLk",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 6 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils.py::test_voc2yolo[0-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[0-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[1-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[1-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[2-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[2-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 6 passed in 0.89 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our results passed\n",
        "\n",
        "```bash\n",
        "tests/test_bbox_utils.py::test_voc2yolo[0-list2numpy] PASSED\n",
        "tests/test_bbox_utils.py::test_voc2yolo[0-list2torch] PASSED\n",
        "tests/test_bbox_utils.py::test_voc2yolo[1-list2numpy] PASSED\n",
        "tests/test_bbox_utils.py::test_voc2yolo[1-list2torch] PASSED\n",
        "tests/test_bbox_utils.py::test_voc2yolo[2-list2numpy] PASSED\n",
        "tests/test_bbox_utils.py::test_voc2yolo[2-list2torch] PASSED\n",
        "```\n",
        "\n",
        "Notice that in each line they indicate the combination, for example, the first line says `test_voc2yolo[0-list2numpy] PASSED`, which means they tested for the case of `0` dimensions and the input type of `numpy`."
      ],
      "metadata": {
        "id": "Mk2SQRujy_1p"
      },
      "id": "Mk2SQRujy_1p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use fixtures "
      ],
      "metadata": {
        "id": "Z_OPaHyjzuvS"
      },
      "id": "Z_OPaHyjzuvS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move on, we talk a little about fixtures.\n",
        "\n",
        "The idea of fixtures is that if you have multiple test functions that take in a \"fixed\" set of inputs (i.e. `GT_BBOXES`), then we should consider using fixtures.\n",
        "\n",
        "For example, if I were to write a new test `yolo2voc`, then I'd expect the `GT_BBOXES` to be called inside the test again.\n",
        "\n",
        "For that we can write a function `gt_bboxes` that has `pytest.fixture` as decorator. Subsequently, we can pass `gt_bboxes` to any test functions.\n",
        "\n",
        "However, the same result can be achieved with our old method, defining a global constant `GT_BBOXES` work as well. However, if you have multiple tests spanned across different scripts (files), and they happen to need the same global constant `GT_BBOXES`, then it is better to define a fixture for it and place it in `conftest.py` (more on that in documentation).\n",
        "\n",
        "For our purpose, we will still package `GT_BBOXES` into a fixture and use it as argument for both `test_voc2yolo` and `test_yolo2voc`."
      ],
      "metadata": {
        "id": "X_FLB2u6z4G9"
      },
      "id": "X_FLB2u6z4G9"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/test_bbox_utils.py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import torch\n",
        "from typing import Union\n",
        "import sys\n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.bbox_utils import voc2yolo, yolo2voc, clone\n",
        "\n",
        "HEIGHT, WIDTH = 480, 640\n",
        "\n",
        "voc = [98, 345, 420, 462]\n",
        "yolo = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "# GT_BBOXES are the ground truth bboxes, they are equivalent and all stem from [98, 345, 420, 462]\n",
        "GT_BBOXES = {\n",
        "    \"voc\": voc,\n",
        "    \"yolo\": yolo,\n",
        "}\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def gt_bboxes():\n",
        "    return GT_BBOXES\n",
        "\n",
        "def list2numpy(list_):\n",
        "    return np.asarray(list_)\n",
        "\n",
        "def list2torch(list_):\n",
        "    return torch.tensor(list_)\n",
        "\n",
        "def expand_dim(\n",
        "    bboxes: Union[np.ndarray, torch.Tensor],\n",
        "    num_dims: int,\n",
        ") -> Union[np.ndarray, torch.Tensor]:\n",
        "    \"\"\"Expand the dimension of bboxes to num_dims.\n",
        "\n",
        "    Note:\n",
        "        np.expand_dims will not work for tuple dim numpy < 1.18.0 which\n",
        "        is not the version in our cicd.\n",
        "    \"\"\"\n",
        "    bboxes = clone(bboxes)\n",
        "    return bboxes[(None,) * num_dims]\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "@pytest.mark.parametrize(\"num_dims\", [0, 1, 2])\n",
        "def test_voc2yolo(gt_bboxes, convert_type, num_dims):\n",
        "    \"\"\"Test conversion from VOC to YOLO.\"\"\"\n",
        "    from_bbox = convert_type(gt_bboxes[\"voc\"])\n",
        "    from_bbox = expand_dim(from_bbox, num_dims)\n",
        "\n",
        "    to_bbox = voc2yolo(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = convert_type(gt_bboxes[\"yolo\"])\n",
        "    \n",
        "    assert expected_bbox.all() == pytest.approx(to_bbox.all(), abs=1e-4)\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "@pytest.mark.parametrize(\"num_dims\", [0, 1, 2])\n",
        "def test_yolo2voc(gt_bboxes, convert_type, num_dims):\n",
        "    \"\"\"Test conversion from YOLO to VOC.\"\"\"\n",
        "    from_bbox = convert_type(gt_bboxes[\"yolo\"])\n",
        "    from_bbox = expand_dim(from_bbox, num_dims)\n",
        "\n",
        "    to_bbox = voc2yolo(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = convert_type(gt_bboxes[\"voc\"])\n",
        "    \n",
        "    assert expected_bbox.all() == pytest.approx(to_bbox.all(), abs=1e-4)"
      ],
      "metadata": {
        "id": "j5iP8htY5eN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3352cdb0-a070-460c-84cb-099fa59c0c39"
      },
      "id": "j5iP8htY5eN8",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tests/test_bbox_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v tests/test_bbox_utils.py -s       # tests for a single file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsKDT85J0bxR",
        "outputId": "c49c4647-5d13-4cc3-adb4-ab203a7063a8"
      },
      "id": "CsKDT85J0bxR",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 12 items                                                             \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils.py::test_voc2yolo[0-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[0-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[1-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[1-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[2-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_voc2yolo[2-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_yolo2voc[0-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_yolo2voc[0-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_yolo2voc[1-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_yolo2voc[1-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_yolo2voc[2-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_yolo2voc[2-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m========================== 12 passed in 0.84 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using parametrize for different transforms"
      ],
      "metadata": {
        "id": "HHRv-T1N08R5"
      },
      "id": "HHRv-T1N08R5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I actually have more than 10 transform functions that converts bounding boxes. This means I have to write 10 cases: `test_voc2yolo`, `test_yolo2voc`, ...\n",
        "\n",
        "Most of the code inside the test function is the same.\n",
        "\n",
        "We can again leverage on parametrize and define the first argument to be `conversion_name` which takes on values such as `[\"voc2yolo\", \"yolo2voc\"]`. These values will then be our identifier on which conversion/transform to use.\n",
        "\n",
        "For that, we need to revamp our `GT_BBOXES` in our fixture `gt_bboxes`.\n",
        "\n",
        "```python\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def gt_bboxes():\n",
        "    \"\"\"Return a dictionary of ground truth bboxes.\"\"\"\n",
        "    return {\n",
        "        \"voc2yolo\": [voc, yolo],\n",
        "        \"yolo2voc\": [yolo, voc],\n",
        "    }\n",
        "```\n",
        "\n",
        "where by construction, the key to be the exact name of the function call of the transformation. This means that if our transform function is `voc2yolo`, then our key must be `\"voc2yolo\"`. Then the values will be a list, where the first element is the ground truth of the input (i.e. voc), and the second element is the ground truth of the output (i.e. yolo). "
      ],
      "metadata": {
        "id": "13tdUAx71A9Y"
      },
      "id": "13tdUAx71A9Y"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TEST_DIR}/test_bbox_utils.py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import torch\n",
        "from typing import Union\n",
        "import sys\n",
        "sys.path.append(\"/content\") # append to import properly.\n",
        "from src.bbox_utils import voc2yolo, yolo2voc, clone\n",
        "\n",
        "HEIGHT, WIDTH = 480, 640\n",
        "\n",
        "voc = [98, 345, 420, 462]\n",
        "yolo = [0.4046875, 0.840625, 0.503125, 0.24375]\n",
        "\n",
        "# GT_BBOXES are the ground truth bboxes, they are equivalent and all stem from [98, 345, 420, 462]\n",
        "GT_BBOXES = {\n",
        "        \"voc2yolo\": [voc, yolo],\n",
        "        \"yolo2voc\": [yolo, voc],\n",
        "}\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def gt_bboxes():\n",
        "    \"\"\"Return a dictionary of ground truth bboxes.\"\"\"\n",
        "    return GT_BBOXES\n",
        "\n",
        "def list2numpy(list_):\n",
        "    return np.asarray(list_)\n",
        "\n",
        "def list2torch(list_):\n",
        "    return torch.tensor(list_)\n",
        "\n",
        "def expand_dim(\n",
        "    bboxes: Union[np.ndarray, torch.Tensor],\n",
        "    num_dims: int,\n",
        ") -> Union[np.ndarray, torch.Tensor]:\n",
        "    \"\"\"Expand the dimension of bboxes to num_dims.\n",
        "\n",
        "    Note:\n",
        "        np.expand_dims will not work for tuple dim numpy < 1.18.0 which\n",
        "        is not the version in our cicd.\n",
        "    \"\"\"\n",
        "    bboxes = clone(bboxes)\n",
        "    return bboxes[(None,) * num_dims]\n",
        "\n",
        "@pytest.mark.parametrize(\"convert_type\", [list2numpy, list2torch])\n",
        "@pytest.mark.parametrize(\"num_dims\", [0, 1, 2])\n",
        "@pytest.mark.parametrize(\"conversion_name\", [\"voc2yolo\", \"yolo2voc\"])\n",
        "def test_correct_transforms(gt_bboxes, convert_type, num_dims, conversion_name):\n",
        "    \"\"\"Test conversion.\"\"\"\n",
        "\n",
        "    conversion_fn = globals()[conversion_name]\n",
        "\n",
        "    from_bbox, expected_bbox = gt_bboxes[conversion_name]\n",
        "\n",
        "    from_bbox = convert_type(from_bbox)\n",
        "    from_bbox = expand_dim(from_bbox, num_dims)\n",
        "\n",
        "    to_bbox = conversion_fn(from_bbox, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    expected_bbox = expand_dim(convert_type(expected_bbox), num_dims)\n",
        "    \n",
        "    assert expected_bbox.all() == pytest.approx(to_bbox.all(), abs=1e-4)"
      ],
      "metadata": {
        "id": "AzkrCgjtwf6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64dab078-3b38-4f7c-b93a-8ecdd3728fd1"
      },
      "id": "AzkrCgjtwf6M",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tests/test_bbox_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v tests/test_bbox_utils.py -s       # tests for a single file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJAsVPin45yY",
        "outputId": "2b93437d-1734-4eee-d3aa-0d92342fddf7"
      },
      "id": "rJAsVPin45yY",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.14, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 12 items                                                             \u001b[0m\n",
            "\n",
            "tests/test_bbox_utils.py::test_correct_transforms[voc2yolo-0-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[voc2yolo-0-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[voc2yolo-1-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[voc2yolo-1-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[voc2yolo-2-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[voc2yolo-2-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[yolo2voc-0-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[yolo2voc-0-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[yolo2voc-1-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[yolo2voc-1-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[yolo2voc-2-list2numpy] \u001b[32mPASSED\u001b[0m\n",
            "tests/test_bbox_utils.py::test_correct_transforms[yolo2voc-2-list2torch] \u001b[32mPASSED\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m========================== 12 passed in 0.84 seconds ===========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we achieved the same results."
      ],
      "metadata": {
        "id": "iOmKxGJt5Cj9"
      },
      "id": "iOmKxGJt5Cj9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}